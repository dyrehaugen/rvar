[["index.html", "Varia 1 Varia", " Varia Dyrehaugen Web Notebook 2024-01-10 1 Varia "],["entropy.html", "2 Entropy 2.1 Social Architecture - Ian Wright 2.2 Information Entropy - Shannon", " 2 Entropy 2.1 Social Architecture - Ian Wright Think of entropy as a number that measures the randomness of a distribution. The higher the entropy the more random the distribution. The most random distribution of all is the uniform distribution. It’s the most random distribution because every x is equally likely. There’s a constraint on entropy maximisation. That constraint is the conservation of energy. We can calculate the distribution that corresponds to maximum entropy subject to a total energy constraint. The answer to this optimisation problem is the exponential distribution. Ian Wright 2.2 Information Entropy - Shannon In a single groundbreaking paper, he laid the foundation for the entire communication infrastructure underlying the modern information age. The heart of his theory is a simple but very general model of communication: A transmitter encodes information into a signal, which is corrupted by noise and then decoded by the receiver. Despite its simplicity, Shannon’s model incorporates two key insights: isolating the information and noise sources from the communication system to be designed, and modeling both of these sources probabilistically. He imagined the information source generating one of many possible messages to communicate, each of which had a certain probability. The probabilistic noise added further randomness for the receiver to disentangle. Before Shannon, the problem of communication was primarily viewed as a deterministic signal-reconstruction problem: how to transform a received signal, distorted by the physical medium, to reconstruct the original as accurately as possible. Shannon’s genius lay in his observation that the key to communication is uncertainty. This single observation shifted the communication problem from the physical to the abstract, allowing Shannon to model the uncertainty using probability. This came as a total shock to the communication engineers of the day. First, Shannon came up with a formula for the minimum number of bits per second to represent the information, a number he called its entropy rate, H. This number quantifies the uncertainty involved in determining which message the source will generate. The lower the entropy rate, the less the uncertainty, and thus the easier it is to compress the message into something shorter. For example, texting at the rate of 100 English letters per minute means sending one out of 26100 possible messages every minute, each represented by a sequence of 100 letters. One could encode all these possibilities into 470 bits, since 2470 ≈ 26100. If the sequences were equally likely, then Shannon’s formula would say that the entropy rate is indeed 470 bits per minute. In reality, some sequences are much more likely than others, and the entropy rate is much lower, allowing for greater compression. Second, he provided a formula for the maximum number of bits per second that can be reliably communicated in the face of noise, which he called the system’s capacity, C. This is the maximum rate at which the receiver can resolve the message’s uncertainty, effectively making it the speed limit for communication. Finally, he showed that reliable communication of the information from the source in the face of noise is possible if and only if H &lt; C. Thus, information is like water: If the flow rate is less than the capacity of the pipe, then the stream gets through reliably. His theorems led to some counterintuitive conclusions. Suppose you are talking in a very noisy place. What’s the best way of making sure your message gets through? Maybe repeating it many times? That’s certainly anyone’s first instinct in a loud restaurant, but it turns out that’s not very efficient. Sure, the more times you repeat yourself, the more reliable the communication is. But you’ve sacrificed speed for reliability. Shannon showed us we can do far better. Repeating a message is an example of using a code to transmit a message, and by using different and more sophisticated codes, one can communicate fast — all the way up to the speed limit, C — while maintaining any given degree of reliability. Another unexpected conclusion stemming from Shannon’s theory is that whatever the nature of the information — be it a Shakespeare sonnet, a recording of Beethoven’s Fifth Symphony or a Kurosawa movie — it is always most efficient to encode it into bits before transmitting it. So in a radio system, for example, even though both the initial sound and the electromagnetic signal sent over the air are analog wave forms, Shannon’s theorems imply that it is optimal to first digitize the sound wave into bits, and then map those bits into the electromagnetic wave. This surprising result is a cornerstone of the modern digital information age, where the bit reigns supreme as the universal currency of information. Shannon’s general theory of communication is so natural that it’s as if he discovered the universe’s laws of communication, rather than inventing them. His theory is as fundamental as the physical laws of nature. In that sense, he was a scientist. Shannon invented new mathematics to describe the laws of communication. He introduced new ideas, like the entropy rate of a probabilistic model, which have been applied in far-ranging branches of mathematics such as ergodic theory, the study of long-term behavior of dynamical systems. In that sense, Shannon was a mathematician. Shannon’s theory has now become the standard framework underlying all modern-day communication systems: optical, underwater, even interplanetary. Shannon figured out the foundation for all this more than 70 years ago. How did he do it? By focusing relentlessly on the essential feature of a problem while ignoring all other aspects. Shannon (Quanta Magazine) "],["ai---artificial-intelligence.html", "3 AI - Artificial Intelligence 3.1 AI Economics Applications", " 3 AI - Artificial Intelligence Smith Limits of Science A whole lot of complex phenomena have so far defied the approach that gave us the laws of physics and genetics. Language, cognition, society, economics, complex ecologies — these things so far don’t have any equivalent of Newton’s Laws, and it’s not clear they ever will. This problem has been recognized for a very long time, and thinkers tried several approaches to get around it. Some hoped that all complex phenomena would be governed by emergent properties — that simplicity would emerge at higher levels of complexity, allowing us to discover simple laws for things like psychology and economics even without connecting those laws to the underlying physics. Indeed, this idea is implicit (or, occasionally, explicit) in the way economists try to write down simple mathematical laws of collective human behavior. People make fun of this approach as “physics envy”, but sometimes it really works; auction theory isn’t derived from physics, but it has been able to make very effective predictions about how much people will pay for Google ads or spectrum rights. Ditto for “gravity models” of trade, migration, retail shopping, etc. Sometimes emergence works. Sometimes, though, it doesn’t — or at least, it doesn’t yet. But in psychology, in macroeconomics, in natural language processing, and many other domains, the search for laws of nature has been mostly stymied so far, and it’s not clear when real progress might ever be made. Wigner goes so far as to postulate that some domains of human knowledge might never be described by such simple, generalizable principles. Other approaches for getting around the problem of complexity — chaos theory, complexity theory — yielded interesting insights, but ultimately didn’t succeed in giving us substantially more mastery of the phenomena they dealt with. In the late 20th century, the problem of complexity was like a looming wall up ahead — as scientists found more and more of the laws that could be found, a larger and larger percentage of the remaining problems were things where laws seemed very hard or potentially even impossible to find. Control without understanding, power without knowledge In 2001, the statistician Leo Breiman wrote an essay (pdf) called “Statistical Modeling: The Two Cultures”, in which he described an emerging split between statisticians who were interested in making parsimonious models of the phenomena they modeled, and others who were more interested in predictive accuracy. He demonstrated that in a number of domains, what he calls “algorithmic” models (early machine learning techniques) were yielding consistently better predictions than what he calls “data models”, even though the former were far less easy, or even impossible, to interpret. This raises an important question: What is the goal of human knowledge? As I see it — and as Breiman sees it — the fundamental objective is not understanding but control. By recording which crops grow in which season, we can feed our families. By understanding that germs cause disease, we can know to wash our hands or get a vaccine, and lower our risk of death. In these situations, knowledge and understanding might be intrinsically satisfying to our curiosity, but that satisfaction ultimately pales in importance to our ability to reshape our world to our benefit. And the “algorithmic” learning models that Breiman talks about were better able to deliver their users the power to reshape the world, even if they offered less promise of understanding what they were predicting. Why should we care about understanding the things we predict? To most of us, raised and inculcated in the age of science, that might seem like a laughable question, but there actually is a good reason. “Understanding”, in the scientific sense, means deriving a simple, generalizable principle that you can apply in other domains. You can write down Kepler’s laws of planetary motion, but Newton’s laws of motion and gravitation let you generalize from planetary orbits to artillery shells. Collapsing observed phenomena to simple, generalizable laws and then expanding these laws again in some other domain to allow you to control other phenomena is fundamental to the awesome power of science. So because you and I sit at the end of 400 years of science being the most powerful tool in the world, we have naturally been taught that it is very, very important to understand things. But what if, sometimes, there are ways to generalize from one phenomenon to another without finding any simple “law” to intermediate between the two? Breiman sadly never lived to see his vision come to fruition, but that is exactly what the people who work in machine learning and artificial intelligence are increasingly doing. In 2009 — just before the deep learning revolution really kicked off — the Google researchers Alon Halevy, Peter Norvig, and Fernando Pereira wrote an essay called “The Unreasonable Effectiveness of Data” that picked up the argument where Breiman left off. They argued that in the cases of natural language processing and machine translation, applying large amounts of data was effective even in the absence of simple generalizable laws. A few excerpts: Sciences that involve human beings rather than elementary particles have proven more resistant to elegant mathematics…An informal, incomplete [list of the grammatical rules that define] the English language runs over 1,700 pages. Perhaps when it comes to natural language processing and related fields, we’re doomed to complex theories that will never have the elegance of physics equations. But if that’s so, we should stop acting as if our goal is to author extremely elegant theories, and instead embrace complexity and make use of the best ally we have: the unreasonable effectiveness of data… So, follow the data…Represent all the data with a nonparametric model rather than trying to summarize it with a parametric model, because with very large data sources, the data holds a lot of detail. For natural language applications, trust that human language has already evolved words for the important concepts. See how far you can go by tying together the words that are already there, rather than by inventing new concepts with clusters of words. Now go out and gather some data, and see what it can do. The basic idea here is that many complex phenomena like language have underlying regularities that are difficult to summarize but which are still possible to generalize. The ability to write down farming techniques is power. The ability to calculate the path of artillery shells is power. And the ability to have a machine reliably and consistently write paragraphs as clear and helpful as the one above [ChatGPT example] is power, even if we don’t really understand the principles of how it’s doing what it does. This power is hardly limited to natural language processing and chatbots. In recent years, Google’s AlphaFold algorithm has outpaced traditional scientific methods in predicting the shapes of folded proteins. We are almost certainly going to call this new type of prediction technique “science”, at least for a while, because it deals with fields of inquiry that we have traditionally called “science”, like protein folding. But I think this will obscure more than it clarifies. I hope we eventually come up with a new term for this sort of black-box prediction method, not because it’s better or worse than science, but because it’s different. A big knock on AI is that because it doesn’t really let you understand the things you’re predicting, it’s unscientific. And in a formal sense, I think this is true. But instead of spending our effort on a neverending (and probably fruitless) quest to make AI fully interpretable, I think we should recognize that science is only one possible tool for predicting and controlling the world. Compared to science, black-box prediction has both strengths and weaknesses. One weakness — the downside of being “unscientific” — is that without simple laws, it’s harder to anticipate when the power of AI will fail us. Our lack of knowledge about AI’s internal workings means that we’re always in danger of overfitting and edge cases. In other words, the “third magic” may be more like actual magic than the previous two — AI may always be powerful yet ineffable, performing frequent wonders, but prone to failure at fundamentally unpredictable times. But even wild, occasionally-uncontrollable power is real power. AI and Economic In the past few decades, as economics has moved away from theory and toward empirics, the most important innovation has been the use of natural experiments — situations where some policy change or seemingly random difference allows you to tell yourself that you’re looking at causation, rather than just correlation. This is different than what I call “history”, because you’re doing more than just documenting facts; you’re verifying causal links. But it’s also different from science, because a lot of the time you don’t exactly know why the causal links are there. In a way, a natural experiment is its own sort of black-box prediction algorithm. A number of subfields of econ, however, are so complex, with so many feedback systems, that they’ve largely resisted the natural experiment approach. These include not just the study of business cycles (what most people call “macro”), but also the study of economic growth, international finance, and a number of others. In these fields, theory (including “structural estimation”) still rules, but predictive power is very low. Might we apply AI tools to these hard problems, in order to predict vast economic forces without needing to understand them? A recent paper by Khachiyan et al. argues that the answer is “yes”. The authors use deep neural nets (i.e., AI) to look at daytime satellite imagery, in order to predict future economic growth at the hyper-local level. The results they achieve are nothing short of astonishing: For grid cells with lateral dimensions of 1.2km and 2.4km (where the average US county has dimension of 55.6km), our model predictions achieve R2 values of 0.85 to 0.91 in levels, which far exceed the accuracy of existing models, and 0.32 to 0.46 in decadal changes, which have no counterpart in the literature and are 3-4 times larger than for commonly used nighttime lights. This isn’t yet AlphaFold, but being able to predict the economic growth of a few city blocks 10 years into the future with even 30% or 40% accuracy is leaps and bounds ahead of anything I’ve ever seen. It suggests that rather than being utter incomprehensible chaos, some economic systems have patterns and regularities that are too complex to be summarized with simple mathematical theories, but which nevertheless can be captured and generalized by AI. Khachiyan et al.’s paper raises the possibility that in a decade or two, macroeconomics might go from being something we simply theorize about to something we can anticipate — and therefore, something we can control. The authors suggest place-based policies, transportation infrastructure construction, and disaster relief as three possible applications of their work. Comment to Noah “…some economic systems have patterns and regularities that are too complex to be summarized with simple mathematical theories, but which nevertheless can be captured and generalized by AI” - yes indeed. AI may be helpfull in demonstrating the our capitalist economic system grows with increased consumption of nature - no decoupling whatsoever. Applying convolutional neural networks to daytime satellite imagery predicts microspatial changes in income and population at a decadal frequency. What are these predictors? Visible asphalt and cement! To site from the paper: Convolutional Neural Networks (CNN) extracts economic information that is latent in spectral data. Asphalt, cement, gravel, soil, water, vegetation, and other materials vary in their reflectance intensity across the light spectrum. The presence of these materials varies enormously within an urban area: more vegetation and loose soil in green spaces; more asphalt and cement around motorways; more steel and wood, together with concrete, in houses and buildings. That’s economic growth! (Comment to Noah Smith on AI in economics: https://noahpinion.substack.com/p/the-third-magic/comments) Noah Smith (2023) The third magic - A meditation on history, science, and AI 3.1 AI Economics Applications Khachiyan Abstract We apply deep learning to daytime satellite imagery to predict changes in income and population at high spatial resolution in US data. For grid cells with lateral dimensions of 1.2km and 2.4km (where the average US county has dimension of 55.6km), our model predictions achieve R2 values of 0.85 to 0.91 in levels, which far exceed the accuracy of existing models, and 0.32 to 0.46 in decadal changes, which have no counterpart in the literature and are 3-4 times larger than for commonly used nighttime lights. Our network has wide application for analyzing localized shocks. Khachiyan Excerpts Recent work in remote sensing and computer science uses convolutional neural networks (CNNs) to predict outcomes from multi-spectral daytime satellite imagery at high spatial resolutions. In our context, a CNN extracts economic information that is latent in spectral data. Asphalt, cement, gravel, soil, water, vegetation, and other materials vary in their reflectance intensity across the light spectrum (e.g., De Fries et al., 1998). The presence of these materials varies enormously within an urban area: more vegetation and loose soil in green spaces; more asphalt and cement around motorways; more steel and wood, together with concrete, in houses and buildings (Zha et al., 2003). The shapes of these materials exhibit similarly wide variation: irregular edges in green spaces, intermittent grids of grass and roofing material in suburbs, larger rectangular clusters in apartment complexes and shopping malls, and compact, interconnected grids in urban centers. It is this complexity that makes a neural network powerful—the network learns the mapping of materials and shapes to the level of economic activity and changes in materials and shapes to changes in economic activity. As an empirical regularity, the features learned by the network are often organized into a hierarchy of complexity (Zeiler and Fergus, 2014), in which early layers learn to identify simple features, such as edges or basic shapes, and subsequent layers learn to compose these simple features into complex objects, such as office buildings, industrial parks, suburban developments. We show that applying convolutional neural networks to daytime satellite imagery predicts microspatial changes in income and population at a decadal frequency. Khachiyan (2021) USING NEURAL NETWORKS TO PREDICT MICRO-SPATIAL ECONOMIC GROWTH (pdf) Kevin Kelly So far, out of the perhaps dozen of cognitive modes operating in our minds, we have managed to synthesize two of them: perception and pattern matching. Everything we’ve seen so far in AI is because we can produce those two modes. We have not made any real progress in synthesizing symbolic logic and deductive reasoning and other modes of thinking. It is those “others” that are so important because as we inch along we are slowly realizing we still have NO IDEA how our own intelligences really work, or even what intelligence is. A major byproduct of AI is that it will tell us more about our minds than centuries of psychology and neuroscience have. Kelly (2023) Interview with Noah Smith "],["collapse.html", "4 Collapse", " 4 Collapse Welsh This year has seen the constant shattering of temperature records. Temperatures in the high thirties, in winter, have been common. The majority of the Mediterranean is going to be uninhabitable without air conditioning for months every year. This includes North Africa and the European areas. The same will be true of most areas of the tropics. Time scale is ten to fifteen years. Because climate change includes weather instability, it will become impossible to get property insurance in increasing areas, starting with the coasts and areas prone to wildfires. Wildfires will continue until the ecology of areas has changed to one suitable to their new temperature and rainfall pattern. In the short to mid term, there will be a lot of river floods, then rivers based on snow pack or coming from glaciers will reduce in size or dry up. Most of the world’s aquifers are drained, and many are poisoned. This means vast areas will become unsuitable for agriculture, which will lead to genuine food shortages. We haven’t had those in a long time, our current shortages are because we can’t be bothered to distribute food, of which we have great excess. But by 2030 we’ll see some real famines, and by 2040 almost everyone’s going to be eating less, even if they aren’t going hungry. The oceans will become increasingly lifeless, and most fisheries will collapse. Even sea farming will be difficult, as oxygen content drops and acidification increases. If you’re middle aged, you’ll see the start of the Sea of Jellyfish. The real danger is if CO2 fixing and O2 emitting plankton collapse, in which case we’ll see some real problems. On land, the great rainforests will mostly die. This includes the Amazon and Congo. They will be replaced by wastelands, and will be almost impossible to regrow under the new circumstances. This will, again, lead to vast increases in CO2. The effect on Brazil will be catastrophic. The first ocean inundations will come sooner than almost anyone thinks and low lying countries and areas which have not built sea walls and pumps will go underwater. Bangladesh is a good weather vane here, but the northern Chinese breadbasket is at risk in the second wave. If this was only about CO2 and global warming the realist optimist types would be right that it’d suck mightily, but whatever. The danger is that we’ve also go ecological collapse going on. I can’t estimate the odds correctly, but collapse of food chains, and in particular collapses of microbes, insects, plankton and so on could lead to drastic issues. The old line is that if the bees go extinct, so do we, but there’s a lot more risk than that, and that’s the “apocalyptic” scenario. In your personal life, you should be preparing. Find a way to get your own water, even if it’s condensation. Food is important but understand that growing it outside is going to be tricky because of climate instability. Food you can count on will have some form of environmental control. Expect everything to come in faster than the consensus ICC estimates. They’ve almost all been wrong to the upside, so consider them the “best case scenario” and don’t plan for that. Climate change and ecological collapse are going to play into geopolitics in a big way. Normally, as I wrote yesterday, the ascendance of China would be all over except the shooting, but China’s going to get hit hard. They’re not stupid, and they know this. They just penned an absolutely massive deal for food from Russia, for example. But they need to do a lot more, and they and everyone else are going to have to change lifestyles. An economy of millions of cars, with sprawling cities makes no damn sense if the future that is coming. Refugee waves are going to be absolutely massive, with hundreds of millions of people on the move. Multiple countries will collapse into warlordism and anarchy. There will be real revolutions, with elite murdered en-masse, because when people start starving and going without water, they will freak. There just isn’t going to be enough to go around, it’s that simple. If you want to survive, beyond the obvious, make friends and join or create strong community groups. You want a lot of people to like you and want you to live. Find a way to be useful, if possible, too. Plumbers and handymen and makers will be taken care of. This is still some ways off, but understand clearly, civilization collapse has started, we are past the peak and past the point where we can stop it with any actions which it is even slightly conceivable we are capable of taking politically. Welsh (2023) Climate Change and Environmental Collapse "],["complexity.html", "5 Complexity 5.1 Wicked Problems 5.2 Predicting Collapse 5.3 Complexity Theory and Financial Regulation 5.4 Holobiont", " 5 Complexity 5.1 Wicked Problems The Wicked Problem of COVID-19 The pandemic’s complexity transcends health, environment, economy, and social boundaries. Any intervention triggers responses in those various fields. From Oz Sahin, Hengky Salim, Emiliya Suprun, Shannon Rutherford , et al., “Developing a Preliminary Causal Loop Diagram for Understanding the Wicked Complexity of the COVID-19 Pandemic,” Griffith University, Australia Systems , May 2020 Wicked Complexity (pdf) 5.2 Predicting Collapse Horstmeyer Abstract The collapse of ecosystems, the extinction of species, and the breakdown of economic and financial networks usually hinges on topological properties of the underlying networks, such as the existence of self-sustaining (or autocatalytic) feedback cycles. Such collapses can be understood as a massive change of network topology, usually accompanied by the extinction of a macroscopic fraction of nodes and links. It is often related to the breakdown of the last relevant directed catalytic cycle within a dynamical system. Without detailed structural information it seems impossible to state, whether a network is robust or if it is likely to collapse in the near future. Here we show that it is nevertheless possible to predict collapse for a large class of systems that are governed by a linear (or linearized) dynamics. To compute the corresponding early warning signal, we require only non-structural information about the nodes’ states such as species abundances in ecosystems, or company revenues in economic networks. It is shown that the existence of a single directed cycle in the network can be detected by a “quantization effect” of node states, that exists as a direct consequence of a corollary of the Perron–Frobenius theorem. The proposed early warning signal for the collapse of networked systems captures their structural instability without relying on structural information. We illustrate the validity of the approach in a transparent model of co-evolutionary ecosystems and show this quantization in systems of species evolution, epidemiology, and population dynamics. Horstmeyer (2021) Predicting collapse of adaptive networked systems without knowing the network (pdf) SI (pdf) 5.3 Complexity Theory and Financial Regulation Battiston Abstract* Traditional economic theory could not explain, much less predict, the near collapse of t he financial system and its long-lasting effects on the global economy. Since the 2008 c risis, there has been increasing interest in using ideas from complexity theory to make sense of economic and financial markets. Concepts, such as tipping points, networks, con tagion, feedback, and resilience have entered the financial and regulatory lexicon, but actual use of complexity models and results remains at an early stage. Recent insights a nd techniques offer potential for better monitoring and management of highly interconnec ted economic and financial systems and, thus, may help anticipate and manage future cris es. Battiston (2021) (pdf) 5.4 Holobiont Bardi Holobionts are non-hierarchical networks of entities that communicate with each other. It is a kind of holobiont that exists in nature, but it is not common. Think of a flock of birds foraging in a field. One bird sees something suspicious, it flies up, and in a moment all the birds are flying away. It is a chain reaction. The flock is endowed with a certain degree of intelligence. It can process a signal and act on it. In Nature, holobionts are not normally fully connected. Their connections are short-range, and signals travel more slowly through the network. It is often called “swarm intelligence” and it can be used to optimize systems. Swarm intelligence does transmit a signal, but it doesn’t amplify it out of control, as a fully connected network does, at least normally. It is a good control system: bacterial colonies and ant colonies use it. Our brains much more complicated: they have short range connections but also long range ones and probably also collective electromagnetic connections. One system that is nearly fully connected is the world wide web. Ideas (also called memes) flare up in the Web when they are stimulated it is the power of propaganda that affects everybody. It is an intelligent system because it can amplify a signal. That is that’s the way it reacts to an external perturbation. It can only flare up and then decline. It can’t be controlled. the problem with our modern propaganda system: it is dominated by memes flaring up out of control. The main actors in this flaring are those “supernodes” (the Media) that have a huge number of long-range connections. That can do a lot of damage: if the meme that goes out of control is an evil meme and it implies, say, going to war against someone, or exterminating someone. It happened and keeps happening again as long as the memesphere is organized the way it is, as a fully connected network. Memes just go out of control. All that means we are stuck with a memesphere that’s completely unable to manage complex systems. And yet, that’s the way the system works. It depends on these waves of out-of-control signals that sweep the web and then become accepted truths. Those who manage the propaganda system are very good at pushing the system to develop this kind of memetic waves, usually for the benefit of their employers. Can the memesphere be re-arranged in a more effective way – turning it into a good holobiont? Probably yes. Holobionts are evolutionary entities that nobody ever designed. They have been designed by trial and error as a result of the disappearance of the unfit. Holobionts do not strive for the best, they strive for the less bad. It may happen that the same evolutionary pressure will act on the human memesphere. The trick should consist in isolating the supernodes (the media) in such a way to reduce their evil influence on the Web. And, lo and behold, it may be happening: the great memesphere may be rearranging itself in the form of a more efficient, locally connected holobiont. Haven’t you heard of how many people say that they don’t watch TV anymore? Nor they open the links to the media on the Web. That’s exactly the idea. Do that, maybe you will start a chain reaction in which everyone will get rid of their TV. And the world will be much better. Bardi (2021) The Mousetrap Experiment: Modeling the Memesphere "],["medicine.html", "6 Medicine 6.1 Biobots 6.2 mRNA Vaccines", " 6 Medicine 6.1 Biobots Houser Tiny “biobots” made from human windpipe cells encouraged damaged neural tissue to repair itself in a lab experiment — potentially foreshadowing a future in which creations like this patrol our bodies, healing damage, delivering drugs, and more. The background: In a study published in 2020, researchers at Tufts University and the University of Vermont (UVM) harvested and incubated skin cells from frog embryos until they were tiny balls. They then sculpted the spheres into specific shapes — dictated by an algorithm — and added layers of cardiac stem cells to them in precise locations. When they were done, they had created “xenobots,” assembled from frog cells, that could move around and perform entirely new actions based on their designs — one kind of xenobot would push pellets around a petri dish, for example, while another would spin in circles. “These are novel living machines,” co-lead researcher Joshua Bongard said at the time. “They’re neither a traditional robot nor a known species of animal. It’s a new class of artifact: a living, programmable organism.” These biological robots might one day navigate the human body, delivering drugs, performing surgery, clearing plaques from artery walls, and more, the team hypothesized in its study. In 2021, the researchers updated their xenobots to be faster, have memories, and no longer require heart cells. But a major hurdle still stood between the group and its dream of using the biobots in medicine: they were made from frog cells. That meant the xenobots would likely trigger an immune response in a person, and while immunosuppression could prevent this, it leaves patients at high risk of infection. Anthrobots, assemble: Building on this xenobot research, Tufts scientists have now developed “anthrobots.” These biobots are derived from adult human cells, meaning a patient could use their own cells and avoid triggering an immune response. The biobots don’t reproduce and biodegrade after about 45-60 days in the lab, which would further reduce risks to patients. Anthrobots are also easier to make than their predecessors. “Anthrobots self-assemble in the lab dish,” said study co-author Gizem Gumuskaya. “Unlike xenobots, they don’t require tweezers or scalpels to give them shape, and we can use adult cells — even cells from elderly patients — instead of embryonic cells.” Houser (2023) Tiny biobots surprise their creators by healing wound 6.2 mRNA Vaccines Eddebo The covid mRNA vaccines can plausibly be connected, via several evident, separate, and likely synergistic causal mechanisms (rather than just through the epidemiological correlations), to heart disease, clotting and thrombotic events, cancer, immunity &amp; autoimmunity issues, neurological issues, reproductive problems, an immunological priming that creates susceptibility to future infections, and finally, they also engender a non-sterilizing immunity that promotes rapid viral evolution with problematic tendencies among the general population. The effects of these products are going to be somewhat erratic. We’re dealing with a complex cocktail of biologically active, synthetic substances that interact with the human body on several levels, and likely also in synergistic ways (i.e. the ingredients, when combined, bring about additional effects beyond that of the separate substances). This can be contrasted to something like insulin, which is a single substance familiar to the body that interacts with us in a predictable way when it is injected. With erratic and complex negative effects, it’s harder to establish clear and unambiguous correlations through empirical data since the signals get fuzzier. A treatment that cumulatively does many times more damage than a less harmful substance will be harder to recognize as unsafe if the negative effects 1) are individually moderate and 2) come through a wide variety of pathways, and 3) are slower to manifest. The vaccines involve at least three major kinds of foreign substances introduced into the body, each with complex effects on the human being - the synthetic nanolipids, the “preservatives” stabilizing the mRNA, as well as the specific mRNA payload itself and the spike protein it generates. This set of foreign substances enter the bloodstream and also accumulate in the liver, spleen, adrenals and the ovaries - following the straightforward and recommended intramuscular (and not intravenous injection. The lipid nanoparticles - LNPs - and their payload do not remain at the site of injection. Marc Girardot argues that they “trickle back” into the bloodstream via the lymphatic system even if they’re administered into muscle tissue, which is plausible. They “trickle back” into the bloodstream via the lymphatic system even if they’re administered into muscle tissue, which is plausible. They are nonetheless evidently distributed throughout the tissues in the body via the bloodstream or the lymphatic vessels, and they accumulate in certain important organs. You mainly find them in the blood and the filtering organs. The vaccine LNPs were not supposed to get into the bloodstream, and there’s now unambiguous, mainstream evidence that they generally do. The synthetic nanolipids (or lipid nanoparticles, LNPs) are basically small globs of partially synthetic fat that encapsulate the mRNA so it can be distributed to the ribosomes and produce the spike protein for immunization. In the covid vaccines, the LNPs consist of four separate types of fats or fat-like substances: cationic lipids, polyethylene glycol, phospholipids and cholesterol. The main problems of the LNPs or synthetic nanolipids are toxicity issues (mainly the cationic lipids), allergy issues (anaphylaxis from the polyethylene glycol), immune system dysregulation, and the fact that the LNPs and their payload tend to accumulate in the liver, spleen, adrenals and in the reproductive organs. Two of the compounds in the LNPs have a significant toxicity profile by themselves, and there’s a long list of potential chemical interactions that could promote further toxic effects in vivo This LNP delivery system is both quite new, and involves a lot of moving parts. The for-profit pharmaceutical industry first began to investigate them as a vehicle for drugs back in 2005, so there has not been much time to evaluate their effects. Due to the complexity of the compounds, proper evaluations would also have been both costly and technically difficult. The cytotoxic effects of the LNPs are still an important causal factor in the etiologies we see here, however. They are particularly important in connection to the cardiovascular system and the etiology of heart disease, clotting and thrombotic events. Transfection is the intended process by which the mRNA is introduced into the cell to generate the immunizing spike protein, but it’s also going to damage and eventually kill the host cell, specifically by necrosis or apoptosis. If this briefly happens in the muscle tissue at the site of inoculation, it’s no big deal even if the inflammation persists for some time. If it takes place in the endothelium it can trigger cardiovascular disease. The synthetic pseudouridine is used as part of the LNP as a sort of preservative. Its intended function is to stabilize the mRNA so that it doesn’t decay, and can be effectively transmitted into the ribosomes, which then produce the immunizing spike protein. The main issue with the N1-Methylpseudouridine is that it disrupts the mRNA translation process through something called frameshifting, causing the ribosomes to produce other things besides spike, while also destabilizing the proteins manufactured by the ribosomes. The specific modifications of the mRNA molecule through what’s known as “codon optimization” which serves to fine-tune the mRNA translation (so that maximum amounts of spike are produced in the ribosomes) also increase translation errors. The massive introduction of billions of LNPs with their codon-optimized payload and synthetic pseudouridine can trigger aberrant protein production and the misfolding of proteins. Prion-related diseases like Creutzfeldt-Jacob disease (“mad cow disease”), but also Alzheimer’s, Parkinson’s, certain forms of diabetes, MS, and many other conditions, are manifest through the cascading misfolding of prion proteins. Cascading means, in other words, that the mere presence of misfolded prions will cause further misfolding in other proteins, reproducing the problem. The mRNA translation problems thus provide a causal mechanism for various pathological conditions in addition to the basic inflammatory and cytotoxic issues we can connect to the LNP as such. It also explains the emergence of disease apart from and in addition to the mechanisms described by the bolus theory, i.e. the LNPs can through translation problems trigger various sorts of illness even without any significant local inflammation and cytotoxicity. The potential consequences of a large-scale introduction of spike protein are an obvious explanation for the unprecedented number of adverse neurological events being reported in pharmacovigilance databases such as VAERS in connection to the covid vaccines. Spike protein is toxic to marine animals with potential ecological harm from contaminated wastewater, and the damage seen in the animals is wide-ranging. Spike protein also, through this signal triggering, seems to play a direct and independent role in the emergence of clotting disorders, thrombosis, pulmonary damage and neurogenerative disorders. The disturbance of DNA repair mechanisms is one of the central causal factors behind cancer, cellular aging, and a long list of catastrophic syndromes. That the spike protein “significantly” disturbs DNA repair mechanisms in general, is very much a cause for alarm. The mRNA covid vaccines generate a certain form of immune system suppression and dysregulation. There’s a substitution in the vaccinated of virus-neutralizing antibodies for non-inflammatory ones, a “class switch” from antibodies that work towards clearing the virus from our system, to a category of antibodies (IgG4), one of whose purposes is to desensitize us to irritants and allergens. One of their important roles is to render you “immune” to allergens, so that they do not trigger an unneccessary inflammatory response. When this happens in relation to a viral pathogen like SARS-CoV-2, the unhelpful result is rather that the virus can remain in the body and its tissues and keep replicating. The reported increase in IgG4 levels detected after repeated vaccination with the mRNA vaccines may not be a protective mechanism; rather, it constitutes an immune tolerance mechanism to the spike protein that could promote unopposed SARS-CoV2 infection and replication by suppressing natural antiviral responses. It’s therefore plausible that multiply vaccinated individuals will tend towards a situation of long-term, repeat infections that do not get cleared, and which promote systemic damage. Combine this with the extensive data on OAS/the Hoskins effect, and we potentially get two separate avenues for immune suppression. I.e. the first one would be the Hoskins effect/antigenic original sin where immunity fixates on the narrow vaccination, the SP from the classic “Wuhan strain” (rather than to the 29 proteins of the entire virus, which would engender a more robust immunity that minor mutations in the virus could not easily get around), and the second one would be in terms of this IgG4 substitution. Both of these separate avenues could then independently undermine the capability of the multiply vaccinated to clear not only covid infections but increasingly also other viral infections which may opportunistically evolve to exploit this immunity gap created by substitution and an overabundance of antibodies that do not clear viruses. This then potentially results in these “invisible”, low-intensity infections which elicit a weak inflammatory response - yet which crowd the body with viruses and promote systemic damage. This situation manifests with a whole set of potential long-term complications of its own. Conclusions There are so many modalities of possible harm here that it’s almost preposterous. We’re not just talking about a couple of elevated risks, but a whole plethora of significant causal mechanisms, with several factors synergistically that can be connected to every single potential negative health outcome that can be discerned in the research. And paradoxically, this overwhelming set of indications are part of the problem. It immediately generates cognitive dissonance. Because on the face of it, it’s not reasonable to entertain the idea that an ostensibly beneficial pharmaceutical product that almost everyone agrees has saved millions of lives, is associated with such an extensive set of etiological mechanisms. Eddebo (2023) On the mRNA vaccines’ pathways for damage "],["science.html", "7 Science 7.1 Reticence and Gradualism 7.2 Dualism 7.3 Reductionism 7.4 Enligthenment 7.5 Scale 7.6 Is science becoming less disruptive? 7.7 Is Economics a Science 7.8 Science in at Networked Economy 7.9 AI - Private Science", " 7 Science Reductionism has become a disease, a viewpoint lacking both intellectual sophistication and emotional depth - blighting our ability to understand what is happening and what to do about it. There are four main pathways to the truth: science, reason, intuition and imagination. Any world wiew that tries to get by without paying due respect to all four is bound to fail (Iain McGilChrist) A paradigm is an internally coherent system of thought that results in useful insights but also finds it difficult to escape its own assumptions. 7.1 Reticence and Gradualism Hansen Scientific reticence, in part, may be a consequence of the scientific method, which is fueled by objective skepticism. Another factor that contributes to irrational reticence among rational scientists is “delay discounting,” a preference for immediate over delayed rewards. The penalty for “crying wolf” is immediate, while the danger of being blamed for having “fiddled while Rome was burning” is distant. Also, larding of papers and research proposals with caveats and uncertainties increases chances of obtaining research support. “Gradualism” that results from reticence seems to be comfortable and well-suited for maintaining long-term support. Reticence and gradualism reach a new level with the Intergovernmental Panel on Climate Change (IPCC). The prime example is IPCC’s history in evaluating climate sensitivity, the most basic measure of climate change, as summarized in our present paper. IPCC reports must be approved by UN-assembled governments, but that constraint should not dictate reticence and gradualism. Climate science clearly reveals the threat of being too late. “Being too late” refers not only to assessment of the climate threat, but also to technical advice on the implications of climate science for policy. Are not we as scientists complicit if we allow reticence and comfort to obfuscate our description of the climate situation and its implications? Does our training – years of graduate study and decades of experience – not make us the best-equipped to advise the public on the climate situation and its implications for policy? As professionals with the deepest understanding of planetary change and as guardians of young people and their future, do we not have an obligation, analogous to the code of ethics of medical professionals, to render to the public our full and unencumbered diagnosis and its implications? That is our aim here. Hansen (2023) PipelinePaper230705 (pdf) 7.2 Dualism If science is the combination of logical definitions with empirical observations, theology is science without the observation. It is a web of human definitions, and nothing more. Although it makes for abysmal science, the theological stance is a great way to sound authoritative while you spout bullshit. The self-styled libertarian ‘philosopher’ Ayn Rand was a master of this technique. Here she is laying out her philosophy of objectivism: The formula defining the concept of existence and the rule of all knowledge: A is A. A thing is itself. You have never grasped the meaning of [this] statement. I am here to complete it: Existence is Identity, Consciousness is Identification. Rand’s writing is a tour-de-force in theological nonsense. Start with a tautology (A = A). Follow it up with some weighty definitions (existence = identity). Claim you have a ‘rule for all knowledge’. Marvelous bullshit. Fix (2022) Dualism in Science, Theology, and Economics 7.3 Reductionism Austin The Fallacy of Reductionism is that reductionism is always the best way to investigate the world, and that more knowledge will always be gleaned from breaking phenomena into parts than contemplating them whole. The Trap of Reductionism has been that because reductionism was so successful in explaining the dead things upon which the Scientific Revolution was founded – planets, light, falling objects etc. – we were insufficiently sceptical about its adequacy as a method to investigate more complex, living things. Yet, we proceeded anyway with the unfortunate consequence that we have delayed, and sometimes derailed, our comprehension of many living things, including ourselves. The trap can also be envisaged as a valley. Hence, Christopher Tape, a biologist, depicts the broad history of biological enquiry as being a ‘downward’ journey of reductionism, now transitioning into an ‘upward’ journey to discoveremergent properties that cannot be gleaned from knowledge of the parts. (Note the appearance of ‘systems’ and ‘life’ on the right-hand side of the Figure) Similar trajectories could be drawn for many other natural and social sciences, all ‘pulled down’ to a reductionist vision. The point is not that this has not yielded considerable benefit, only that it promoted a method of investigation whose cost was the postponement of our comprehension of emergent properties. As that recognition has sunk in, many disciplines have recognized they have reached the point of diminishing returns to reductionism. Encouragingly, there is now a ‘Systemic Spring’ underway in which multiple disciplines are racing to incorporate the insights of complexity thinking into their subjects, with the natural and social sciences having the most to gain. Indeed, in general terms, the more ‘complex’ the object of a discipline’s enquiry, the more there is to gain – or, equivalently, the more that the reductionist world view has inadvertently held back. As Brian Arthur, one of the key proponents of complexity in the field of economics, recently expressed it: ‘complexity is not a science, rather it is a movement within science.’ Werner Heisenberg, one of the key pioneers of quantum mechanics, sensed how deeply our penchant to divide had sunk and how difficult it would be to root out: ‘The Cartesian partition has penetrated deeply into the human mind during the three centuries following Descartes and it will take a long time for it to be replaced by a really different attitude toward the problem of reality.’ What is genuinely exciting about systemic science is that it introduces a rigorous way of seeing relation over part. Gregory Bateson, one of the most prominent system thinkers of the 20th Century, who tried to lean against the reductionist tide in the social sciences when it was at its height, said: ‘The major problems in the world are the result of the difference between how nature works and the way people think.’ Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis 7.4 Enligthenment Intriguingly, we use the term Enlightenment for two major cognitive developments: the Western Enlightenment of the 17th Century and the Buddhist Enlightenment of 5th Century BC. They offer strikingly divergent recommendations about how to be in the world. Western Enlightenment is about using reason for human progress, crystallized in a quantitative-based scientific method. Buddhist Enlightenment intuited that human striving is the source of unhappiness and that progress has treadmill or trap-like characteristics. The Western Enlightenment, right from its Baconian outset extolled an extractive attitude towards Nature – ‘let the human race recover that right over Nature which belongs to it by divine bequest’. It is a worldview that leads one eventually to describe Nature as an ‘asset’ and as ‘natural capital’. Buddhism does not even recognize Nature as separate. Possibly the time is ripe for some Third Enlightenment – a meta-Enlightenment?! – which might reconcile why it is that humankind already hastwo major Enlightenments on the books. Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis 7.5 Scale Chu Significance The size of scientific fields may impede the rise of new ideas. Examining 1.8 billion citations among 90 million papers across 241 subjects, we find a deluge of papers does not lead to turnover of central ideas in a field, but rather to ossification of canon. Scholars in fields where many papers are published annually face difficulty getting published, read, and cited unless their work references already widely cited articles. New papers containing potentially important contributions cannot garner field-wide attention through gradual processes of diffusion. These findings suggest fundamental progress may be stymied if quantitative growth of scientific endeavors—in number of scientists, institutes, and papers—is not balanced by structures fostering disruptive scholarship and focusing attention on novel ideas. Chu Abstract In many academic fields, the number of papers published each year has increased significantly over time. Policy measures aim to increase the quantity of scientists, research funding, and scientific output, which is measured by the number of papers produced. These quantitative metrics determine the career trajectories of scholars and evaluations of academic departments, institutions, and nations. Whether and how these increases in the numbers of scientists and papers translate into advances in knowledge is unclear, however. Here, we first lay out a theoretical argument for why too many papers published each year in a field can lead to stagnation rather than advance. The deluge of new papers may deprive reviewers and readers the cognitive slack required to fully recognize and understand novel ideas. Competition among many new ideas may prevent the gradual accumulation of focused attention on a promising new idea. Then, we show data supporting the predictions of this theory. When the number of papers published per year in a scientific field grows large, citations flow disproportionately to already well-cited papers; the list of most-cited papers ossifies; new papers are unlikely to ever become highly cited, and when they do, it is not through a gradual, cumulative process of attention gathering; and newly published papers become unlikely to disrupt existing work. These findings suggest that the progress of large scientific fields may be slowed, trapped in existing canon. Policy measures shifting how scientific work is produced, disseminated, consumed, and rewarded may be called for to push fields into new, more fertile areas of study. Chu (2021) Slowed canonical progress in large fields of science (pdf) Eugyppius on Chu At scale, the scientific enterprise rapidly becomes a kind of intramural spectator sport, with the vast majority of “scientists” reduced to passively observing the dialogue unfolding among higher-ups within their own field, while most of their own work – undertaken for careerist purposes – goes unread and unnoticed. Eugyppius (2022) The larger a scientific field, the more conformist that field becomes, and the more lethargic its progress 7.6 Is science becoming less disruptive? Smith One of the most worrying trends in economics is that we keep spending more and more on research, but our productivity growth doesn’t seem to speed up much. The most important paper showing this is Bloom et al. (2020) Are Ideas Getting Harder to Find?. Technological progress is not just the ultimate source of all our material wealth, it’s our most effective weapon against existential challenges like climate change. So the prospect that progress might simply get too expensive for our society to sustain is fairly scary. There are various hypotheses for why research is getting less productive on average. One is the “low-hanging fruit” hypothesis — the idea that there are a certain number of relatively easy, relatively high-impact discoveries to be made, and that we’ve already made most of these. There’s only one periodic table, one structure of DNA, and one Schrödinger Equation, just like you can only invent electric power, the internal combustion engine, semiconductors and sewage treatment once. The alternative “burden of knowledge” hypothesis is that there’s plenty of new big stuff left to discover, but scientists just don’t have enough time in their lives to discover it, because the corpus of existing knowledge keeps on increasing, requiring them to spend more and more of their productive lifetimes simply getting up to speed. There are other explanations too, and it’s important to note that these hypotheses aren’t mutually exclusive. Anyway, a very interesting paper just came out in Nature that offers both new evidence and a new perspective on this debate. In “Papers and patents are becoming less disruptive over time”, authors Park, Leahey, &amp; Funk argue that game-changing scientific research is becoming less common. We find that papers and patents are increasingly less likely to break with the past in ways that push science and technology in new directions. Our results suggest that slowing rates of disruption may reflect a fundamental shift in the nature of science and technology. The way Park et al. measure the “disruptiveness” of a paper is, basically, how much of a break in the literature it causes. If a paper really changes the state of our knowledge, the reasoning goes, then subsequent papers in the field will tend not to cite research that was done before the breakthrough. If you have Newton, you don’t need Aristotle. So the authors measure a paper’s disruptiveness with something called a CD index, which represents the likelihood that later work that cites that paper will also cite things that the paper itself cites. They argue that the similarity in the decline between different fields is evidence against the idea that we’re picking “low-hanging fruit”, because we wouldn’t expect the easy discoveries to be mined out at similar rates in different fields. An alternative explanation is that academia and patent law have both changed so as to encourage an increasing amount of crappy useless research — as a signaling mechanism for academic jobs, and as a way of patent-trolling or defensive patenting in industry. That could indicate that we’re still finding all the good stuff, but that we’re also paying people to do a lot of useless crap on top of that. The authors’ preferred explanation for the decline in their measure of disruptiveness is that researchers are using “narrower portions” of existing knowledge. As evidence, they show that citation diversity is down across fields — in other words, more and more, papers are citing the same stuff. Comment by Sorcelators I really disagree with this “low-hanging fruit” idea. I used to work as a theoretical physicist at a major university until around ten years ago, and the problem I saw then was certainly not a lack of good ideas to work on. I had far more good ideas than I knew what to do with, and so did most of my colleagues. The problem is that academia is just a shithole–poor funding, time increasingly consumed by administrative tasks, micromanagement of every activity, and absolutely insane decisions being made at really every turn I just don’t honestly see how anyone can look at a situation like that and say “yeah the problem is obviously that there are no good ideas left.” The problem is that academia is oppressive, frustrating, and demoralizing, and it just isn’t possible to consistently do good work in an environment like that. Pick up, say, one of Feynman’s autobiographies and compare his academic experiences to a modern university. Smith (2022) Three economics happenings of note Park Park (2022) Papers and patents are becoming less disruptive over time (pdf) 7.7 Is Economics a Science Roberts For me, economics is a science – if a social science dealing with humans, not a physical science. As a science, it requires scientific method. For me, that means you start with a hypothesis that has realistic assumptions that have been ‘abstracted’ from reality and then construct a model or set of laws that can be tested against evidence. The model can use mathematics to refine its precision, but eventually the evidence decides. In my view, like physicists and astronomers, economists too must be able to develop theories about the economies in the real world and test them empirically so that we can make predictions and hopefully avoid the economic crises that modern economies have on a regular basis. Benoit Cœuré in his Paris lecture dismissed the criticism that economists failed to predict the outbreak of the financial crisis. “This criticism is nonsense. Do we expect physicians to predict illnesses? We don’t, of course. But we expect them to help us cure illnesses. Economists should do the same.” So it’s not the job of economics to forecast or predict but to develop policies to cure any messes that emerge. Another recent Nobel prize winner, Esther Duflo, reckoned economists should give up on the big ideas and instead just solve problems like plumbers “lay the pipes and fix the leaks”. Economists were more like engineers than physicists. Keynes made a similar point: that economists should be like dentists – sorting out troublesome teething problems so that capitalism can then run smoothly. Duflo reckons the analogy of plumbers means that pure scientific method of analysing cause and effect was less important than practical fixes. So economists should be more like doctors than medical researchers. Plumbers, dentists, engineers, doctors – but not, it seems, social scientists. In my view, real world economics must look at the ‘big picture’. Economists should not be just doctors but social scientists, or more accurately they should develop an economics that recognises the wider social forces that drive economic models. That is called political economy, mostly not taught in universities. The most prestigious mainstream Quarterly Journal of Economics, currently the most-cited journal in the field of economics, has never published an article on climate change! I finish by saying to you all: remember that there is a world out there beyond supply and demand curves and mathematical formulae. Economics and economists should not be sucked into just being like dentists fixing teeth, but also use their skills and the scientific method to understand the big picture and so help to make a better world for all. Roberts (2023) Why real-world economics matters 7.8 Science in at Networked Economy Thompson What is HAPPENING here? Between 2012 and 2023, it appears that average OECD reading, math and science scores have declined consistently. This is not (just) about pandemic learning loss. It’s not about one US city. It’s not even just the US. This is the entire developed world. Comments In a network economy your connections count not your knowledge. Thompson (2023) tweet 7.9 AI - Private Science Science morphes into private business "],["evolution.html", "8 Evolution 8.1 Organization of Life 8.2 Intentional Evolution 8.3 Metabolic Scaling 8.4 Hiarchies", " 8 Evolution 8.1 Organization of Life The conflict between lower-level selfishness and higher-level welfare pervades the biological world. Cancer cells selfishly spread at the expense of other cells within the body, without contributing to the common good, ultimately resulting in the death of the whole organism. In many animal societies, the dominant individuals act more like tyrants than wise leaders, taking as much as they can for themselves until deposed by the next tyrant. Single species can ravage entire ecosystems for nobody’s benefit but their own. But goodness has its own advantages, especially when those who behave for the good of their groups are able to band together and avoid the depredations of the selfish. Punishment is also a powerful weapon against selfishness, although it is often costly to wield. Every once in a great while, the good manage to decisively suppress selfishness within their ranks. Then something extraordinary happens. The group becomes a higher-level organism. Nucleated cells did not evolve by small mutational steps from bacterial cells but as groups of cooperating bacteria. Likewise, multi-cellular organisms are groups of highly cooperative cells, and the insects of social insect colonies, while physically separate, coordinate their activities so well that they qualify as super-organisms. Life itself might have originated as groups of cooperating molecular reactions. Only recently have scientists begun to realize that human evolution represents a similar transition. In most primate species, members of groups cooperate to a degree but are also each other’s main rivals. Our ancestors evolved to suppress self-serving behaviors that are destructive for the group, at least for the most part, so that the main way to succeed was as a group. Teamwork became the signature adaptation of our species. Extant hunter-gatherer societies still reflect the kind of teamwork that existed among our ancestors for thousands of generations. Individuals cannot achieve high status by throwing their weight around but only by cultivating a good reputation among their peers. Most of human moral psychology – including its other-oriented elements such as solidarity, love, trust, empathy, and sympathy, and its coercive elements such as social norms enforced by punishment – can be understood as products of genetic evolution operating among groups, favoring those that exhibited the greatest teamwork. From Genes to Culture Teamwork in our ancestors included physical activities such as childcare, hunting and gathering, and offense and defense against other groups. Human teamwork also acquired a mental dimension including an ability to transmit learned information across generations that surpasses any other species. This enabled our ancestors to adapt to their environments much more quickly than by the slow process of genetic evolution. They spread over the globe, occupying all climatic zones and hundreds of ecological niches. The diversity of human cultures is the cultural equivalent of the major genetic adaptive radiations in dinosaurs, birds, and mammals. The invention of agriculture initiated a positive feedback process between population size and the ability to produce food leading to the mega-societies of today. Cultural evolution differs from genetic evolution in important respects but not in the problem that lurks at every rung of the social ladder. Just like genetic traits, cultural traits can spread by benefitting lower-level units at the expense of the higher-level good – or by contributing to the higher-level good. There can be cultural cancers, no less so than genetic cancers. And for teamwork to exist at any given rung of the social ladder, there must be mechanisms that hold the wolves of selfishness at bay. A nation or the global village is no different in this respect than a human village, a hunter-gatherer group, an ant colony, a multi-cellular organism, or a nucleated cell. Modern nations differ greatly in how well they function at the national scale. Some manage their affairs efficiently for the benefit of all their citizens. They qualify at least as crude superorganisms. Other nations are as dysfunctional as a cancer-ridden patient or an ecosystem ravaged by a single species. Whatever teamwork exists is at a smaller scale, such as a group of elites exploiting the nation for its own benefit. The nations that work have safeguards that prevent exploitation from within, like scaled-up villages. The nations that don’t work will probably never work unless similar safeguards are implemented. Accomplishing teamwork at the level of a nation is hard enough, but it isn’t good enough because there is one more rung in the social ladder. Although many nations have a long way to go before they serve their own citizens well, a nation can be as good as gold to its own citizens and still be a selfish member of the global village. In fact, there are many examples in the international arena, where nations protect their own perceived interests at expense of the common global future. Wilson and Hessen 8.2 Intentional Evolution The Evolutionary Manifesto Evolutionary Manifesto Cooperative Evolutionary Transitions Abstract Stewart Major Cooperative Evolutionary Transitions occur when smaller-scale entities cooperate together to give rise to larger-scale entities that evolve and adapt as coherent wholes. Key examples of cooperative transitions are the emergence of the complex eukaryote cell from communities of simpler cells, the transition from eukaryote cells to multicellular organisms, and the organization of humans into complex, modern societies. A number of attempts have been made to develop a general theory of the major cooperative transitions. This paper begins by critiquing key aspects of these previous attempts. Largely, these attempts comprise poorly-integrated collections of separate models that were each originally developed to explain particular transitions. In contrast, this paper sets out to identify processes that are common to all cooperative transitions. It develops an alternative theoretical framework known as Management Theory. This general framework suggests that all major cooperative transitions are the result of the emergence of powerful, evolvable ‘managers’ that derive benefit from using their power to organize smaller-scale entities into larger-scale cooperatives. Management Theory is a contribution to the development of a general, “all levels” understanding of major cooperative transitions that is capable of identifying those features that are level-specific, those that are common across levels and those that are involved in trends across levels. Stewart: General Theory 8.3 Metabolic Scaling Hatton Metabolic scaling theory has had a profound influence on ecology, but the core links between species characteristics have not been formally tested across the full domain to which the theory claims to apply. We compiled datasets spanning all eukaryotes for the foremost body mass scaling laws: metabolism, abundance, growth, and mortality. We show that metabolism and abundance scaling only follow the canonical ±3/4 slopes within some taxonomic groups, but across eukaryotes reveal reciprocal near ±1 slopes, broadly supporting the “energetic equivalence rule.” In contrast to metabolism, growth follows consistent ∼3/4 scaling within many groups and across all eukaryotes. Our findings are incompatible with a metabolic basis for growth scaling and instead point to growth dynamics as foundational to biological scaling. Scaling laws relating body mass to species characteristics are among the most universal quantitative patterns in biology. Within major taxonomic groups, the 4 key ecological variables of metabolism, abundance, growth, and mortality are often well described by power laws with exponents near 3/4 or related to that value, a commonality often attributed to biophysical constraints on metabolism. However, metabolic scaling theories remain widely debated, and the links among the 4 variables have never been formally tested across the full domain of eukaryote life, to which prevailing theory applies. Here we present datasets of unprecedented scope to examine these 4 scaling laws across all eukaryotes and link them to test whether their combinations support theoretical expectations. We find that metabolism and abundance scale with body size in a remarkably reciprocal fashion, with exponents near ±3/4 within groups, as expected from metabolic theory, but with exponents near ±1 across all groups. This reciprocal scaling supports “energetic equivalence” across eukaryotes, which hypothesizes that the partitioning of energy in space across species does not vary significantly with body size. In contrast, growth and mortality rates scale similarly both within and across groups, with exponents of ±1/4. These findings are inconsistent with a metabolic basis for growth and mortality scaling across eukaryotes. We propose that rather than limiting growth, metabolism adjusts to the needs of growth within major groups, and that growth dynamics may offer a viable theoretical basis to biological scaling. Hatton (2019) Linking Scaling Laws across Eukaryotes (pdf) Blair Fix (twitter) 8.4 Hiarchies Turchin One of the greatest puzzles of human evolutionary history concerns the how and why of the transition from small-scale, ‘simple’ socie- ties to large-scale, hierarchically complex ones. Multilevel selection suggests that complex hierarchies can arise in response to selection imposed by intergroup conflict. People living in small-scale societies are fiercely egalitarian and use a variety of ’leveling institutions to reduce inequality. Complex societies are vastly inegalitarian. Small-scale societies have simple structure. Thus, local communities may be grouped in larger units (‘tribes’), but usually there are no levels of organization above that, and there are no permanent control centers. Complex societies, on the other hand, are centralized and have many levels of hierarchical organization. Complex societies have states – coercion-wielding hierarchical organizations managed by administrative specialists (bureaucracies). There is a strong correlation between hierarchical complexity and state organization. Multilevel Selection The theory of multilevel selection provides insights into the evolu- tion of such traits as altruism that are subject to conflicting selec- tion pressures. In the pithy characterization of D. S. Wilson and E. O. Wilson (2007), ‘Selfishness beats altruism within groups. Altruistic groups beat selfish groups’. Whether altruism spreads in the population, or not, depends on the balance of within-group (in- dividual level) and between-group (higher level) selection forces. Human groups need to be well-integrated by within- group cooperation in order to effectively compete against other groups. There are biological limits on the size of an egalitarian group, in which the basis of cooperation is face-to-face interactions. The main limit has to do with the size of the human brain. According to the ‘social brain’ hypothesis the evolution of human brain size and intelligence during the Pleistocene was largely driven by selective forces arising from intense competition between individuals for increased social and reproductive success. One can view language as a tool that originally emerged for simplifying the formation and improving the efficiency of coalitions and alliances. The huge and energetically demanding brains of humans, ac- cording to this theory, evolved in order to store and process large amounts of social data. To function well in a social group an indi- vidual needs to remember who did favors for whom and, alterna- tively, who cheated whom. One must be able to calculate the po- tential ramifications of one’s actions towards another individual and how it will affect the relationships with third parties. The prob- lem is, as the group increases in size, the potential number of rela- tionships that one must keep in mind grows exponentially. Once a human group attains the size of roughly 150 individuals, even the hypertrophied human brain becomes overwhelmed. For group size to increase beyond the few hundred individuals typical of small-scale human societies, evolution had to break through the barriers imposed by face-to-face sociality. Humans evolved the capacity to demarcate group membership with symbolic markers. Markers such as dialect and language, clothing, ornamentation, and religion allowed humans to determine whether someone personally unknown to them was a member of their cooperating group or, vice versa, an alien and therefore an enemy. The second evolutionary innovation was hierarchical organization. The elementary building block for hierarchical organizations is a bond between a superior and an inferior ‘agents’. If agents are individual humans, then this relationship takes the form of one between a leader and a follower, or a lord and a vassal. The growth of hierarchies occurs primarily by adding extra levels of organization and, therefore, is not limited by social channel capacity. Any member of a hierarchy needs to have a face-to-face relationship only with, at most, \\(n + 1\\) persons: the maximum number of subordi- nates (the ‘span of control’), \\(n\\), plus an additional link to its own superior. Hierarchical societies are also not limited by social channel capacity, and can potentially reach any size, as long as it is possible to add new hierarchical levels. An acephalous tribe is the largest social scale a human group can achieve without the benefit of centralized organization. Greater social complexity requires leaders – chiefs, kings, caliphs, presidents, prime ministers, or politburo chairmen. Adding extra levels of social organization beyond a complex chiefdom usually requires transition to a more formal political or- ganization – the state. The state ischaracterized by a formal division of labor: separate organizations specializing in administration (a bureaucracy), coer- cion (an army), law (a judiciary), and religion (a church). Large-scale hierarchically complex societies arose as a result of evolutionary pressures brought on by warfare. As Charles Tilly (1975) famously said, ‘states made war, and war made states’ The key insight from evolutionary theory (and, specifically, multilevel selection) is that warfare is an extreme form of parochial altruism, driven by the ‘cooperate to compete’ evolutionary logic. Up until the present the force driving the evolution of increased social scale has always been competition/conflict in opposition to some other societies. If the global state were to arise, where will it find the external threat that would keep it unified? Unless (or until) the humanity experiences a major evolutionary breakthrough that will provide a different basis for large-scale cooperation, the rise of a stable state unifying all humanity is unlikely. The history of the European Union (EU), a most audacious and innovative experiment in building a supranational community, ap- pears to support this pessimistic conclusion. Two reasons for the apparent reversal of the integrative dynamic in Europe. First, adversarial relations with the Soviet block (the ‘Evil Empire’) helped to suppress internal bickering among the member states. Second, rapid expansion into Central and Eastern Europe, by simply adding new members in a completely unstructured way, was clearly a mistake, as lasting increase in social scale can be accomplished only by adding extra layers of hierarchical organization. Without an international authority possessing sufficient coercive power to hold individual states in check, great powers will continue their attempts to gain power at each other’s expense leading, inevitably, to interstate rivalry and war. All through the history, and for the foreseeable future, integration among humans required conflict against other humans. Even if a world-wide state were to arise, according to this logic, it would rapidly fission into multiple parts. On the other hand, neither history nor evolution is destiny. Humans have transcended their evolutionary limitations before. Turchin: Evolution of Complex Hierarchical Societies (pdf) "],["philosophy.html", "9 Philosophy 9.1 Cartesian Disconnect", " 9 Philosophy Bo Harvey Society is like a game which is not centrally directed. If it were, society would be more like ‘an army’ than a game. Laws and government are there to ensure the game does not break down. This ‘ordinary language philosophy’ expects dissolution of philosophical problems. (Rather than solution, for these were not real problems). If philosophical problems are understood to be conceptual, and all conceptual issues have to do with linguistic custom, close enough analysis of linguistic custom and usage entails their dissolution. There is nothing in the world itself that is problematical—much less contradictory. Trouble arises in our way of speaking and use of concepts, and the job of the philosophy is to ensure proper use to, ‘ensure the identity and stability of the system, by preventing unorthodox moves within it.’ The true origins of philosophical problems, in other words, exist at the level of logical rather than historical genesis. An increased faith in philosophy’s ability to change the world was constructed in the wake of political philosophy’s Rawlsian rebirth— much akin to the increased philosophical faith in the disappearance of philosophical problems by ordinary language philosophy. Rawlsians emphasized consensus-seeking deliberation, legalistic formulations of political problems, and a focus on individual ethics in the face of the horrors of history; but they largely did not address questions regarding the role of conflict in constituting social relations, notions of power and the substantive theorisation of the state, the relation of capitalism to democracy, and the role of the history of racism and colonialism Exactly what is ‘ordinary’ about the concept of ‘ordinary’ internal to ‘ordinary language philosophy’ might have been more obvious to its originators at Oxford than it was to anyone else. Though perhaps one can see how this ordinariness might be contrasted with the comparatively unordinary babble of foreigners—particularly if they are French. At almost the same time as the publication of Rawls’ Theory of Justice in 1971, new intellectual influences—precisely those denaturalising, anti-essentialising, and particularising ones—entered Anglo-American academies. Humanities and social science disciplines in Anglo-American universities were in many ways wholly transformed following the reception of mid-century French and German thought, despite—or, perhaps, precisely because of—the fact that this transformation took place on theoretical bases that bore little relation to the particular histories of the different disciplines which received them. The name for this transformation has been retrospectively called ‘Theory’, but has also been termed, with varying degrees of relevance, accuracy, and redundancy, ‘critical theory’, ‘postmodernism’, ‘Western Marxism’, ‘critical Marxism’, and, now more notoriously, ‘cultural Marxism’. These are of course some of the denaturalizing, anti-essentialising, and particularising forces that uniquely under-affected political philosophy, yet out of these influences arose a conceptual vocabulary that could be used to think through much of what Rawls is now seen to have left out. The widespread influence of Foucault in cultural studies, sociology, intellectual history, literary theory, and anthropology (but not liberal egalitarian political philosophy)—particularly in terms of his theorisation of power—is but one paradigmatic example. ‘Theory’, of course, resembles philosophy insofar as philosophy is construed as a kind of meta-discipline. At the same time, it is self-evidently separate from disciplinary philosophy. This is the case even though much of what informs ‘Theory’ is itself often predicated upon critiques of what was understood as philosophy as itself a self-sufficient discipline. It can be easy to forget that the received divisions between intellectual disciplines is largely a nineteenth-century institutional phenomenon, and whether one locates their origins to the beginnings of the ‘classical age’, the medieval university, or the European Enlightenment, the history of intellectual disciplines is more discontinuous than the stories they tell themselves about themselves. Classical political economy ‘forgets’ the inequality subtending all bourgeois equality. In the Rawlsian social-contractual tradition, the ‘original position’ qua thought-experiment replaces the fictitious primordial condition of equal exchange, but its function is the same. In the first instance, theoretical analysis is replaced by imaginary history. In the second instance, theoretical analysis is replaced by a game. Through this lens, the ‘ideological function’ of political philosophy writ large becomes one of obfuscation. ‘We need a Marxist political philosophy’ is a thought that has spawned countless urgent re-thinkings, but one wonders whether Marx—that critic of philosophy who nevertheless saw the need to critically reflect on its forms—would have thought ‘Marxist political philosophy’ a contradiction in terms. Harvey (2023) The Future of Political Philosophy 9.1 Cartesian Disconnect Eddebo I think that modern philosophy’s almost feverish quest to find its way back to reality and get out of the dreamlike shadow world of skepticism is in many ways an expression of trauma. That this fixation on one’s own self as cut off from the outside world, and the corresponding approach to the external world around us as a pile of dead, abstract and replaceable objects that we can plunder and subdue as best we like, I think we have to consider all of this as a set of unhealthy coping strategies that have emerged in a culture marked by generations of trauma. Trauma that has to do with the enormous upheavals connected to modernity in all its aspects, not least industrialization and the radical changes in production conditions and relationships between people, and between people and the outside world, that industrialization brought about. But this disconnect is thus in many ways the basic issue of modern philosophy. And if we raise our perspective a little bit, we see a distinct pattern of thought from the late Middle Ages until today that is characterized by this rift between the self and the world. Historically, this movement runs from nominalism in the struggle of universals, then via the Reformation, Bacon and Descartes to Hegel, Kant, empiricism, rationalism, and out of this ideological muddle, perhaps mainly from French utopian socialism and British political economy, then grows the modern ideologies, which then end up in an historical end point in logical positivism and existentialism as a kind of diametrically opposed set of answers to this basic problem with the world outside the self - in the end we then come all the way to secularization, to the radical dissolution of postmodernism and whatever we’re supposed to call this unique condition of our time. John Zerzan, the American primitivist thinker, writes the following: Since the Neolithic, there has been a steadily increasing dependence on technology, civilization’s material culture. As Horkheimer and Adorno pointed out, the history of civilization is the history of renunciation. One gets less than one puts in. This is the fraud of technoculture, and the hidden core of domestication: the growing impoverishment - of self, society, and Earth. Meanwhile, modern subjects hope that somehow the promise of yet more modernity will heal the wounds that afflict them. A defining feature of the present world is built-in disaster, now announcing itself on a daily basis. But the crisis facing the biosphere is arguably less noticeable and compelling, in the First World at least, than everyday alienation, despair, and entrapment in a routinized, meaningless control grid. Influence over even the smallest event or circumstance drains steadily away, as global systems of production and exchange destroy local particularity, distinctiveness, and custom. Gone is an earlier pre-eminence of place, increasingly replaced by what Pico Ayer calls “airport culture” — rootless, urban, and homogenized. Descartes’ entire philosophy is, in a sense, one big coping project relating to man’s existential vulnerability in a time where all traditional relationships are seriously beginning to get torn apart, when the great upheavals of modernity are taking a more concrete form - even if Descartes’ thinking still confirmed reproduced this disintegration in his radical distinction between body and soul. And this obstacle that Descartes formulated and then tried to get around is thus, in various ways, taken for granted as a starting point for basically the entire philosophical and scientific work of the modern period that follows. Hegel, he also clearly sees this civilizational trauma and tries to come to terms with the dichotomies, but then also takes the disintegration for granted. He tries to put the body, the world and the soul back together with the help of his dialectical method. And on this road it is. Immanuel Kant tries to find his way back to the thing in itself, to the unadulterated reality. Durkheim builds the science of sociology around the analysis of anomie and perceived rootlessness. Freud emphasizes how civilization is characterized by a radical “discontent with culture”. We have Kierkegaard weaving his philosophy around the concept of anxiety. Sartre’s existential desperation and disgust at the experience of being. And Marx, not least Marx, who builds most of his political and economic theory around alienation, around the condition of being alienated from oneself, one’s surroundings, and the fruits of one’s work. Modernity is characterized by separation, and reproduces this separation in our experience of reality and ourselves. Eddebo (2023) Beyond Descartes and Private Subjectivity "],["religion.html", "10 Religion", " 10 Religion Taleb Ironically, modernists fall for what I have called the opiate of the middle classes[i.e., the PMC], that is social science and stock market speculation. They refuse religion on rational grounds, then fall for economic forecasters, stock market analysts, and psychologists. We know that economic forecasts work no better than astrology; stock market analysts are more pompous but much less elegant than the bishop, and psychology papers do not replicate meaning their results do not hold. My co-author Rupert Read and I have argued (using evolutionary arguments) that religion, via interdicts, allows the intergenerational transmission of survival heuristics and is effective in nudging people into some classes of behavior. By some irony, “nudging” theory developed by social scientists (which earned Richard Thaler a Nobel in economic sciences) has been recently shown to be nonreplicable, owing to a statistical artifact. Nonreplicable is the polite scientific term to mean that it is no different from astrology. Listen to the bishop — the recipient of generations of survival wisdom — not the psychologist. Taleb (2022) On Christianity "],["agency.html", "11 Agency", " 11 Agency Mumford’s mega-machine as biological metaphor All living things are essentially robots — bits of matter that have become animated. None of the fundamental constituents (molecules and atoms) have any agency. Nonetheless, through the miracle of complexity, matter somehow organizes into forms that at least appear (to us) to have agency. ‘Mmajor evolutionary transitions’ happens when a whole new level of natural selection emerges. Life started as replicating molecules, that then organized into larger proteins (RNA), that then organized into prokaryotic cells, that then merged into eukaryotic cells, that then organized into multicellular organisms, that then grouped into eusocial animals … At least in the context of human evolution, the emergence of civilization counts as a major evolutionary transition. We went from being a social primate to being ‘ultrasocial’. At least in the context of human evolution, the emergence of civilization counts as a major evolutionary transition. We went from being a social primate to being ‘ultrasocial’. Enter Mumford’s ‘megamachine’. The emergence of civilization went hand in hand with the concentration of power. Humans, for the first time, organized in large-scale hierarchies. If you want to frame this transition in terms of evolutionary theory, then it’s just part of a longer story. With every major transition, units that were previously ‘autonomous’ became cogs in the emergent larger ‘machine’. But in a strict scientific sense, it’s still ‘machines all the way down’. There is no unit where you can distinguish between ‘living’ matter and ‘dead’ matter. It’s all just matter. Blair Fix "],["hegel.html", "12 Hegel 12.1 Science of Logic", " 12 Hegel Lefebvre: in contrast to Feurbach’s humanism, which is in fact a myth of a naturalism, Hegel’s idealism “saw that man is not given biologically, but produces himself in history, through life in society, that he creates himself in a process.” 12.1 Science of Logic Mathematical Interpretation Wright Hegel’s Science of Logic, written in the early 1800s, is a difficult book, to say the least. One difficulty is that Hegel’s project seems fantastical: he aims to discover the fundamental structure of everything from pure reflection alone. Materialists and empiricists will rightly hesitate. Another difficulty is Hegel’s methodology, which is shocking: doubting even Rene Descartes’ ‘I’ he claims to start his enquiry with zero assumptions – and yet derives multiple propositions. How is it possible to reason with no axioms or inference rules? Another difficulty is Hegel’s language: difficult ideas often necessitate technical terms and complex locutions, but Hegel twists the reader up, down, left and right and the effect is dizzying. So it’s a difficult book, and one which most readers, over the centuries, have either never picked up, or quickly put down. Hegel’s beginning is irreducibly dynamic in the sense that the core concepts – pure being and pure nothing – are not merely concepts but actual occurrent processes (that we intuit by self-referential philosophical reflection). I will begin to apply the mathematical language of the calculus to represent change. We simply have no choice but to use currently available languages to describe the phenomena, but, of course, the phenomena isn’t those languages. Change implies a sequence of different ‘states’ that the thing that changes exhibits. Pure being changes positively with respect to itself; in consequence, pure being increases exponentially. Pure nothing changes negatively with respect to itself; in consequence, pure nothing decreases exponentially. The coming-to-be of pure being and the ceasing-to-be of pure nothing are identical once we remove the idea of an arrow of time. So we can make sense of Hegel’s startling claim that pure being and pure nothing are ‘the same’ and also ‘they are not the same’. They are not the same because pure being is about existence, whereas pure nothing is about non-existence, and hence they self-interact in different ways (pure being affirms itself, pure nothing denies itself). But they are also the same because the shape of these self-interactions are isomorphic. Their behaviour is identical. These different behaviours are isomorphic to each other, via the reciprocal map, \\(f(x)=1/x\\). Notably, the map is an involution, i.e. \\(f(f(x))=x\\), and therefore is its own inverse. In the language of calculus, the coming-to-be of pure being speeds towards infinity. In this sense, pure being explodes. For example, any natural or mechanical systems (which we cannot properly talk about here, but only mention by analogy) undergoing exponential growth quickly fall apart. They could only exist, at best, for a short period of time. Similarly, the ceasing-to-be of pure nothing collapses to zero. Again, any natural or mechanical systems that obeyed this exponential law of decrease would quickly cease and become entirely inert. In this sense, pure nothing implodes. In both cases, the systems ‘vanish’, either by exploding or imploding. So although pure being and pure nothing are present at the metaphysical bedrock, and imply each other, as self-referential systems they are unstable. They cannot permanently exist in their pure states of self-reflection. Thought contemplating itself can never catch its own tail, but will endlessly chase it, caught forever in a self-referential loop. As thought contemplating nothing it can never eradicate its own existence, and therefore forever sustains some residual of thought in the self-referential loop. We can now, at last, explicate Hegel’s ‘causal’ sense of vanishing into an opposite. Consider pure being in the state epsilon close to zero (where we imagine epsilon is a really small magnitude as close to zero as we wish). This state is arbitrarily close to the asymptote of pure nothing. But since being is a coming-to-be this ’empty’ state rapidly vanishes towards the state of pure being (infinity). More prosaically: when we try to contemplate absolutely nothing there’s an irreducible element of our own existence, which when noticed, takes over, and flowers into pure being. We can make sense of Hegel’s beginning by interpreting this passage as essentially talking about isomorphic positive and negative feedback loops. Hegel’s philosophical reflection asks us to perform the following mental exercise: shed all your knowledge and assumptions, including your own existence as an individual person and simply contemplate the existence of thought. When you do that you’ll enter a self-referential feedback loop. This mental state has paradoxical properties (especially from the point-of-view of formal logic). First, it seems like you are contemplating pure being (that is existence itself) but also it seems like you are contemplating pure nothing (zero content, or non-existence). Both points-of-view make sense. But the interpretations are unstable, and don’t settle down, and spontaneously flip back-and-forth. And, furthermore, what the feedback loops seem to strive towards – the state of pure being or pure nothing – can never be reached. In this sense, they cannot exist, both ‘logically’ and ‘causally’. Since the beginning doesn’t make sense and cannot exist it therefore cannot really be the beginning after all. The beginning must be something else. At this point Hegel introduces the concept of ‘sublation’. Becoming is the sublated unity of being and nothing. They no longer self-relate but ‘interpenetrate’ each other Figure: Becoming is the sublated unity of being and nothing. They no longer self-relate but ‘interpenetrate’ each other The unity of being and nothing expressed as a system of coupled first-order differential equations. We need to refer to these equations. I’ll call them ’Hegel’s equations‘, and sometimes ’Hegel’s contradiction‘. ‘Determinations are of unequal value’ in the straightforward sense that, in general, \\(dx/dt\\) is not equal to \\(dy/dt\\). Hegels Sublation: Grasped as thus distinguished, each is in their distinguishedness a unity with the other. Becoming thus contains being and nothing as two such unities, each of which is itself unity of being and nothing; the one is being as immediate and as reference to nothing; the other is nothing as immediate and as reference to being; in these unities the determinations are of unequal value The coupled system mirrors Hegel’s natural language description of sublation remarkably well. The reference is such ‘that is to say, it passes over into it‘. In the coupled system, we have a ‘substance’ that actually flows from being into nothing, and the ‘substance’ leaves nothing and enters into being. Now we also have a typically Hegelian claim: becoming is both a ‘ceaseless unrest’ and a ‘quiescent result’. So the ‘vanishing’ that previously implied that pure being and pure nothing could not exist, now, in this sublated state, ‘vanishes the vanishing itself’ such that we now have ceaseless unrest that paradoxically collapses into a stable result (that presumably doesn’t vanish by either exploding or imploding). Hegel’s equations are equivalent to two second-order differential equations that, in terms of their motion, are independent (although they are coupled in terms of their initial conditions). Now we have the acceleration of being (or nothing) changing negatively with respect to itself. We need to solve the second-order linear differential equations. Since this deduction applies to both being (x) and nothing (y) I’ll just solve for the temporary variable (z): In this mathematical interpretation of becoming, being changes according to cos(t) and nothing changes according to -sin(t). Being and nothing oscillate between -1 and 1, forever. When being realises its maximum (at an absolute value of 1) then nothing is at its absolute minimum of 0. When nothing realises its maximum, then being is at its minimum. What one ‘gains’ the other ‘loses’, but neither ‘side’ ever wins. What is being gained, and what is being lost? It’s tempting to introduce familiar physics-based concepts, such as amplitude or energy etc. But Hegel employs the term ‘indeterminate being’, which is the status of pure being and pure nothing prior to their sublation. I will use the slightly more evocative term ‘substance’. Being and nothing continually exchange their substance with one another: at one time being is more substantial, at another time nothing is. Define the total substance contained within the unity as the sum of the squares of x and y (to handle the negative values). As soon as we do that, we immediately see that Hegel’s equations instantiate a simple conservation law: \\[x^{2} + y^{2} = cos^{2}t + sin^{2}t = 1\\] We can think of this perpetual trade-off between being and nothing as either eternal conflict, or an eternal dance of co-operation. Hegel, more simply, describes it as ‘a ceaseless unrest’. The union of being and nothing is unstable, any equilibrium is immediately undermined, and the opposing concepts remain in perpetual contradiction. We’ve shown that Hegel’s contradiction does generate ceaseless unrest. But in typically Hegelian fashion, becoming is not merely a ‘ceaseless unrest’ but also a ‘quiescent result’. Becoming as a ‘quiescent result’: being and nothing always vary but are bounded. The 2-D state-space of Hegel’s equations is a perfect circle. Hegel’s equations, that is the unity of being and nothing, trace a perfect circle in state-space. Becoming is indeed ceaseless unrest, but that unrest is always bounded. Pure being, which merely self-relates, explodes, and pure nothing, which also self-relates, implodes; in this sense, neither can exist. In contrast, Hegel’s equations define a stable dynamic system: their sublated unity neither explodes or implodes, but is a ‘quiescent result’ that reproduces itself indefinitely. In fact if we – somehow – managed to be outside observers and ‘measured’ the total substance of becoming we would notice no change whatsoever. Hegel’s equations, as we’ve seen, obey a conservation law. The ceaseless unrest on the inside conserves the total substance and so, on the outside, we would observe perfect calm, a truly quiescent result. So becoming both preserves its identity over time (conservation of substance) and changes (the internal oscillation). The unity of being and nothing determines a new kind of whole: a dynamic and contradictory unity. Pure being and nothing ‘sink from their initially represented self-subsistence’ and are turned into ‘moments’ of a bigger whole where they are ‘distinguished but at the same time sublated’. Hegel resolves this paradox by a (logical? causal?) operator he calls sublation. Hegel remarks that sublation is ‘one of the most important notions in philosophy’. A sublation, in typically Hegelian fashion, both preserves or maintains and puts an end to. Where did this operator come from? I think Hegel would argue that this operator is observable within the phenomenon itself. Becoming is the name Hegel gives to the sublation of pure being and pure nothing. Suddenly, everything changes: we ‘put an end to’ pure being and pure nothing as self-referential concepts (as uncoupled differential equations); and now they reciprocally refer to each other (as coupled differential equations). So each presupposes the other, and neither is a unique starting point. There cannot be being without nothing, or nothing without being. The logical paradox is resolved. In consequence, the unity of being and nothing determines a beginning that does make sense and can exist: the beginning is an irreducibly dynamic and contradictory unity. According to Hegel the fundamental structure of becoming must be present, as both a logical and natural necessity, in anything that exists at all. My mathematical interpretation covers only chapter 1 of Hegel’s monumental and obscure Science of Logic. In subsequent chapters, Hegel derives the necessary existence of further categories, such as quality, finitude, infinity, multiplicity, quantity, measure and the syllogisms of ‘ordinary’ logic. We should explore how far this new, mathematical interpretation of Hegel’s opening chapter extends to his later chapters. At some point, the semantics of Hegel’s metaphysical theory and the semantics of systems of differential equations must surely break down. But who knows? We might yield more insights into Hegel’s philosophy by pursuing this project. Regardless, Hegel – at least as far as he is concerned – derives and critiques the Kantian categories from his assumption free starting point, and, if this derivation is successful, then that would constitute evidence that fundamental aspects of our cognition are the manifestation of the contradiction between being and nothing. Those with a physics background will have already noticed that Hegel’s equations imply that the unity of being and nothing instantiates simple harmonic oscillators. Simple harmonic oscillators are the bread-and-butter of physics courses simply because harmonic oscillation is ubiquitous in nature, both in the microcosm (quantum) and the macrocosm (general relativity). As above, so below. Quantum field theory, the currently dominant theory of fundamental particles, is essentially simple harmonic motion taken to increasing levels of abstraction. In other words, simple harmonic motion is indeed a fundamental structure that appears, again and again, at all levels of physical reality. Hegel’s contradiction is not merely simple harmonic motion, but rather a 2-D, system of coupled harmonic oscillators with additional properties that relate to complex analysis and holomorphic functions. But here let’s simply note the following: it’s utterly remarkable that Hegel’s psychedelic, assumption-free starting point, which is resolutely conceptual and abstract – and makes no reference to physical reality or empirical knowledge whatsoever – nonetheless, according to the interpretation developed here, implies a structure of ‘becoming’ that is equivalent to the fundamental structure found everywhere in physical reality. Physicists might not ask, and perhaps could not answer, why oscillatory motion is ubiquitous in nature. Philosophy, in particular Hegel’s metaphysics, in contrast, provides a candidate explanation of this empirical phenomenon: according to Hegel, everything that exists necessarily is a unity of being and nothing and therefore – according to the mathematical interpretation developed here – must exhibit harmonic motion. Ian Wright "],["marxism.html", "13 Marxism 13.1 Transformation Problem 13.2 Marx durability", " 13 Marxism 13.1 Transformation Problem Ian Wright My paper, “A category mistake in the classical labour theory of value”, tackles Ricardo and Marx’s problematic in the context of a formal model of capitalist production. The formality is austere but has the advantage that it imparts precise semantics to some of the key concepts of the labour theory of value. This helps pinpoint a certain kind of logical error in the classical theory. Philosophers, such as Gilbert Ryle (1984 [1949]) and Ludwig Wittgenstein (1953), argue that the underlying cause of a long-lived and insoluble problem is often a hidden conceptual confusion or mistake. The problem is insoluble because the conceptual framework in which the problem is stated is itself faulty. Empirical study, or experimental activity, cannot resolve such problems. Rather, the problem must be deflated or dissolved by applying conceptual analysis. For instance, Ryle introduced the term “category-mistake” (Ryle 1984[1949], ch.1) to denote the conceptual error of expecting some concept or thing to possess properties it cannot have. For example, John Doe may be a relative, friend, enemy or stranger to Richard Roe; but he cannot be any of these things to the “Average Taxpayer”. So if “John Doe continues to think of the Average Taxpayer as a fellow-citizen, he will tend to think of him as an elusive an insubstantial man, a ghost who is everywhere yet nowhere” (Ryle 1984 [1949], p.18). In the paper, I argue that the contradictions of the classical labour theory of value derive from a “theoretically interesting category-mistake” (Ryle 1984 [1949], p.19), specifically the mistake of supposing that classical labour-values, which measure strictly technical costs of production, are of the same logical type as natural prices, which measure social costs of production, and in consequence labour-values and prices, under appropriate equilibrium conditions, are mutually consistent. Since this supposition is mistaken, Ricardo’s search for an invariable measure of value and Marx’s search for a conservative transformation attempt to discover a commensurate relationship between concepts defined by incommensurate cost accounting conventions. They therefore seek an impossible “elusive and insubstantial man” or “ghost”. Once the category mistake has been identified we can resolve the classical problems by “giving prominence to distinctions which our ordinary forms of language make us easily overlook” (Wittgenstein 1953, § 132). Such distinctions then solve, or more accurately, dissolve the problems. The key step is to notice that we can and should define a different measure of labour cost – total labour costs – that generalises the classical measure to include real costs induced by the institutional conditions of production. We then immediately possess a more general labour theory of value that includes both total and classical (i.e., technical) measures of labour cost. The general theory then applies the different measures in distinct, but complementary, theoretical roles, and in consequence separates issues normally conflated in the classical theories. The classical authors attempt to explain the structure of total costs of production – which include both technical costs due to the material conditions of production (e.g., the cost of physical capital and labour inputs) and additional social costs due to the institutional conditions of production (e.g., the cost of money-capital, state imposed taxes, etc.) – in terms of the structure of technical costs of production alone, which explicitly ignore institutional conditions. This conceptual error is the underlying cause of the almost two hundred year history of the “value controversy”. In the paper I explain why the more general theory has both an invariable measure of value and lacks a transformation problem. The main technical result is the theorem that natural prices are proportional to physical real costs of production measured in labour time. Hence, prices and labour costs, in appropriate equilibrium conditions, are “two sides of the same coin”. The measurement relation, missing from the classical theory, is therefore established, which implies that labour costs can in principle explain economic value. The more general theory therefore removes the primary theoretical obstacle that has hindered the development of the classical theory of value since its inception. Ian Wright 13.2 Marx durability Milanovic The greatness of social scientists is shown by their “durability”, that is, by the ability to be relevant over the long haul. Whoever has read Marx must know that his program was much wider than the transformation of economics. The critique of political economy, as the subtitle of Capital says, is just one part of that overall program. His program was to propose an entirely new understanding of human history in which economic forces play an important, and perhaps, key role. Once the meaning of history discovered, this knowledge needs to be coupled with conscious action to bring about the change that history “reveals” to us. Our actions bring that historical evolution, discovered intellectually, to fruition. This is where the key Marxist concept, praxis, appears: it is the unity of theory (idea) and practice. It is at that point that that ideas (if they indeed are “correct” in the sense that they have seized the key determinants of evolution of human society) become material forces. Idea is then as much of a tool of change as a demonstration, the barricade, or the invention of a new machine. Why do I need to go through this long explanation of Marx’s system (which of course is something that has been hinted at since Gramsci and Lukacs but became much better known after Marx’s manuscripts were published in the 1960s)? Because it shows that reducing Marx’s objective to influencing economists like J. S. Mill and others is totally misunderstanding Marx. Marx was in the business of influencing the world history and social sciences as a whole. This was supposed to be done through the workers’ movement which imbued with the “correct” ideology becomes the instrument that changes history, brings about classless society, and lets us enter “the realm of freedom”. Marx was not in the business of applying for a job at the University of Cambridge, or writing refereed papers. Comment by Krassen Dimitrov Excellent essay. There are some differences, however, with the other social scientists and social events that you reference. The October revolution was not really an event that validated Marx’s theory - it was just a serendipitous coup by a group of people who happened to read and espouse Marx. Not a proletarian revolution as proscribed by Marx, by any means; heck that country hardly had any proletariat to speak of. Further, once in power the Bolsheviks institutionalized Marxism as state ideology, which is not something that happened to any of the other scholars that you mentioned. Their theories are mostly pure science, or in the case of Keynes applied science, which happened to be corroborated by world events. The Marx - October Revolution connection is quite different. That does not invalidate the intellectual merit of Marx’s writings, but it is just a different paradigm. I think a more relevant comparison is probably early Christian doctrine and its institutionalization by Constantin and the Roman Empire. While powerful, emotive and viable for over 300 years, the Christian cult was not meant to become the state religion of the most powerful Empire. It was an extrinsic event that vastly increased Christianity’s relevance in history. While Marx’s writings are as compelling as any of the other scholars you mention, the October revolution did indeed elevate his historical status in the XXth century far above that of any other social scientist, in a somewhat random and stochastic way. End of Comment Milanovic (2022) Die Marx Frage "],["socialism.html", "14 Socialism", " 14 Socialism Wright Contrary to conventional wisdom the defining characteristic of socialism is not the abolition of market relations and its replacement by centrally planned, top-down production. Economic planning has no bearing whatsoever on whether a set of social relations are exploitative or not. The essence of socialism is a hoped-for system of property relations, which we’ll call the “communal system”. In this system, the renting of people has been abolished (just as liberal democracy abolished the selling of people, i.e. slavery). People no longer are workers available to rent by the owners of firms. Instead, people are workers available to join as equal members of a democratic firm, who together lay claim on the residual income. A socialist firm is owned by its working members who hire-in capital at pre-agreed rental prices (compared to capitalism, the contracts are reversed). Capital, not labour, is now the ex ante cost of production. In consequence, the working members democratically distribute the firm’s residual income to themselves. This blog is about getting from here to there … Where is here? “Here” is the social system we live in. We know we live in a capitalist system. But do we understand what this really means? Contrary to conventional wisdom the defining characteristic of capitalism is not market exchange. Market relations have existed since classical times. The essence of capitalism is a system of property relations, which I’ll call the “wage system”. Here, the capitalist firm hires-in labour at a pre-agreed rental price. The labour is mixed with other inputs and produces goods or services for sale in the market. Normally, firms sell at prices that exceed their costs of production, which includes the cost of used-up material inputs, rent, interest on capital loans, and labour costs etc. Here, labour is just another ex ante cost of production. Any remaining revenue — the residual income — then gets distributed as profits to the owners of the firm. What’s wrong with this? Essentially, the wage system is a theft-based system of property relations. The mere legal ownership of a firm is sufficient to lay claim on its residual income. The owner of a capitalist firm can, as John Stuart Mill, put it: “grow richer, as it were in their sleep”. Yet this residual income is the fruit of others’ labour. Taking resources from others, without giving anything back in exchange, is theft. This is why Marxists label capitalism an exploitative economic system. But wait. Isn’t profit a just reward for the risk of capital investment? Someone has to fund the firm. Surely owners deserve their returns too? Actually, no. There’s a big difference between advancing capital to a firm, and owning the firm. Let’s say you advance capital to a firm. You should expect repayment of your capital, plus a risk premium and collateral security, as a just exchange. But by lending capital you do not become an owner of the firm. Once your loan is repaid you have no further claims on the firm’s revenue. This contract is based on the principle of exchange: in essence, it allows the loaner and loanee to exchange the time when they consume a set of resources. There’s no theft here. But let’s say you want more. You advance capital to a firm, and in addition to the above, you expect joint ownership of the firm, i.e. equity capital. In this case you do become an owner of the firm. And so, once your initial loan and risk premium is repaid, you still retain a claim on the firm’s revenue. In fact you lay claim to the residual income, that is the fruits of others’ labour. This contract is not based on the principle of exchange. You now get to take resources from others, solely in virtue of the paper claim of holding “equity”. You do not have to give anything to the firm in return for your claim. The contract bestows the right to take wealth produced by others by fiat. This is theft. All ideological justifications of capitalism obscure this theft, and render it normal, almost entirely unnoticed, and socially acceptable. It’s largely an unquestioned and seemingly natural aspect of our economic relations. Capitalist property relations are not merely unjust, however. They are also the hidden and root cause of the major social ills of our day, in particular those caused by extreme income and wealth inequality. The capitalist firm minimises input costs, including wages, in order to maximise the residual income of the owners of the firm. This causes a two-class distribution of income and wealth, with a Pareto long tail of the super-rich. And it’s also the major cause of imperialism, which has a material foundation in the massive wealth disparities between countries. In consequence, capitalist property relations are also undesirable. Where is there? We might prefer to live in a system that avoids these widespread injustices and social ills. Traditionally this is the goal of socialism. But do we understand what socialism really means? Contrary to conventional wisdom the defining characteristic of socialism is not the abolition of market relations and its replacement by centrally planned, top-down production. Economic planning has no bearing whatsoever on whether a set of social relations are exploitative or not. The essence of socialism is a hoped-for system of property relations, which we’ll call the “communal system”. In this system, the renting of people has been abolished (just as liberal democracy abolished the selling of people, i.e. slavery). People no longer are workers available to rent by the owners of firms. Instead, people are workers available to join as equal members of a democratic firm, who together lay claim on the residual income. A socialist firm is owned by its working members who hire-in capital at pre-agreed rental prices (compared to capitalism, the contracts are reversed). Capital, not labour, is now the ex ante cost of production. In consequence, the working members democratically distribute the firm’s residual income to themselves. What’s right with this? The communal system is an inherently egalitarian system because all income from production is earned in essentially the same manner: by the contribution of labour. Absentee owners no longer lay claim to the fruit of others’ labour in virtue of a piece of paper (e.g. “equity”). The systematic economic theft, characteristic of capitalism, has been abolished. Socialism is not merely just, however. It also eradicates extreme income and wealth inequality for the simple reason that the entire working population earns the same kind of economic income, which is a kind of profit share. The split of the population into two main economic classes, that is workers (wage-earners) and capitalists (profits via ownership of firms), disappears along with the major social ills caused by extreme inequality. We need to formulate a political economy of socialism that simultaneously constitutes a political practice that crowds-out capitalist property relations. Ian Wright Somers on Polanyi Polanyi defines socialism as “the tendency inherent in an industrial civilization to transcend the self-regulating market by consciously subordinating it to a democratic society.” Three important points follow from this unusual definition. First, Polanyi offers us no telos, or predefined endpoint, for this process. Perhaps private ownership will ultimately disappear and be replaced by various forms of collective ownership, but we simply do not know. Second, since there is no end to history, there will be no end to struggles and conflicts, and there is no guarantee that democratic gains will not be reversed, as they were with the triumph of European fascism. Finally, the core of the socialist project is to extend and deepen democratic governance of the economy; this is the only way to make sure that democratic gains will not be reversed. Polanyi was unequivocal in defending what some derided as “bourgeois democracy”—parliamentary government and the related bundle of political rights. But he also believed adamantly that measures to expand democracy through political rights would amount to little without an equal foundation in social and economic rights. In the 1920s he was drawn toward G.D.H. Cole’s vision of guild socialism, in which an elected parliament would share ultimate power with representatives of the various workers’ councils that would own and direct enterprises. By the 1940s he was more in tune with expansive versions of U.S. industrial democracy, in which employees would share power with managers through a system of collective bargaining that did not recognize managers’ prerogative to make certain decisions on their own. All of this suggests that Berman’s notion of Polanyi as favoring the “primacy of politics” is not exactly right. Politics is not simply his preferred pole of the traditional dichotomy between state and market. Polanyi, for example, consistently defends the presence of regulated markets in a just society. Even more important, Polanyi sees politics and government as components of his larger concept of the social, which also includes civil society, social relations, and cultural practices. It would thus be more precise to call Polanyi a theorist of the “primacy of the social.” The divide between the political class and “the people” increasingly appears as an unbridgeable divide marked by hostility and deep distrust. Polanyi knew very well that this kind of divide is greatly exacerbated by the policies that we now call market fundamentalism. When people are told for a generation that government must make no decisions that interfere with the autonomous logic of the market, and when international bond markets can dictate national policies, it is inevitable that people will start to lose faith in democratic governance and its capacity to help them solve their problems. (Margaret Somers is professor of sociology and history at the University of Michigan. Fred Block is research professor of sociology at the University of California, Davis. Their new book is The Power of Market Fundamentalism: Karl Polanyi’s Critique (Harvard).) Somers (2023) The Return of Karl Polanyi "],["anarchism.html", "15 Anarchism 15.1 Kropotkin’s evolutionary holism", " 15 Anarchism 15.1 Kropotkin’s evolutionary holism Sometimes—not very often—a particularly cogent argument against reigning political common sense presents such a shock to the system that it becomes necessary to create an entire body of theory to refute it. Such interventions are themselves events, in the philosophical sense; that is, they reveal aspects of reality that had been largely invisi-ble but, once revealed, seem so entirely obvious that they can never be unseen. Much of the work of the intellectual Right is identifying, and heading off, such challenges. Cooperation is just as decisive a factor in natural selection than competition. It is not love, and not even sympathy (understood in its proper sense) which induces a herd of ruminants or of horses to form a ring in order to resist an attack of wolves; not love which induces wolves to form a pack for hunting; not love which induces kittens or lambs to play, or a dozen of species of young birds to spend their days together in the autumn; and it is neither love nor personal sympathy which induces many thousand fallow-deer scattered over a territory as large as France to form into a score of separate herds, all marching towards a given spot, in order to cross there a river. It is a feeling infinitely wider than love or personal sympathy—an instinct that has been slowly developed among animals and men in the course of an extremely long evolution, and which has taught animals and men alike the force they can borrow from the practice of mutual aid and support, and the joys they can find in social life…. It is not love and not even sympathy upon which Society is based in mankind. It is the conscience—be it only at the stage of an instinct—of human solidarity. It is the unconscious recognition of the force that is borrowed by each man from the practice of mutual aid; of the close dependence of every one’s happiness upon the happiness of all; and of the sense of justice, or equity which brings the individual to consider the rights of every other individual as equal to his own. Upon this broad and necessary foundation the still higher moral feelings are developed. (Kropotkin: Mutual Aid) The only viable alternative to capitalist barbarism is stateless socialism, a product, as the great geographer never ceased to remind us, “of tendencies that are apparent now in the society” and that were “always, in some sense, imminent in the present.” To create a new world, we can only start by rediscovering what is and has always been right before our eyes. Andrej Grubacic &amp; David Graeber: Mutual Aid Kropotkin "],["feudalism.html", "16 Feudalism", " 16 Feudalism Brown Abstract In this article I reflect on feudalism and the attack I launched in 1974 against it and such similar constructs as feudal system, feudal society, and feudal monarchy. I first review the reasons for my campaign and its timing. Re-evaluating the extent and gravity of the disapproval the term had long elicited, I reconsider the relationship between my uncompromising assault and earlier opposition to feudalism. Before examining the reactions to the article, positive and negative, I treat the feudal constructs’ appeal and powers of endurance, and the cognitive roots of their advocates’ attachment to them. In appraising the article’s reception, I discuss Susan Reynolds’s book, Fiefs and Vassals: The Medieval Evidence Reinterpreted, published in 1994, and the similarities and differences between our approaches to the feudal constructs and to medieval society and politics. In a final section I assess the diminished fidelity that feudalism has commanded since 2000, and the progressive waning of the feudal constructs’ influence on studies of medieval Europe, which focus increasingly on the complexities of its evolution. The conclusion reiterates the call I issued in 1974 to renounce the constructs and cautiously forecasts their imminent demise, except as evidence of the styles of conceptualization that led their sixteenth- and seventeenth-century fabricators to invent them. Brown (2022) Feudalism: Reflections on a Tyrannical Construct’s Fate (paywall) "],["social-democracy.html", "17 Social Democracy 17.1 Transition Risks", " 17 Social Democracy 17.1 Transition Risks Adam Przeworski Przeworski argues that European socialist parties in the first half of the 20th century faced a sequence of electoral dilemmas. The first dilemma was whether or not to participate in bourgeois elections, when universal suffrage was progressively established in Europe. The question was whether or not participation would contribute to the struggle for socialism or strengthen the capitalist order. According to Przeworski, most socialist parties have opted to get involved in elections, since it was a means to advance some of the interests of workers in the short run and, as references to Friedrich Engels and Eduard Bernstein illustrate in Przeworski’s book, to move toward socialism. According to Przeworski, the decision to participate in bourgeois elections led to another dilemma. Given that manual workers were not the numerical majority in any European country, to win elections they had to choose whether or not to compromise their socialist principles and adopt a social democratic agenda to attract the support of allies, especially the middle class. Such compromise had major consequences for socialist parties, including the withdrawal of support of workers, the abandoning of extra-parliamentary tactics, and progressively the defection from socialist policies when in power. Wikipedia (2023) Adam Przeworski Tooze As we get into the details of green transition, one thing to draw out is its temporalities: what particular challenges are attached to different moments in the transition? This is an urgent question reminiscent of Adam Przeworski’s classic u-curve of socialist transition, where the move away from capitalist production would involve losses before it brought gains to workers. Przeworski’s work in the 1980s was designed as a challenge to social democratic hopes in seamless electoral progress, so how will we mitigate the analogous climate problem amid a polycrisis defined in part by increasing tensions between states? Here’s an IMF working paper: During the “mid-transition” period, when the economic transformation is most rapid, cross-border risks could generate or exacerbate instability in the economic, political, and financial spheres, which may become detrimental to the global transition process Source: Espagne et al. (2023) Cross-Border Risks of a Global Economy in Mid-Transition "],["democracy.html", "18 Democracy", " 18 Democracy Coccoma In a poll conducted in January 2020, 65 percent of respondents said that everyday people selected by lottery—who meet some basic requirements and are willing and able to serve—would perform better or much better compared to elected politicians. In March last year a Pew survey found that a staggering 79 percent believe it’s very or somewhat important for the government to create assemblies where everyday citizens from all walks of life can debate issues and make recommendations about national laws. The idea—technically known as “sortition”—has been spreading. Perhaps its most prominent academic advocate is Yale political theorist Hélène Landemore. Her 2020 book Open Democracy: Reinventing Popular Rule for the Twenty-First Century explores the limitations of both direct democracy and electoral-representative democracy, advocating instead for government by large, randomly selected “mini-publics.” As she put it in conversation with Ezra Klein at the New York Times last year, “I think we are realizing the limits of just being able to choose rulers, as opposed to actually being able to choose outcomes.” She is not alone. Rutgers philosopher Alex Guerrero and Belgian public intellectual David Van Reybrouck have made similar arguments in favor of democracy by lottery. In the 2016 translation of his book Against Elections, Van Reybrouck characterizes elections as “the fossil fuel of politics.” “Whereas once they gave democracy a huge boost,” he writes, “much like the boost that oil gave the economy, it now it turns out they cause colossal problems of their own.” Sortition got a popular, if perhaps unwitting, shout-out in the summer of 2020, when Andrew Yang—then in the thick of a run for president—tweeted, “There are times when I think one could replace our leaders with citizens chosen at random and get a better result.” The message went viral. Elections reward well-positioned insiders who have the connections and war chest to wage a campaign. They also attract ambitious social climbers. Today even the most virtuous candidates have to solicit truckloads of money—anywhere from $500,000 to $2 million for a credible run. Once in office, winners spend much of their time raising revenue for reelection. The amount of actual legislating—investigations of issues, research into policy, seeking the common good—is small, and legions of lobbyists exercise influence to the tune of $3.5 billion a year. Modern liberal governments are not democracies; they are oligarchies in disguise, overwhelmingly following the policy preferences of the rich. Even if democracy by lottery worked for Athens, could it work today in the massive states of the modern world? A growing body of empirical work on citizens’ assemblies suggests it could. (As Trinidadian Marxist historian C. L. R. James put it in his 1956 essay “Every Cook Can Govern,” whose title he took from Lenin, “We must get rid of the idea that there was anything primitive in the organization of the government of Athens.” “On the contrary,” he wrote, “it was a miracle of democratic procedure.”) Meanwhile, democracy by lottery has exploded onto the practical political stage elsewhere in the West. The turning point came in 2004 in British Columbia, where the government convened a full-blown citizens’ assembly. Through lotteries, over 150 people representative of the general population came together to study the province’s electoral system. They heard from experts, consulted with the public, and deliberated as a group before voting on recommendations. They put their proposals to a referendum, gaining about 57 percent support—just shy of the 60 percent supermajority imposed on them by the legislature. Despite coming up short, the undertaking marked a watershed in democratic practice. Citizens’ assemblies comprised of everyday people boast many virtues. Take two traits Americans say they revere: equality and representation. Lotteries realize the ideal of equality in ways that elections can’t even begin to approach. The latter favor the already powerful; lotteries, by contrast, give every citizen the exact same chance at selection. Moreover, by employing democratic lotteries to select members, assemblies channel the public’s beliefs much more accurately than elected legislatures. The key to the success of citizens’ assemblies lies in their impartiality and balance, along with their integration with expert testimony and counsel. Rather than going by gut or brute opinion, assembly members develop a comprehensive understanding of issues. They engage in meaningful conversations, challenging each other’s assumptions and working together to find common ground. Because of their diversity, citizens’ assemblies bring people of very different backgrounds together to search for answers. Facilitators work to ensure that every voice gets heard, and participants discover—often to their surprise—a new understanding for people on the other side. Partisan electoral politics looks very different today, of course. And surprisingly, as Landemore has found, everyday people even arrive at smarter decisions than panels of specialists. The process is at once more democratic—respectful discussions where everyone’s ideas matter—and more effective, resulting in better policy. A party is just a machine for getting votes. Democracy Without Elections: The foundation is currently conducting a campaign in Scotland to create a second house in the regional parliament populated by everyday Scots; this chamber would work alongside elected MPs, the way it now does in the German-speaking province of Belgium. Down the road, the group envisions a campaign to reform the British House of Lords into a similar allotted body. The final goal is ambitious: the end of politicians. In a 2013 article for the Journal of Deliberative Democracy, Bouricius offers a bold alternative: using multiple groups of citizens to pass legislation, each one filled by lottery. The first, an assembly of up to 400 representatives, would act as an agenda council. With the aid of professional staff, they would investigate social problems and set topics for legislation. Members would serve one three-year term before rotating out. The second chamber would consist of 150 different members, chosen by lot and randomly assigned to panels of three to five citizens. Each panel would be responsible for a particular policy area. They would take expert testimony, solicit proposals from the public, and work together to produce a bill. Again, representatives would serve one term of three years. Final passage of the law, however, would fall to a third body, dubbed the policy jury. Numbering some 400 people, this group would meet for just a few days to consider the proposed bill. After hearing from advocates and opponents of the legislation, they would vote by secret ballot on whether to adopt it as law. Not stopping with the legislature, Bouricius also imagines using citizens’ assemblies to select qualified chief executives. Bouricius’s vision is just one among many; still others are explored in the recent Verso collection Legislature by Lot (2019), edited by political scientist John Gastil and late Marxist sociologist Erik Olin Wright. How exactly to implement any of these proposals remains an open question. Some think such a transformation could come about gradually, with citizens’ assemblies stripping power from politicians bit by bit over time until the latter become figureheads. Others propose swifter action. Either way, democracy by lottery would amount to a revolution, and like all revolutions, it begins in the mind. Our political culture conditions us to believe that democracy is predicated on suffrage, but the legacy of the Athenian assembly reminds us that another system is possible: one where everyday people actually make the decisions determining their lives. 80 million eligible voters didn’t vote in 2020’s presidential contest. When asked why in a poll, two-thirds of respondents stated that elections have little to do with the way decisions get made in government. Coccoma (2022) The Case for Abolishing Elections "],["liberalism.html", "19 Liberalism 19.1 How Liberalism fails", " 19 Liberalism 19.1 How Liberalism fails Liberal capitalist democracy has generated a heterogeneous set of opponents: Romantic poets, Persian ayatollahs, aristorcrats, the Catholic Church, and fascists. However, for all the genuine and substantial differences separating these diverse oppositions, one can detect a shared critique. Liberal capitalist democracy is seen as one-sided in its emphasis on individualism, materialism, achievement, and rationality. The Roman Catholic preference for the family over the individual and the Nazi preference for “race” in place of the individual are radically different critiques, but the general critique is the same: liberal capitalism fails adequately to provide for the essential group needs and dimension of human existence. Batson (2022) from ken Jowitt The Lenisist Extinction Batson (2022) what to call china? "],["oligarchy.html", "20 Oligarchy", " 20 Oligarchy Winters Everyone is by now aware of the staggering shift in fortunes upward favoring the wealthy. Less well understood is that this rising inequality is not the result of something economically rational, such as a surge in productivity or value-added contributions from financiers and hedge-fund CEOs, but is rather a direct reflection of redistributive policies that have helped the richest get richer. The tiny proportion of wealthy actors among eligible voters cannot account for the immense political firepower needed to keep winning these policy victories. While motivated and mobilized minorities—those organized over issues like gay marriage, for example—can sometimes win legislative victories despite broad opposition from the electorate, America’s ultra-rich all together could barely fill a large sports stadium. They never assemble for rallies or marches, sign petitions, or mount Facebook or Twitter campaigns. So how do they so consistently get their way? One increasingly popular answer is that America is an oligarchy rather than a democracy.1 The complex truth, however, is that the American political economy is both an oligarchy and a democracy; the challenge is to understand how these two political forms can coexist in a single system. Sorting out this duality begins with a recognition of the different kinds of power involved in each realm. Oligarchy rests on the concentration of material power, democracy on the dispersion of non-material power. The American system, like many others, pits a few with money power against the many with participation power. The chronic problem is not just that electoral democracy provides few constraints on the power of oligarchs in general, but that American democracy is by design particularly responsive to the power of money…. Oligarchy should be understood as the politics of wealth defense, which has evolved in important ways throughout human civilization. For most of history, this has meant oligarchs were focused on defending their claims to property. They did so by arming themselves or by ruling directly and jointly over armed forces they assembled and funded. Every great increase in wealth required oligarchs to spend additional resources on armaments, castles, militias and other means of defense. The greatest transformation in the politics of wealth defense and thus of oligarchy came with the rise of the modern state. Through its impersonal system of laws, the armed modern state converted individual oligarchic property claims into secure societal property rights. In exchange, oligarchs disarmed and submitted to the same protective legal infrastructure that applied to all citizens (in theory if not always in practice). Property rights offered reliable safeguards not only against potential antagonists without property, but also, no less important, against other oligarchs and the armed state itself that administered the entire arrangement. Winters (2011) Oligarchy and Democracy "],["fascism.html", "21 Fascism 21.1 Fourteen Steps to Fascism (Eco) 21.2 Liberalism breeds Fascism 21.3 Fascism as a way to end Capitalism? 21.4 Hitler’s Volkstaat", " 21 Fascism George Orwell wrote in the 1940s:“The word fascism has now no meaning except in so far as it signifies ‘something not desirable’.” Fascism is the concentrated expression of the general offensive undertaken by the world bourgeoisie against the proletariat. The definition from the source (Benito himself): merging of corporate and state power. Max Horkheimer’s famous pronouncement: “He who wishes not to speak of capitalism, should hold his peace about fascism”. Edobo Fascism must be taken to refer to a particular form of authoritarianism that brings out and emphasizes the tools of external and internal violence of capitalism and of the social structures characterized by the industrial mode of production. Fascism is the organized violence of the hierarchical capitalist state in its pure form. Fascism is a deep-seated and ubiquitous tendency within the social order of modern civilization. It’s in the source code; it’s in the DNA of our society and its constituent roles and relations of production. It’s in the stories we tell to make sense of ourselves and the world. It’s in our very identities. Yeah, what’s probably most important of all, it’s an essential part of the authority structure to which we as semi-infantilized denizens of industrial society are attached and submit to. Yet this part is hidden in plain sight. The deeper structural facts do not align with the more palatable stories we like to tell ourselves and our children, no matter how important those fundamental facts happen to be. Fascism is thus in a sense a key part of the collective Shadow of modern society. Edebo (2023) Eternal fascism 21.1 Fourteen Steps to Fascism (Eco) Kilian Umberto Eco’s 14 steps to fascism: The cult of tradition, which involves a blending of old religions and values. “Truth has already been spelled out once and for all,” Eco writes, “and we can only keep interpreting its obscure message.” This might apply to the U.S. obsession with the precise interpretation of the Constitution, but it doesn’t seem to fit Canada. The rejection of modernism, by which Eco means the values of the Enlightenment and evidence-based rationality. That’s very Trumpian, and in a recent book Gen. Michael Hayden, former head of the CIA and National Security Agency, defends U.S. spy agencies as founded on Enlightenment values. While Canadian conservatives dislike the evidence for climate change, we generally respect free expression and scientific evidence. The cult of action for action’s sake. “Just do it” is OK for suburban joggers, but not for Canadian politicians. It’s impossible to imagine a Stephen Harper or Justin Trudeau even considering an impulsive stunt like a summit with Kim Jong-Un or Vladimir Putin. No analytical criticism. “In modern culture,” says Eco, “the scientific community praises disagreement as a way to improve knowledge. For Ur-Fascism, disagreement is treason.” Apart from kicking an occasional malcontent out of caucus, Canadian politicians tolerate a lot of disagreement, though the Harper Conservatives certainly disliked talkative scientists. Fear of difference. We got a whiff of this in late-stage Harperism, what with “barbaric cultural practices” and anti-hijab bylaws. But most Canadians appear to thrive on cultural, ethnic and sexual variation. Appeal to a frustrated middle class. When the middle class has been frustrated by 40 years of economic stagnation, everyone tries to offer something. Justin Trudeau keeps promising help especially to those trying to become middle class, but that may backfire: “In our time,” Eco wrote presciently in 1995, “when the old ‘proletarians’ are becoming petty bourgeois … the fascism of tomorrow will find its audience in this new majority.” Obsession with a plot. “The followers must feel besieged,” Eco wrote, “but the plot must come from the inside.” In the Cold War, it was Reds under the beds. Now it’s useful to raise the alarm about fellow citizens who support immigration and “illegal” migrants, who write “fake news,” or work for the “deep state.” This is a tougher sell in Canada than in the U.S., but some Canadians do love a good plot. Anti-elitism. “The followers must feel humiliated by the ostentatious wealth and force of their enemies,” Eco wrote, and plenty of Canadian politicians have invoked some supposedly privileged group as the source of all our woes. Pacifism is trafficking with the enemy. For the fascist, “life is permanent warfare,” Eco says, so the desire for peace amounts to treason. The U.S. is literally in a permanent war, but Canada still likes to present itself as a peacekeeper, honest broker and very discreet arms merchant. Contempt for the weak. Trump’s mockery of a journalist with a disability has become a meme. His backers don’t much care if migrant families are separated and traumatized. But they themselves feel weak and hard done by. Fascism offers what Eco calls “popular elitism,” and what Americans call “American exceptionalism”— the belief that they live in the best country in the world, especially if they are “real Americans” who trace their descent back to Europe. Again, Canada has flirted with this attitude, turning away Jewish refugees before the Second World War (“one is too many”) and fretting about “illegal border crossers” crossing into Quebec, Ontario or Manitoba. The cult of heroism. “The Ur-Fascist hero is impatient to die,” Eco wrote. “In his impatience, he more frequently sends others to death.” The closest that we ever got to that idea was during our Afghan involvement, when Stephen Harper turned part of the Trans-Canada into a “Highway of Heroes,” a path of glory leading to Canadian graves. Machismo, “which implies both disdain for women and condemnation of nonstandard sexual habits, from chastity to homosexuality.” Many right-wingers decline to march in Pride parades, but every sensible Canadian politician shows up for them waving a rainbow banner. Selective populism. “Individuals as individuals have no rights,” said Eco, and a majority is meaningless. Instead, individuals form a monolithic people; “the Leader pretends to be their interpreter.” Parliamentary governments are, by fascist definition, rotten. Eco prophesied that “There is in our future a TV or Internet populism, in which the emotional response of a selected group of citizens can be presented and accepted as the Voice of the People.” Today we call them bots and trolls. Ur-Fascism speaks Newspeak, drawing on “an impoverished vocabulary and an elementary syntax, in order to limit the instruments for complex and critical reasoning.” Anyone who’s seen videos of Donald Trump at a rally, or read any of his tweets, knows how fluent he is in Newspeak. Canada has not abandoned English to quite such an extent, but our politicians are far too fond of talking points. Evidently they think Canadians can’t follow an idea developed through several related and grammatical sentences. Kilian (2022) Fourteen Steps to Fascism 21.2 Liberalism breeds Fascism Mattei [Tooze (2022) The centenary of Mussolini’s “March on Rome” and the dilemmas of the liberal expert class. ](https://adamtooze.substack.com/p/chartbook-166-19222022-the-centenary] 21.3 Fascism as a way to end Capitalism? Milanovic on Victor Serge I would like to finish with Serge’s observations on two fascists whom he personally knew at the time when they were communists: Jacques Doriot (“Zinoviev liked him”) and Nicola Bombacci. They were both killed in retribution at the end of the War. Bombacci was one of the fifteen executed together with Mussolini. Their transition from communism to fascism is explained by the need for restless activity, great organizational skills, and ambition. But there is one interesting, small ideological detail: both, Serge thinks, might have seen fascism within Marxist scheme as a ruse of history where decrepit capitalism adopts fascism as a way to save itself; yet fascism, by imposing a strong state rule over private sector, gradually transforms it, and creates an economy that can, in a future evolution, be readily taken over by workers. In such a bizarre way, fascism was, he believes, seen by the former communists, as a way to end capitalism. Milanovic (2023) The Book of the Dead 21.4 Hitler’s Volkstaat Tooze The debate over Götz Aly’s controversial book Hitler’s Volksstaat (2005), which appeared in English as Hitler’s Beneficiaries. It is a fascinating book that seeks to offer a sweeping overall assessment of the balance of costs and benefits of the Nazi regime to the German population. Hitlers Volksstaat reveals both the strengths and weaknesses of Aly’s scholarship. It is brilliantly original in its microscopic reconstructions. It takes a particular type of genius to realize the significance of Reichsbahn baggage allowances for the history of World War 2. But it was the bigger picture that really caused the sensation. Looking at the fiscal account, what Aly claimed was that ordinary Germans had paid only 10 percent of the costs of the Hitler’s wars. Their share was so low because they had benefited from two types of redistribution. A much larger contribution, 20 percent of the total, had been taken from the profits of businesses and higher-income groups in the Reich by means of progressive taxation. The Nazi regime had, in short, made good on some of its social promise. The rest - 70 percent - was paid for by plunder, first of the Jews in Germany and Austria and then of the entire economy of the rest of Europe. the fact that the regime demanded so little from the German population, confirmed that they were not in fact fanatical Nazi’s willing to sacrifice for their beliefs. Nor, however, were the majority of Germans coerced, as the left had claimed on behalf of the German working-class. They were, Aly claimed, bought off. They were the beneficiaries of a Nazi wartime welfare state funded by redistribution and plunder – first from the Jews and then from the entire population of occupied Europe. Rather than heaving a sigh of relief and rejoicing in the fact that the Nazi regime was not sustained by fanatical ideological loyalty, for Aly this was cause for resigned despair. He was a veteran of radical politics in the 1960s, 70s and 80s. For him the data confirmed that the true continuity of German history was not fanatical beliefs of any kind, but rather the the persistence of a depoliticized populace clamorously dependent on the state. His critique was directed at the holy cow of redistribution itself. The welfare state, Aly wanted his readers to realize, was in Germany closely linked to predation. Very ordinary people were capable of doing extremely violent things for very ordinary reasons. Aly’s attempt to demonstrate the redistributive effect of fiscal policy in the Third Reich was rendered misleading by his failure to consider the underlying development of income shares. Once one allows for the underlying dynamics of income shares, which vastly outweighed the impact of taxation, his revisionism falls flat. It was not the working class but German business that was the big beneficiary of Hitler’s regime. Many of the territories that Germany occupied, like Denmark, the Netherlands, Belgium and France were rich. They clearly contributed on a large scale. Those exactions particularly of labour were the material driver of resistance to the Nazi occupation. But could they really have been large enough to provide 70 percent of the resources needed for total war? The answer, is clearly no. So how did Aly get the thunder of world history so wrong? He arrived at his topsy turvy conclusions, by treating war borrowing as though it imposed no burden on the German population. It would be repaid in the future out of reparations to be imposed on the defeated enemies of Germany. The magical idea that borrowing allow societies to shift economic burdens into the future, or conversely, resources from the future to be teleported into the present, is by no means peculiar to Aly. It recurred a few years later in the equally wrong-headed argumentation offered by Wolfgang Streeck in his Adorno lectures, published as Buying Time. The fallacy originates in a conflation between micro and macroeconomic perspectives. An individual borrower may transfer purchasing power across periods, but societies as a whole cannot, unless they borrow from the “outside” i.e. from other societies. For everyone “buying time” there must be someone “selling” it. At a deeper level the attribution of such metaphysical powers to debt echoes the uncanniness of debt relations, a feeling of unease which is shared on the left, by folks like Aly and Streeck, as much as it is by conservatives. One of the first to truly skewer the alchemical fallacy of war finance by means of treasure was John Maynard Keynes in his pamphlet on How to Pay for the War published in 1940. But we should not fall into the fallacy of thinking that it took a surpassing genius of Keynes’s ilk to figure this out. Hitler’s Finance Minister Schwerin von Krosigk spelled out the argument perfectly clearly in his memoirs Bilanz des Zweiten Weltkrieges. The contribution of German society to the Nazi war effort was not 30 percent, as suggested by Aly, but 72-75 percent. Consumption in Nazi Germany was not at the level of modern affluence. Nor did it prioritize consumption as the main driver of growth as was the case in the US or the UK for much of the recent past. But nor was the experience of Nazi Germany in the 1930s in any way akin to that in the Soviet Union. No Volksgenossen starved for rearmament, or even for the war effort. Indeed, the regime systematically awakened the promise of a German socialism to come, not the reality supposedly diagnosed by Aly, but a promise of an “economic miracle” that lay in the future, after victory and conquest were complete. Tooze (2023) Chartbook #186 Solicitous dictatorship. The political economy of authoritarianism "],["history.html", "22 History 22.1 Personalities", " 22 History 22.1 Personalities 22.1.1 Jimmy Carter Bergman Carter, who lost his bid for re-election in a so-called landslide to Reagan in 1980, is often painted as a “failed president” – a hapless peanut farmer who did not understand how to get things done in Washington, and whose administration was marked by inflation, an energy crisis and the Iran hostage disaster. He was not in over his head or ineffective, weak or indecisive – he was a visionary leader, decades ahead of his time trying to pull the country toward renewable energy, climate solutions, social justice for women and minorities, equitable treatment for all nations of the world. He faced nearly impossible economic problems – and at the end of the day came so very close to changing the trajectory of this nation. He startled the globe by personally brokering the critical Middle East peace treaty between Anwar Sadat and Menachem Begin at Camp David. He ceded access to the Panama canal, angering conservatives who thought he was giving away an American asset. Through the Alaska Natural Interests Lands Conservation Act, he doubled the national park system and conserved over 100m acres of land – the most sweeping expansion of conserved land in American history. Carter was right on asking us to drive less, to reduce our dependence on foreign oil, to focus on conservation and renewable energy. Not only was Carter’s vision a path not taken, it was a path mocked. Reagan removed the solar panels from the White House, politicized the environmental movement and painted it as a fringe endeavor. Carter was a trained engineer who believed in science. He understood things on a global scale, and believed in forecasting. Preparing for the long run is rare in politics. Bergman on Carter (Guardian) "],["law.html", "23 Law 23.1 Earth System Law", " 23 Law 23.1 Earth System Law Earth system law is a legal paradigm that aligns with, and responds to, the Earth system’s functional, spatial, and temporal complexities, including physical and societal dynamics, at the planetary scale. While no comprehensive and representative juridical framework currently exists that embraces gov­ ernmentality concerning the Earth system let alone the dynamics of the global hydrological cycle – legal systems are important as they constitute the formal rules that underscore society’s rights and obligations, established by public legislative bodies, for shaping behaviors and establishing sanctions for violations through the judiciary. In this context, the Earth system law paradigm provides a useful set of guidelines for institutional reform that are responsive to the normative, ethical, and regulatory challenges of the Anthropocene 23.1.1 Hydrological Cycle Ahlstrom Abstract The global hydrological cycle is characterized by complex interdependencies and self-regulating feedbacks that keep water in an ever-evolving state of flux at local, regional, and global levels. Increasingly, the scale of human impacts in the Anthropocene is altering the dynamics of this cycle, which presents additional challenges for water governance. “Earth system law” provides an important approach for addressing gaps in governance that arise from the mismatch between the global hydrological cycle and dispersed regulatory architecture across institutions and geographic regions. In this article, we articulate the potential for Earth system law to account for core hydrological problems that complicate water governance, including delay, redistribution, intertwinements, permanence, and scale. Through merging concepts from Earth system law with existing policy and legal principles, we frame an approach for addressing hydrological problems in the Anthropocene and strengthening institutional fit between established governance systems and the global hydrological cycle. We discuss how such an approach can be applied, and the challenges and implications for governing water as a cycle and complex social-hydrological system, both in research and practice. Ahlstrom Memo A key challenge to water governance in the Anthropocene is improving institutional fit for addressing the global roots of core hydrological problems, while preserving gains in representation, institutional power, and other successes that have accompanied recent governance reforms. Earth system law provides a valuable framing for addressing uncertainty that recognizes the multi-level and inter­ connected characteristics of hydrological flows and the polycentric governance structures that have evolved to manage water resources at multiple scales. The global hydrological cycle is characterized by complex in­ terdependencies and self-regulating feedbacks that keep water in an ever-evolving state of flux at local, regional, and global levels. Increas­ ingly, the scale of human impacts in the Anthropocene is altering the dynamics of this cycle, and legal regimes for governing water are not presently able to address these challenges effectively. Drawing on research of complex social-hydrological systems, we illustrate the pressing need to integrate scientific understanding of the global hy­ drological cycle from the physical sciences with insights from the broad environmental governance and legal scholarship. We argue that there is not only a need to address “institutional fit” between global hydrological processes and legal and regulatory systems, but that this disconnect – and the consequences thereof – is deeply rooted in the Anthropocene. New modes of thinking are therefore required to develop solutions for governing water as a cycle and integrated social-hydrological system. Ahlstrom (2021) Earth system law social-hydrological systems [(pdf)](pdf/Ahlstrom_2021_Earth_system_law_social-hydrological_systems.pdf "],["sustainabilty-transition.html", "24 Sustainabilty Transition", " 24 Sustainabilty Transition Feola Abstract Sustainability transition research (STR) has failed to engage in any significant analyses or cri- tiques of capitalism. This article argues that capitalism is not a ‘landscape’ factor, but rather permeates the workings of socio-technical systems in ways that must be recognised both for elaborating rigorous accounts of transition trajectories and for enhancing the capacity of STR to support future societal sustainability transitions. This argument is developed specifically in re- lation to the three challenges of STR: the analysis of the actual sustainability of sustainability transitions, the application of transition theory to cases in the Global South, and the move to- wards a forward-looking STR. The article identifies three main implications of this argument with respect to interdisciplinarity, the validity of current theoretical frameworks, and the practice of STR. Ultimately, the article invites STR scholars to be more openly reflexive not only about possible theoretical biases, but also regarding their own roles in society. Feola Memo As a scientific field with roots in innovation, science and technology studies, and evolutionary economics, STR has essentially taken capitalism for granted. In carving out its space at the ‘meso-level’, STR has generally viewed capitalism at the landscape level in the much used multi-level perspective (MLP) framework (Geels, 2002). This strategy might help to distinguish STR from other approaches to studying societal transitions and transformations (Feola, 2015). Indeed, STR has achieved great depth of understanding of transitions from this perspective, thus complementing the understandings generated by other approaches to studying non-linear societal change (e.g., Fischer-Kowalski and Rotmans, 2009; Hölscher et al., 2018). However, I contend that STR omits capitalism at its own peril. Capitalism is more than an additional ‘landscape’ factor, and its core elements are not neutral givens, but rather defining elements of capitalist socio-technical systems (Kostakis et al., 2016; Wilhite, 2016). Capitalism permeates the workings and logics of socio-technical systems in ways that are critical both in the elaboration of rigorous accounts of transition trajectories and for the capacity of STR to support future societal sustainability transitions. To take capitalism as an implicit given in STR implies the impossibility of a serious analytical examination of its economic, political, social and cultural conditions and dynamics, its diversity, its influence on sustainability transitions in different contexts, and the possibility that sustainability transitions might involve potentially fundamental changes in the capitalist system. Blindness to capitalism also risks a return to an idealised image of the capitalist economy, which will constrain, rather than support STR. Capitalism Definition Capitalism is defined in this paper as an historically specific form of social and economic organisation, which is characterised economically by the private property of the means of production, the freedom to pursue economic gains through production and the market, the transformation of labour power into a commodity, the owners’ control of the means of production and the destination of value generated through production, and the generalisation of production and exchange of commodities. Capitalism Dynamics The most fundamental dynamics of capitalism relate to the imperative of capital accumulation. Strategies for capital accumulation include the externalization of costs, the lowering of labour costs, and the search for surplus value through the penetration of capitalist relations (commodification) in biophysical and human bodily and emotional life spheres. Privatization and commodification are often accompanied by the enclosure of biophysical and other resources in processes of ac- cumulation by dispossession, which may entail economic and extra-economic means, including violence. The process of accumulation is characterized by the concentration of capital and by exclusionary social relations and rising levels of inequality. Other strategies for capital accumulation are the geographical expansion of the market economy and the displacement of capital over space and time. Capitalism is ‘constituted’ by space-time arrangements in which ‘time and space work together in ways particular to the capitalist mode of producing, distributing, selling, consuming and disposing of commodities’ Capitalism Architecture Capitalism also entails a ‘more comprehensive cultural, social and political architecture’. In other words, accumulation depends not only on economic structures and strategies, but on extra-economic ones. Culturally, capitalism permeates and shapes individual and collective identities and relations beyond the economic sphere, and includes the principles of competition, individualisation, rationalisation, commodification of human and non-human beings, and the imaginary of progress based on endless accumulation. Politically, capitalism rests on state structures that participate in its reproduction both in periods of stability and crisis. The state in a capitalist system is a ‘strategic field’; it reflects and mediates capitalist power relations through regulation, discourses, and material resources; it often undertakes unprofitable activities that capital does not undertake, and it obtains revenues from taxation thus ultimately depending on continuous economic growth for its stability. Feola (2021) Capitalism in sustainability transitions research: Time for a critical turn? (pdf) "],["asteroids.html", "25 Asteroids", " 25 Asteroids Nasa has given Earth the all clear on the chances of an asteroid called Apophis hitting our planet any time in the next century, having worried space scientists for over 15 years. The 340-metre (1,100ft) chunk of space rock hit the headlines in 2004 after its discovery led to some worrying forecasts about its orbit. It became a “poster child for hazardous asteroids”, according to one Nasa expert. It was supposed to come frighteningly close in 2029 and again in 2036. Nasa ruled out any chance of a strike during those two close approaches a while ago, but a potential 2068 collision still loomed. But new telescope observations mean that collision has been ruled out and Apophis has been officially taken off the US space agency’s asteroid “risk list”. Apophis will come within 32,000km (20,000 miles) of Earth on Friday 13 April 2029, enabling astronomers to get a good look. That is about one-tenth of the distance to the moon and closer than the communication satellites that encircle the Earth at 36,000km. Although most asteroids are found in the belt of space between Mars and Jupiter, not all of them reside there. Apophis belongs to a group known as the Aten family. These do not belong to the asteroid belt and spend most of their time inside the orbit of the Earth, placing them between our planet and the sun. That makes them particularly dangerous because they spend the majority of their orbit close to the sun, whose overwhelming glare obscures them to telescopes on Earth – rather like a second world war fighter ace approaching out of the sun. Apophis (The Guardian) "],["brain-and-mind.html", "26 Brain and Mind", " 26 Brain and Mind Austin on McGilchrist Right Brain Therapy Western civilization might benefit from a cultural-scale version of the Right Brain Therapy The two brain hemispheres uphold distinct ways of attending to the world, and while both are beneficial, they are intrinsically in tension with each other. Hence, successful cognition and living would seem to require the ability to juggle or balance the two modes of perception, but as McGilchrist documents in the second half of his book, the long record of Western cultural history points to an inexorable, accelerating, rise of left-brain thinking and behaviour displacing a right-brain awareness and way of being. Through the reinforcing dynamic of culture, we have fallen into what might be termed a ‘left-brain runaway’ in which we make the world with left-brain ideas such that culture encourages and rewards yet more left-brain thinking, and so on, now in seemingly unstoppable fashion. A foundational difference is the left brain’s inclination to divide versus the right brain’s capacity to see things whole. The bihemispheric brain constitutes a ‘unity of the idea of unity and the idea of division.’ McGilchrist’s thesis is both timeless and timely. It is timeless in that it identifies an innate tension in the cognition required of successful living organisms, and locates that, for us and other mammals, in our bi-hemispheric brain structure. One of the powerful things McGilchrist accomplishes is to pick up the dualism ropes from where Descartes placed them – between mind and body – and re-lay them down the longitudinal fissure separating the [brain] hemispheres. This divides the world a new between rival capacities to perceive dualistically or holistically. We do have a Cartesian mind that can formulate a mind-body split, but we also have a non-Cartesian mind that senses this might be a trap. Both seem useful, even though they lead us to different extremes. The left brain divides and divides again to end up chasing the Higgs Boson. The right brain patterns gestalt after gestalt eventually reaching Lovelock’s Gaia Hypothesis – ‘it is all one thing’ including this mind thinking this thought – and from there, perhaps, out beyond ‘science’ and towards the sacred. While the idea of a whole civilization in the grip of a runaway cognitive dynamic may seem fanciful, it becomes easier to comprehend when one recognizes the reinforcing nature of culture. Runaway thinking can happen at the level of a whole culture because minds and culture constitute a loop in a complex system. Peter Richerson and Robert Boyd, authorities on cultural evolution, define culture as: ‘…socially learned information stored in individuals’ brains that is capable of affecting behaviour’ While we ‘scaffold’ culture with many cultural artifacts – literature, buildings, laws and more – the main locus of a living human culture is in the plastic brains of its human members. The reflexivity between our plastic brains and the plastic culture in which those brains are fully immersed – the social ‘imaginary’ – constitutes a feedback loop in which mind shapes culture shapes mind, a so-called ‘mind-culture co-evolution’. Runaway Culture This mind-culture-mind loop now exhibits runaway dynamics: our left-brain ‘way of being’ has brought forth a left-brain culture that encourages and rewards further left-brain thinking, and so it goes on. Bit by bit, we lose our capacity to ‘be in the world’ in a more meditative, less calculative, way that is the preserve of the right brain and plausibly a greater part of how we used to be. Reductionism earned its spurs because it proved spectacularly successful at explaining the behaviour of ‘dead’ things that were the dominant objects of enquiry at the dawn of the Scientific Revolution. Alas, those early successes profoundly shaped the way we believed all science should be conducted, so that we applied an intrinsically reductionist scientific method to a more complex ‘living’ natural world, including ultimately ourselves. The problem, as is increasingly understood, is that complex systems exhibit emergent properties, which cannot be anticipated even from complete knowledge of the parts, but only discerned from observation of the whole. We conceived of Homo Economicus and built a logical model of the world around that conception and have ever since been trying to live up – live down, really – to that self-image. We have been striving to make our behaviour fit a simple model rather than adjusting our models to a new comprehension of our complex behaviour. Effectively, a subplot of our broader mind-culture co-evolution has been a ‘mind-market coevolution’, in which human minds have made markets have shaped minds. Collective Right Brain Therapy Also alert to latest developments in neuroscience is American psychologist, Allan Schore, who has crystallized the implications for individual mental wellbeing in his concept of ‘right brain therapy’. Rooted in new understanding of the role a holistic, emotionally dominant, right brain plays in supporting mental health it emphasizes empathic connection and the significance of non-verbal communication in achieving interpersonal awareness and understanding. It is effectively a program of rehabilitation for a right brain withered by modern culture. McGilchrist – who cites Schore’s work – does not use his term but leads us towards the idea that civilization’s largest problems might benefit from collective right brain therapy. Where Einstein said you cannot solve problems with the same sort of thinking that created them, what McGilchrist effectively says is that you cannot solve problems with the same brain hemisphere that created them. While our sustainability crisis presents as rising sea-levels, shrinking forests and disappearing species, the front line of our struggle is the corpus callosum that divides the human left and right brain. This is where our sustainability crisis will ultimately be resolved, or not. Schore, too, argues for the primacy of the right brain, which is chronologically foundational in the human lifespan.35 It is firmly in the driving seat during the crucial development period of infancy, before the slower-developing left brain comes ‘online’ and language skills are acquired. Moreover, throughout life, the right brain remains the dominant hemisphere for monitoring one’s own emotions and those of others’, and so for interpersonal connection. However, and very curious this, the left brain seems entirely unaware of its dependency upon its neighbouring hemisphere. Their different ways of being in the world induce an intriguing asymmetry: a right brain alive to the connection in the world knows that it needs the left brain, while a left brain intent on division seems not to know that it needs the right brain. The left hemisphere tends to positive [or reinforcing] feedback, and we can become stuck. The right hemisphere…is capable of freeing us through negative [or modulating] feedback.’ Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis "],["antifragile-therapy.html", "27 Antifragile Therapy 27.1 Optimal Cancer Treatment", " 27 Antifragile Therapy 27.1 Optimal Cancer Treatment Abstract West Antifragility is a recently coined word using to describe the opposite of fragility. Sys- tems or organisms can be described as antifragile if they derive a benefit from systemic variability, volatility, randomness, or disorder. Herein, we introduce a mathematical framework to quantify the fragility or antifragility of cancer cell lines in response to treatment variability. This framework enables straightforward prediction of the optimal dose treatment schedule for a range of treatment schedules with identical cu- mulative dose. We apply this framework to non-small-cell lung cancer cell lines with evolved resistance to ten anti-cancer drugs. We show the utility of this antifragile framework when applied to 1) treatment resistance, 2) collateral sensitivity of sequen- tial monotherapies, and 3) combination therapies. Over the past decades it has become increasingly clear that the benefit of a cancer therapeutic agent is determined not only by its molecular action but also by its schedule. However, because of the costs associated with clinical trials and the combinatorial size of the potential search space, optimal treatment strategies remain elusive. As a result, most therapies are administered in a fashion to maximize cell kill, meaning they are given as frequently as is logistically feasible (weekly for chemotherapies, daily for orally available targeted therapies) and at the maximum dose patients can safely tolerate. At the same time, translating into the clinic alternative schedules which have been shown to perform better in vitro, in vivo, and/or in silico has been challenging, and has failed on several occasions. For example, even though “bolus-dosing” of EGFR inhibitor for EGFR-Mutant NSCLC, in which daily low dose treatment is supplemented with a weekly high dose of therapy, was shown to better control therapy resistance than the standard-of-care continuous schedule in a mathematical model 27 , as well as in in vitro 27 and in in vivo experiments 28 , it failed to do so in patients 37 . One reason for this discrepancy is the fact that it is often difficult to understand why a given schedule is optimal. In this paper, we have shown how that the theory of antifragility, pioneered in financial risk management, provides a general tool to compare schedules in an intuitive yet formal fashion. In particular, we have demonstrated that the curvature of the dose response curve determines whether regimens should seek to maintain a constant treatment level, or should induce fluctuations between high and low periods of exposure. Importantly, this assessment can be made graphically and does not require specialist knowledge of complex optimization techniques. Moreover, it is easily generalizable as it can be applied to dose response curves obtained from any experimental or theoretical model system. West (2020) Antifragile Therapy (bioRxiv) (pdf) "],["civilization.html", "28 Civilization 28.1 Fermi Paradox and Drake Equation", " 28 Civilization “Before our white brothers arrived to make us civilized men, we didn’t have any kind of prison. Because of this, we had no delinquents. Without a prison, there can be no delinquents. We had no locks nor keys and therefore among us there were no thieves. When someone was so poor that he couldn’t afford a horse, a tent or a blanket, he would, in that case, receive it all as a gift. We were too uncivilized to give great importance to private property. We didn’t know any kind of money and consequently, the value of a human being was not determined by his wealth. We had no written laws laid down, no lawyers, no politicians, therefore we were not able to cheat and swindle one another. We were really in bad shape before the white men arrived and I don’t know how to explain how we were able to manage without these fundamental things that (so they tell us) are so necessary for a civilized society.” - John (Fire) Lame Deer, Sioux Lakota - 1903-1976 28.1 Fermi Paradox and Drake Equation Dessler Enrico Fermi, one of the giants of 20th century physics, famously asked, “Where is everybody?” In this case, “everybody” referred to extraterrestrial aliens: given the vastness of the universe and long time it has existed, there should be advanced extraterrestrial civilizations throughout the universe. Why don’t we see any? Enter the Drake Equation, devised by astronomer Frank Drake to put Fermi’s paradox on more quantitative grounds. It provided a framework on how to estimate how many active, communicative extraterrestrial civilizations there are in the Milky Way galaxy right now. The approach is order-of-magnitude estimation at its best: break the desired quantity into the product of simpler terms, some that you can estimate and others that you can at least put a bound on. Here’s how Drake did it: We can make educated guesses for each of these parameters: R*: Recent estimates suggest about 2 new stars are formed in the Milky Way every year. fP: Most stars have planets, so we’ll estimate this term to be 0.9. Ne: This is speculative, but let’s assume 1 in 5 of stars with planets has a planet in the habitable zone, so this term is 0.2. fl, fi, fc : These terms must be in the range zero to one, but beyond that no one has any idea, so let’s make a wild guess that they’re all equal to 0.1. L: This is also unknown, but let’s assume that advanced civilizations last for ten million years. Multiplying these terms together produces an estimate that there are 3,600 advanced civilizations in the Milky Way. Note that N is proportional to L, the average lifespan of an advanced civilization. If, instead of 10,000,000 years, advanced civilizations last only 10,000 years, then N drops to 3.6 intelligent civilizations in the Milky Way. And if advanced civilizations last on average only 1,000 years, then there would only be 0.36 of them in the Milky Way. Meaning we might be the only one. This could be the answer to Fermi’s Paradox. What does this have to do with climate change? Climate change is the kind of test that civilizations have to pass if they want to last a long time. William Nordhaus, in his Nobel Prize acceptance speech, highlighted this reality by noting that: Technological change raised humans out of Stone Age living standards. Climate change threatens, in the most extreme scenarios, to return us economically whence we came. We are failing this test. And it’s not the only one: When a non-negligible fraction of society responds to a virus pandemic by eating horse paste instead of getting a safe and effective vaccine, it’s easy to see how it could all be over for humanity in a 1,000 years, if not sooner. Thus, one plausible answer to Fermi’s question, “Where are they,” is that their own stupidity and greed killed them. It’s conceivable that climate change is a threat that most civilizations need to deal with. Advanced civilizations civilizations inherently require energy, which initially might be sourced from the combustion of carbon-based materials. That would lead to the emission of greenhouse gases, which would lead to climate change. So perhaps, in climate change, we’re facing the same test that most or all advanced civilizations need to pass. But climate change is just one challenge. We also need to deal with threats of nuclear war, pandemics, economists, and many potential disasters, all of which could knock humanity off its pedestal. If we fail any of those tests, then some future alien society might one day wonder where we are. Dessler (2024) “Where is everybody?”: The Fermi Paradox, the Drake Equation, and climate change "],["consciousness.html", "29 Consciousness", " 29 Consciousness The Economist IN AN IDEAL world, science would work by making unambiguous predictions based on a theory, and then testing those predictions in ways that leave no wiggle room about which are right and which wrong. In practice, it rarely happens quite like that, especially in biology. But, the coronavirus always permitting, a group of neuroscientists plan to apply this method over the course of the coming year to the most mysterious biological phenomenon of all: human consciousness. They are organising what is known as an “adversarial collab­oration competition” between two hypotheses about how consciousness is generated in brains. The contestants are Giulio Tononi’s integrated information theory (IIT) and Stanislas Dehaene’s global workspace theory (GWT). The competition was dreamed up at the Allen Institute for Brain Science, in Seattle, and is being paid for by the Templeton World Charity Foundation. The practical side of things is being led by Lucia Melloni of the Max Planck Institute for Empirical Aesthetics, in Frankfurt. Dr Tononi, of the University of Wisconsin, Madison, thinks consciousness is a direct consequence of the interconnectedness of neurons within brains. IIT argues that the more the neurons in a being’s brain interact with one another, and the more complex the resulting network is, the more the being in question feels itself to be conscious. Because the parts of a human brain where neuronal connectivity is most complex are the sensoryprocessing areas (in particular, the visual cortex) at the back of the organ, these, IIT predicts, are where human consciousness will be seated. Dr Dehaene, who works at the Collège de France, in Paris, reckons by contrast that the action, when it comes to consciousness, involves a network of brain areas—particularly the prefrontal cortex. This part of the brain receives sensory information from elsewhere in the organ, evaluates and edits it, and then sends the edited version out to other areas, to be acted on. It is the activity of evaluating, editing and broadcasting which, according to GWT, generates feelings of consciousness. One difference between IIT and GWT, accordingly, is that the former is a “bottom up” explanation, whereas the latter is “top down”. Supporters of IIT think consciousness is an emergent property of neural complexity that can exist to different degrees, and could, in principle, be measured as a number (for which they use the Greek letter phi). GWT-type consciousness, by contrast, is more of an all-or-nothing affair. Distinguishing between the two would be a big step forward for science. It would also have implications for how easy it might be to build a computer that was conscious. The competition’s experiments will be conducted on 500 volunteers at six sites in America, Britain, China and the Netherlands. Three techniques will be used: functional magnetic-resonance imaging (fMRI), magnetoencephalography (MEG) and electrocorticography (ECoG). fMRI measures blood flow, which in turn relates to the level of activity in the part of the brain being examined (the more blood that is flowing through an area, the more active it is). MEG records fluctuating magnetic fields produced by electrical activity in the brain. Neither of these is intrusive. ECoG, however, records electrical activity directly from the surface of the cerebral cortex. This part of the project will therefore rely on volunteers who are undergoing brain surgery for reasons, such as to treat epilepsy, which require the patient to remain conscious throughout the procedure. Half the data collected will be analysed immediately, by researchers independent of the protagonists, who have no axe to grind for either side. The other half will be locked away for future reference, in case confirmatory analyses need to be done. In the spirit of adversarial collaboration, the two sides have hammered out a set of tests that both agree should produce different results, depending on which theory is correct. These depend on the fact that GWT predicts brain activity only when attention is actively being paid to something, whereas mere conscious awareness of something is enough for IIT to predict activity. The tests’ details vary (some involve stationary letters, objects or faces on a screen while others have shapes moving across the screen). In all of them, though, the distinction between attention and awareness is clear—and so, therefore, are the predictions. Whatever emerges from the experiment will not be anywhere near a definitive explanation of consciousness. In particular, it will not address the “hard” problem of the phenomenon: the “feeling of what it is like to be something” that was raised in 1974 by Thomas Nagel, an American philosopher, in an essay titled “What is it like to be a bat?” It will, however, by providing what are known as neural correlates of conscious experience, point to directions in which future investigations might usefully travel. Geoffrey Carr, The Economist (Nov 17th 2020) Memo Burkeman The feeling of being inside your head, looking out, or of having a soul. One spring morning in Tucson, Arizona, in 1994, an unknown philosopher named David Chalmers got up to give a talk on consciousness, by which he meant the feeling of being inside your head, looking out – or, to use the kind of language that might give a neuroscientist an aneurysm, of having a soul. Though he didn’t realise it at the time, the young Australian academic was about to ignite a war between philosophers and scientists, by drawing attention to a central mystery of human life – perhaps the central mystery of human life – and revealing how embarrassingly far they were from solving it. Hard Problem of Consciousness – and it’s this: why on earth should all those complicated brain processes feel like anything from the inside? Why aren’t we just brilliant robots, capable of retaining information, of responding to noises and smells and hot saucepans, but dark inside, lacking an inner life? Philosophers had pondered the so-called “mind-body problem” for centuries. What the hell is this that we’re dealing with here? Conscious sensations, such as pain, don’t really exist, no matter what I felt as I hopped in anguish around the kitchen; or, alternatively, that plants and trees must also be conscious. In recent years, a handful of neuroscientists have come to believe that it may finally be about to be solved – but only if we are willing to accept the profoundly unsettling conclusion that computers or the internet might soon become conscious, too. Cartesian dualism Science had been vigorously attempting to ignore the problem of consciousness for a long time. The source of the animosity dates back to the 1600s, when René Descartes identified the dilemma that would tie scholars in knots for years to come. On the one hand, Descartes realised, nothing is more obvious and undeniable than the fact that you’re conscious. In theory, everything else you think you know about the world could be an elaborate illusion cooked up to deceive you – at this point, present-day writers invariably invoke The Matrix – but your consciousness itself can’t be illusory. On the other hand, this most certain and familiar of phenomena obeys none of the usual rules of science. It doesn’t seem to be physical. It can’t be observed, except from within, by the conscious person. It can’t even really be described. The mind, Descartes concluded, must be made of some special, immaterial stuff that didn’t abide by the laws of nature; it had been bequeathed to us by God. Cartesian dualism, remained the governing assumption into the 18th century and the early days of modern brain study. But it was always bound to grow unacceptable to an increasingly secular scientific establishment that took physicalism – the position that only physical things exist – as its most basic principle. And yet, even as neuroscience gathered pace in the 20th century, no convincing alternative explanation was forthcoming. So little by little, the topic became taboo. Few people doubted that the brain and mind were very closely linked: if you question this, try stabbing your brain repeatedly with a kitchen knife, and see what happens to your consciousness. But how they were linked – or if they were somehow exactly the same thing – seemed a mystery best left to philosophers in their armchairs. As late as 1989, writing in the International Dictionary of Psychology, the British psychologist Stuart Sutherland could irascibly declare of consciousness that “it is impossible to specify what it is, what it does, or why it evolved. Nothing worth reading has been written on it.” It was only in 1990 that Francis Crick, the joint discoverer of the double helix, used his position of eminence to break ranks. Neuroscience was far enough along by now, he declared in a slightly tetchy paper co-written with Christof Koch, that consciousness could no longer be ignored. “It is remarkable,” they began, “that most of the work in both cognitive science and the neurosciences makes no reference to consciousness” – partly, they suspected, “because most workers in these areas cannot see any useful way of approaching the problem”. They presented their own “sketch of a theory”, arguing that certain neurons, firing at certain frequencies, might somehow be the cause of our inner awareness – though it was not clear how. Consciousness can’t just be made of ordinary physical atoms. So consciousness must, somehow, be something extra – an additional ingredient in nature. It may be true that most of us, in our daily lives, think of consciousness as something over and above our physical being – as if your mind were “a chauffeur inside your own body”, to quote the spiritual author Alan Watts. But to accept this as a scientific principle would mean rewriting the laws of physics. Everything we know about the universe tells us that reality consists only of physical things. If this non-physical mental stuff did exist, how could it cause physical things to happen – as when the feeling of pain causes me to jerk my fingers away from the saucepan’s edge? Science has dropped tantalising hints that this spooky extra ingredient might be real. Daniel Dennett, the high-profile atheist and professor at Tufts University outside Boston, argues that consciousness, as we think of it, is an illusion: there just isn’t anything in addition to the spongy stuff of the brain, and that spongy stuff doesn’t actually give rise to something called consciousness. Common sense may tell us there’s a subjective world of inner experience – but then common sense told us that the sun orbits the Earth, and that the world was flat. Consciousness, according to Dennett’s theory, is like a conjuring trick: the normal functioning of the brain just makes it look as if there is something non-physical going on. To look for a real, substantive thing called consciousness, Dennett argues, is as silly as insisting that characters in novels, such as Sherlock Holmes or Harry Potter, must be made up of a peculiar substance named “fictoplasm”; the idea is absurd and unnecessary, since the characters do not exist to begin with. To Dennett’s opponents, he is simply denying the existence of something everyone knows for certain. The Hard Problem is nonsense, kept alive by philosophers who fear that science might be about to eliminate one of the puzzles that has kept them gainfully employed for years. Life is just the label we give to certain kinds of objects that can grow and reproduce. Eventually, neuroscience will show that consciousness is just brain states. Solutions have regularly been floated: the literature is awash in references to “global workspace theory”, “ego tunnels”, “microtubules”, and speculation that quantum theory may provide a way forward. But the intractability of the arguments has caused some thinkers, such as Colin McGinn, to raise an intriguing if ultimately defeatist possibility: what if we’re just constitutionally incapable of ever solving the Hard Problem? After all, our brains evolved to help us solve down-to-earth problems of survival and reproduction; there is no particular reason to assume they should be capable of cracking every big philosophical puzzle. There’s actually no mystery to why consciousness hasn’t been explained: it’s that humans aren’t up to the job. Panpsychism “Panpsychism”, the dizzying notion that everything in the universe might be conscious, or at least potentially conscious, or conscious when put into certain configurations. If humans have it, and apes have it, and dogs and pigs probably have it, and maybe birds, too – well, where does it stop? Physicists have no problem accepting that certain fundamental aspects of reality – such as space, mass, or electrical charge – just do exist. They can’t be explained as being the result of anything else. Explanations have to stop somewhere. The panpsychist hunch is that consciousness could be like that, too – and that if it is, there is no particular reason to assume that it only occurs in certain kinds of matter. Anything at all could be conscious, providing that the information it contains is sufficiently interconnected and organised. The human brain certainly fits the bill; so do the brains of cats and dogs, though their consciousness probably doesn’t resemble ours. But in principle the same might apply to the internet, or a smartphone, or a thermostat. Integrated information theory Unlike the vast majority of musings on the Hard Problem, moreover, Tononi and Koch’s “integrated information theory” has actually been tested. A team of researchers led by Tononi has designed a device that stimulates the brain with electrical voltage, to measure how interconnected and organised – how “integrated” – its neural circuits are. Sure enough, when people fall into a deep sleep, or receive an injection of anaesthetic, as they slip into unconsciousness, the device demonstrates that their brain integration declines, too. Among patients suffering “locked-in syndrome” – who are as conscious as the rest of us – levels of brain integration remain high; among patients in coma – who aren’t – it doesn’t. Gather enough of this kind of evidence, Koch argues and in theory you could take any device, measure the complexity of the information contained in it, then deduce whether or not it was conscious. But even if one were willing to accept the perplexing claim that a smartphone could be conscious, could you ever know that it was true? Surely only the smartphone itself could ever know that? Koch shrugged. “It’s like black holes,” he said. “I’ve never been in a black hole. Personally, I have no experience of black holes. But the theory [that predicts black holes] seems always to be true, so I tend to accept it.” It would be poetic – albeit deeply frustrating – were it ultimately to prove that the one thing the human mind is incapable of comprehending is itself. Burkeman: Mystery of Consciousness (Guardian 2015) "],["crime.html", "30 Crime 30.1 Free Will", " 30 Crime In the 1680s, a Huron-Wendat statesman named Kondiaronk, who had been to Europe and was intimately familiar with French and English settler society, engaged in a series of debates with the French governor of Quebec, and one of his chief aides, a certain Lahontan. In them he presented the argument that punitive law and the whole appa-ratus of the state exist not because of some fundamental flaw in human nature but owing to the existence of another set of institutions—private property, money—that by their very nature drive people to act in such ways as to make coercive measures necessary. Mutual Aid Kropotkin 30.1 Free Will Memo Burkeman A growing chorus of scientists and philosophers argue that free will does not exist. They argue that our choices are determined by forces beyond our ultimate control – perhaps even predetermined all the way back to the big bang – and that therefore nobody is ever wholly responsible for their actions. The experience of possessing free will – the feeling that we are the authors of our choices – is so utterly basic to everyone’s existence that it can be hard to get enough mental distance to see what’s going on. Suppose you find yourself feeling moderately hungry one afternoon, so you walk to the fruit bowl in your kitchen, where you see one apple and one banana. As it happens, you choose the banana. But it seems absolutely obvious that you were free to choose the apple – or neither, or both – instead. That’s free will: were you to rewind the tape of world history, to the instant just before you made your decision, with everything in the universe exactly the same, you’d have been able to make a different one. “This sort of free will is ruled out, simply and decisively, by the laws of physics,” says one of the most strident of the free will sceptics, the evolutionary biologist Jerry Coyne. Leading psychologists such as Steven Pinker and Paul Bloom agree, as apparently did the late Stephen Hawking, along with numerous prominent neuroscientists, including VS Ramachandran, who called free will “an inherently flawed and incoherent concept”. Arguments against free will go back millennia, but the latest resurgence of scepticism has been driven by advances in neuroscience during the past few decades. Now that it’s possible to observe – thanks to neuroimaging – the physical brain activity associated with our decisions, it’s easier to think of those decisions as just another part of the mechanics of the material universe, in which “free will” plays no role. And from the 1980s onwards, various specific neuroscientific findings have offered troubling clues that our so-called free choices might actually originate in our brains several milliseconds, or even much longer, before we’re first aware of even thinking of them. We would be forced to conclude that it was unreasonable ever to praise or blame anyone for their actions, since they weren’t truly responsible for deciding to do them “Illusionism”, the idea that although free will as conventionally defined is unreal, it’s crucial people go on believing otherwise Start with what seems like an obvious truth: anything that happens in the world, ever, must have been completely caused by things that happened before it. And those things must have been caused by things that happened before them – and so on, backwards to the dawn of time: cause after cause after cause, all of them following the predictable laws of nature, even if we haven’t figured all of those laws out yet. It’s easy enough to grasp this in the context of the straightforwardly physical world of rocks and rivers and internal combustion engines. But surely “one thing leads to another” in the world of decisions and intentions, too. Our decisions and intentions involve neural activity – and why would a neuron be exempt from the laws of physics any more than a rock? To have what’s known in the scholarly jargon as “contra-causal” free will – so that if you rewound the tape of history back to the moment of choice, you could make a different choice – you’d somehow have to slip outside physical reality. You just are some of the atoms in the universe, governed by the same predictable laws as all the rest. It was the French polymath Pierre-Simon Laplace, writing in 1814, who most succinctly expressed the puzzle here: how can there be free will, in a universe where events just crank forwards like clockwork? His thought experiment is known as Laplace’s demon, and his argument went as follows: if some hypothetical ultra-intelligent being – or demon – could somehow know the position of every atom in the universe at a single point in time, along with all the laws that governed their interactions, it could predict the future in its entirety. It’s true that since Laplace’s day, findings in quantum physics have indicated that some events, at the level of atoms and electrons, are genuinely random, which means they would be impossible to predict in advance, even by some hypothetical megabrain. But few people involved in the free will debate think that makes a critical difference. Those tiny fluctuations probably have little relevant impact on life at the scale we live it, as human beings. And in any case, there’s no more freedom in being subject to the random behaviours of electrons than there is in being the slave of predetermined causal laws. By far the most unsettling implication of the case against free will, for most who encounter it, is what it seems to say about morality: that nobody, ever, truly deserves reward or punishment for what they do, because what they do is the result of blind deterministic forces (plus maybe a little quantum randomness). Consider the case of Charles Whitman. Just after midnight on 1 August 1966, Whitman – an outgoing and apparently stable 25-year-old former US Marine – drove to his mother’s apartment in Austin, Texas, where he stabbed her to death. He returned home, where he killed his wife in the same manner. Later that day, he took an assortment of weapons to the top of a high building on the campus of the University of Texas, where he began shooting randomly for about an hour and a half. By the time Whitman was killed by police, 12 more people were dead, and one more died of his injuries years afterwards – a spree that remains the US’s 10th worst mass shooting. Within hours of the massacre, the authorities discovered a note that Whitman had typed the night before. “I don’t quite understand what compels me to type this letter,” he wrote. “Perhaps it is to leave some vague reason for the actions I have recently performed. I don’t really understand myself these days. I am supposed to be an average reasonable and intelligent young man. However, lately (I can’t recall when it started) I have been a victim of many unusual and irrational thoughts [which] constantly recur, and it requires a tremendous mental effort to concentrate on useful and progressive tasks … After my death I wish that an autopsy would be performed to see if there is any visible physical disorder.” Following the first two murders, he added a coda: “Maybe research can prevent further tragedies of this type.” An autopsy was performed, revealing the presence of a substantial brain tumour, pressing on Whitman’s amygdala, the part of the brain governing “fight or flight” responses to fear. It’s impossible to know if the brain tumour caused Whitman’s actions. What seems clear is that it certainly could have done so – and that almost everyone, on hearing about it, undergoes some shift in their attitude towards him. If you find the presence of a brain tumour in these cases in any way exculpatory, though, you face a difficult question: what’s so special about a brain tumour, as opposed to all the other ways in which people’s brains cause them to do things? A neurological disorder appears to be just a special case of physical events giving rise to thoughts and actions. What all this means is that retributive punishment – punishing a criminal because he deserves it, rather than to protect the public, or serve as a warning to others – can’t ever be justified. Retribution is central to all modern systems of criminal justice, yet ultimately it’s a moral injustice to hold someone responsible for actions that are beyond their control. It’s capricious. The “public health-quarantine” model of criminal justice would transform the institutions of punishment in a radically humane direction. You could still restrain a murderer, on the same rationale that you can require someone infected by Ebola to observe a quarantine: to protect the public. But you’d have no right to make the experience any more unpleasant than was strictly necessary for public protection. And you would be obliged to release them as soon as they no longer posed a threat. The main focus would be on redressing social problems to try stop crime happening in the first place – just as public health systems ought to focus on preventing epidemics happening to begin with. Burkeman: Is free will an illusion? (Guardian 2021) "],["energy-empires.html", "31 Energy Empires", " 31 Energy Empires Fix The history of empires’ is not written in the speeches and proclamations of elites. Instead, it’s written in the language of energy. Although the motivations for empire building differ between societies, the end result is always the same. A successful empire centralizes the flow of energy. This means that energy use (per person) in the empire’s core will dwarf energy use in the periphery. The degree that this is true marks the degree that the empire is successful. Energy use, then, provides a window into the rise and fall of empires. How can we estimate the energy use of early empires? We make an educated guess. That’s exactly what Ian Morris does in his book The Measure of Civilization. Keep in mind that Morris’ data is less of a measurement and more of a back-of-the-envelope guess. In his book Ages of Discord, Peter Turchin analyzes the long-waves of American social stability. His waves match (at least roughly) what’s written in the energy data. Turchin finds that social stability peaked after independence, but reached a trough by the Civil War. Stability rose again during the good times after World War II. But today, social stability is on the decline. After the quagmire of civil war, American fortunes rose again. By the turn of the 20th century, the American empire was in full swing. As revealed by energy consumption, the ‘American century’ lasted roughly 70 years (1900–1970). During this time, the typical American consumed about 6 times more energy than the world average. The peak of US supremacy came during World War II. At the height of its war machine, the US consumed roughly one third of the world’s energy. The end of US dominance came around 1970. The decline was halted, briefly, during the boom of the 1990s. But then the dotcom bubble burst, and the energy slide continued. The pinnacle of US empire has long passed. During the Middle Ages, China was the center of world civilization. But with the European renaissance, that would change. In the 19th century, Europe colonized China (although never as completely as it colonized neighboring India). Colonization eventually ended in the 20th century with the Chinese communist revolution. But this revolution didn’t end Chinese suffering. If anything, it exacerbated it. Still, China eventually emerged as an industrial power. In the last 40 years, its transformation has been remarkable. And the whole story is written in the ‘book of energy’. When Mao died, the Chinese government abandoned hardline command and control. Instead, it let (big) firms take some of the economic reigns. After this reform, the growth of Chinese energy use was spectacular. Chinese energy consumption has only recently surpassed the world average. Today, the typical Chinese citizen consumes about 1.3 times more energy than the world average. While China is on the rise, it has yet to catch up to the fading imperial power, the United States. Today, the typical American consumes about 3.8 times more energy than the world average. Although the US is in relative decline, it’s still a far wealthier nation (on average) than China. China is marked by a great urban-rural divide. In 2012, Chinese urbanites earned about 300% more than their rural counterparts. In contrast, American urbanites earn only 36% more than their rural counterparts.4 This divide means that speaking about ‘average Chinese energy use’ is misleading. Some Chinese live much like modern Americans. Others live more like Americans of the 18th century. In the language of world-systems theory, we’d say that China ‘includes its own periphery’. Blair Fix: Why America won’t be great again Thomson Nazi Energy Empire Energy has been central to geopolitics since the industrial revolution. Oil-fuelled American industrialisation induced frenzied fears in Europe. From the 1870s, the conjunction of American oil resources allied to a continental single market and a high-tariff customs union drove a European scramble for energy resources and land empires in Africa. After the discovery of oil in the Middle East at a time when the US was moving to an oil-based navy, Britain, France, and Germany competed over the crumbling Ottoman empire. Britain’s victory in that competition – helped by its ability to project land power from India into the Persian Gulf – left it with a Middle Eastern empire that was a pivotal feature of the Eurasian map until the late 1960s. Germany’s defeat was central to the Nazis’ expansionist ambitions. From the mid-1920s, Hitler became obsessed with American mass car society and the domestic oil supply that fuelled it. In retaining Ford and General Motors’ presence in Germany, Nazi industrial strategy used short-term technological dependency on the United States in order to bring American-level mass automated production to Germany. In Hitler’s geopolitical fantasy, the autobahns German producers had learned to build would stretch out across Eurasia under German control. During the Second World War, he conceived Operation Barbarossa as the German equivalent of the conquest of the American West, since Russia was where on Europe’s immediate borders there were large quantities of oil. Thompson "],["fertility.html", "32 Fertility", " 32 Fertility Abstract Roser The global average fertility rate is just below 2.5 children per woman today. Over the last 50 years the global fertility rate has halved. And over the course of the modernization of societies the number of children per woman decreases very substantially. In the pre-modern era of 4.5 to 7 children per woman were common. At that time the very high mortality at a young age kept population growth low. As health improves and the mortality in the population decreases we typically saw accelerated population growth. This rapid population growth then comes to an end as the fertility rate declines and approaches 2 children per woman. Max Roser: Fertility Rate "],["mathematics.html", "33 Mathematics 33.1 Riemann Zeta Function 33.2 Wolfram’s Ruliad 33.3 Motion as Contradiction", " 33 Mathematics 33.1 Riemann Zeta Function Ian Wright The relation between Riemann zeroes and primes is provable but mysterious. A bridge from perfect continuity to perfect discreteness. Excitement here about a new interpretation of zeroes. I’ll attempt to understand it, but I’m a relative amateur and this work is highly technical. Baez Pólya suggested that the nontrivial zeros of the Riemann zeta function should correspond to the eigenvalues of some interesting self-adjoint operator. Since then there’s been a lot of work looking for this operator. This might give a physics-inspired proof of the RH Baez Twitter Thread 33.2 Wolfram’s Ruliad Wolfram The Entangled Limit of Everything I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It’s yet another surprising construct that’s arisen from our Physics Project. And it’s one that I think has extremely deep implications—both in science and beyond. In many ways, the ruliad is a strange and profoundly abstract thing. But it’s something very universal—a kind of ultimate limit of all abstraction and generalization. And it encapsulates not only all formal possibilities but also everything about our physical universe—and everything we experience can be thought of as sampling that part of the ruliad that corresponds to our particular way of perceiving and interpreting the universe. We’re going to be able to say many things about the ruliad without engaging in all its technical details. (And—it should be said at the outset—we’re still only at the very beginning of nailing down those technical details and setting up the difficult mathematics and formalism they involve.) But to ground things here, let’s start with a slightly technical discussion of what the ruliad is. In the language of our Physics Project, it’s the ultimate limit of all rulial multiway systems. And as such, it traces out the entangled consequences of progressively applying all possible computational rules. In thinking about finding a fundamental theory of physics, one thing always bothered me. Imagine we successfully identify a rule that describes everything about our universe. Then the obvious next question will be: “Why this rule, and not another?” Well, how about if actually the universe in effect just runs every possible rule? What would this mean? It means that in a sense the “full story” of the universe is just the ruliad. But the ruliad contains everything that is computationally possible. So why then do we have the perception that the universe has specific laws, and that definite things happen in it? It all has to do with the fact that we are bounded observers, embedded within the ruliad. We never get to see the full ruliad; we just sample tiny parts of it, parsing them according to our particular methods of perception and analysis. And the crucial point is that for coherent observers like us, there are certain robust features that we will inevitably see in the ruliad. And these features turn out to include fundamental laws of our physics, in particular general relativity and quantum mechanics. How should we think about the ruliad mathematically? In many ways, the ruliad is more an object of metamathematics than of mathematics itself. For in talking about the effects of all possible rules, it in a sense transcends individual mathematical theories—to describe a kind of metatheory of all possible theories. Wolfram (2022) The Concept of the Ruliad 33.3 Motion as Contradiction Wright Scientific laws are almost always expressed as differential systems. A differential system describes a causal structure with the power to control a variable. Integration, in effect, activates the power, generating change in that variable over time. The causal structure is a feedback loop that changes what it controls, and therefore what it controls changes. Numerical integration illustrates, in discrete steps, how the loop’s output becomes its new input. The derivative, as a component of the loop, represents a potential that does not exist. The causal structure, when it exercises its power, brings the potential into actual existence. The closed-loop solution. Motion is logically possible because (i) reality can form closed-loops, where (ii) a component A represents the non-existence of the state of another component B, and (iii) the causal structure of the loop is such that the state of B becomes that which A represents. In consequence, although the arrow-in-flight is where it is and is not where it is not (as Zeno pointed out) it is also potentially not where it is (as Aristotle suggested). This is a Hegelian solution to Zeno’s paradox in disguise, albeit refined by the mathematics of the calculus. For example, we can restate the closed-loop solution using Hegel’s concepts. The entire system constitutes a “dialectical unity of opposites”. The derivative is a “moment” within this unity, symbolizing non-being, or “nothingness.” The control variable is another “moment”, symbolizing being. Being and non-being are in real contradiction (they are “opposed determinations”). They each “vanish” into the other: non-being becomes being, and being becomes non-being. The system is therefore “self negating”, not in a strictly logical sense, but in the sense of having the power to change itself. The negation is itself negated, repeatedly. And so the real contradiction resolves itself as a process of change over time, or “becoming”. According to Plato, existence is nothing but the power to produce or undergo change. And Hegel argues, in the Science of Logic, that everything that exists must be a contradictory unity of being and nothing – which is becoming. Hegel and Marx devoted considerable attention to the philosophical implications of the calculus because they saw deep parallels to the dialectic. If we agree with Engels that “Dialectics is nothing more than the science of the general laws of motion and development of nature, human society and thought” then the calculus, properly interpreted, is a mathematical formalization of the dialectical theory of change. For every differential equation is a real contradiction. And Hegelian dialectics is no “dead dog” but has, all along, been hiding in plain sight. The snake devours its own tail. “Suppose a contradiction is pointed out in any sort of object or concept (and there is simply nothing anywhere in which a contradiction, i.e. opposite determinations, could not and would not have to be pointed out …) When such a contradiction is recognized, the conclusion is usually drawn that ‘Therefore, the object is nothing’, just as Zeno first demonstrated with regard to movement, namely that it contradicts itself and that therefore it does not exist … This kind of dialectic thus merely stops at the negative side of the result and abstracts from what is at the same time actually on hand, namely a determinate result, here a pure nothing, but a nothing that contains being and likewise a being that contains nothing within itself. Thus, existence is (1) the unity of being and nothing in which the immediacy of these determinations has disappeared and with it the contradiction in their relationship, — a unity in which they are now only moments. (2) Since the result is the sublated contradiction, it is in the form of a simple unity with itself or itself as being, but a being with negation or determinateness.” Hegel, “Encyclopedia of the Philosophical Sciences in Basic Outline, Part 1: Science of Logic”, §89, p. 145. Cambridge University Press. Wright (2023) Motion as contradiction: Zeno, Hegel and the calculus "],["pandemics.html", "34 Pandemics 34.1 Origins of SARS2 34.2 How the West lost Covid 34.3 Pantent Protection Waiver", " 34 Pandemics 34.1 Origins of SARS2 Wade Virologists like Daszak had much at stake in the assigning of blame for the pandemic. For 20 years, mostly beneath the public’s attention, they had been playing a dangerous game. In their laboratories they routinely created viruses more dangerous than those that exist in nature. They argued that they could do so safely, and that by getting ahead of nature they could predict and prevent natural “spillovers,” the cross-over of viruses from an animal host to people. If SARS2 had indeed escaped from such a laboratory experiment, a savage blowback could be expected, and the storm of public indignation would affect virologists everywhere, not just in China. “It would shatter the scientific edifice top to bottom,” Some older methods of cutting and pasting viral genomes retain tell-tale signs of manipulation. But newer methods, called “no-see-um” or “seamless” approaches, leave no defining marks. Nor do other methods for manipulating viruses such as serial passage, the repeated transfer of viruses from one culture of cells to another. If a virus has been manipulated, whether with a seamless method or by serial passage, there is no way of knowing that this is the case. Both the SARS1 and MERS viruses had left copious traces in the environment. The intermediary host species of SARS1 was identified within four months of the epidemic’s outbreak, and the host of MERS within nine months. Yet some 15 months after the SARS2 pandemic began, and after a presumably intensive search, Chinese researchers had failed to find either the original bat population, or the intermediate species to which SARS2 might have jumped, or any serological evidence that any Chinese population, including that of Wuhan, had ever been exposed to the virus prior to December 2019. Natural emergence remained a conjecture which, however plausible to begin with, had gained not a shred of supporting evidence in over a year. As long as that remains the case, it’s logical to pay serious attention to the alternative conjecture, that SARS2 escaped from a lab. Why would anyone want to create a novel virus capable of causing a pandemic? Ever since virologists gained the tools for manipulating a virus’s genes, they have argued they could get ahead of a potential pandemic by exploring how close a given animal virus might be to making the jump to humans. And that justified lab experiments in enhancing the ability of dangerous animal viruses to infect people, virologists asserted. With this rationale, they have recreated the 1918 flu virus, shown how the almost extinct polio virus can be synthesized from its published DNA sequence, and introduced a smallpox gene into a related virus. These enhancements of viral capabilities are known blandly as gain-of-function experiments. With coronaviruses, there was particular interest in the spike proteins, which jut out all around the spherical surface of the virus and pretty much determine which species of animal it will target. In 2000 Dutch researchers, for instance, earned the gratitude of rodents everywhere by genetically engineering the spike protein of a mouse coronavirus so that it would attack only cats. Virologists started studying bat coronaviruses in earnest after these turned out to be the source of both the SARS1 and MERS epidemics. In particular, researchers wanted to understand what changes needed to occur in a bat virus’s spike proteins before it could infect people. Researchers at the Wuhan Institute of Virology, led by China’s leading expert on bat viruses, Shi Zheng-li or “Bat Lady,” mounted frequent expeditions to the bat-infested caves of Yunnan in southern China and collected around a hundred different bat coronaviruses. Shi then teamed up with Ralph S. Baric, an eminent coronavirus researcher at the University of North Carolina. Their work focused on enhancing the ability of bat viruses to attack humans so as to “examine the emergence potential (that is, the potential to infect humans) of circulating bat CoVs [coronaviruses].” In pursuit of this aim, in November 2015 they created a novel virus by taking the backbone of the SARS1 virus and replacing its spike protein with one from a bat virus (known as SHC014-CoV). This manufactured virus was able to infect the cells of the human airway, at least when tested against a lab culture of such cells. The SHC014-CoV/SARS1 virus is known as a chimera because its genome contains genetic material from two strains of virus. If the SARS2 virus were to have been cooked up in Shi’s lab, then its direct prototype would have been the SHC014-CoV/SARS1 chimera, the potential danger of which concerned many observers and prompted intense discussion. “If the virus escaped, nobody could predict the trajectory,” said Simon Wain-Hobson, a virologist at the Pasteur Institute in Paris. Baric and Shi referred to the obvious risks in their paper but argued they should be weighed against the benefit of foreshadowing future spillovers. Scientific review panels, they wrote, “may deem similar studies building chimeric viruses based on circulating strains too risky to pursue.” Given various restrictions being placed on gain-of function (GOF) research, matters had arrived in their view at “a crossroads of GOF research concerns; the potential to prepare for and mitigate future outbreaks must be weighed against the risk of creating more dangerous pathogens. In developing policies moving forward, it is important to consider the value of the data generated by these studies and whether these types of chimeric virus studies warrant further investigation versus the inherent risks involved.” That statement was made in 2015. From the hindsight of 2021, one can say that the value of gain-of-function studies in preventing the SARS2 epidemic was zero. The risk was catastrophic, if indeed the SARS2 virus was generated in a gain-of-function experiment. Shi set out to create novel coronaviruses with the highest possible infectivity for human cells. Her plan was to take genes that coded for spike proteins possessing a variety of measured affinities for human cells, ranging from high to low. She would insert these spike genes one by one into the backbone of a number of viral genomes (“reverse genetics” and “infectious clone technology”), creating a series of chimeric viruses. These chimeric viruses would then be tested for their ability to attack human cell cultures (“in vitro”) and humanized mice (“in vivo”). And this information would help predict the likelihood of “spillover,” the jump of a coronavirus from bats to people. It cannot yet be stated that Shi did or did not generate SARS2 in her lab because her records have been sealed, but it seems she was certainly on the right track to have done so. On December 9, 2019, before the outbreak of the pandemic became generally known, Daszak gave an interview in which he talked in glowing terms of how researchers at the Wuhan Institute of Virology had been reprogramming the spike protein and generating chimeric coronaviruses capable of infecting humanized mice. “And we have now found, you know, after 6 or 7 years of doing this, over 100 new SARS-related coronaviruses, very close to SARS,” Daszak says around minute 28 of the interview. “Some of them get into human cells in the lab, some of them can cause SARS disease in humanized mice models and are untreatable with therapeutic monoclonals and you can’t vaccinate against them with a vaccine. So, these are a clear and present danger…. Once you have generated a novel coronavirus that can attack human cells, you can take the spike protein and make it the basis for a vaccine. One reason for SARS1 being so hard to handle is that there were no vaccines available to protect laboratory workers. The long history of viruses escaping from even the best run laboratories. The smallpox virus escaped three times from labs in England in the 1960’s and 1970’s, causing 80 cases and 3 deaths. Dangerous viruses have leaked out of labs almost every year since. Coming to more recent times, the SARS1 virus has proved a true escape artist, leaking from laboratories in Singapore, Taiwan, and no less than four times from the Chinese National Institute of Virology in Beijing. The two closest known relatives of the SARS2 virus were collected from bats living in caves in Yunnan, a province of southern China. If the SARS2 virus had first infected people living around the Yunnan caves, that would strongly support the idea that the virus had spilled over to people naturally. But this isn’t what happened. The pandemic broke out 1,500 kilometers away, in Wuhan. The bats’ range is 50 kilometers, so it’s unlikely that any made it to Wuhan. For the lab escape scenario, a Wuhan origin for the virus is a no-brainer. Wuhan is home to China’s leading center of coronavirus research where, as noted above, researchers were genetically engineering bat coronaviruses to attack human cells. They were doing so under the minimal safety conditions of a BSL2 lab. If a virus with the unexpected infectiousness of SARS2 had been generated there, its escape would be no surprise. Researchers have documented the successive changes in its spike protein as the virus evolved step by step into a dangerous pathogen. After it had gotten from bats into civets, there were six further changes in its spike protein before it became a mild pathogen in people. After a further 14 changes, the virus was much better adapted to humans, and with a further four, the epidemic took off. But when you look for the fingerprints of a similar transition in SARS2, a strange surprise awaits. The virus has changed hardly at all, at least until recently. From its very first appearance, it was well adapted to human cells. For those who think SARS2 may have escaped from a lab, explaining the furin cleavage site is no problem at all. “Since 1992 the virology community has known that the one sure way to make a virus deadlier is to give it a furin cleavage site at the S1/S2 junction in the laboratory,” writes Steven Quay, a biotech entrepreneur interested in the origins of SARS2. “At least 11 gain-of-function experiments, adding a furin site to make a virus more infective, are published in the open literature, including [by] Dr. Zhengli Shi, head of coronavirus research at the Wuhan Institute of Virology.” Different organisms have different codon preferences. Human cells like to designate arginine with the codons CGT, CGC or CGG. But CGG is coronavirus’s least popular codon for arginine. Keep that in mind when looking at how the amino acids in the furin cleavage site are encoded in the SARS2 genome. Now the functional reason why SARS2 has a furin cleavage site, and its cousin viruses don’t, can be seen by lining up (in a computer) the string of nearly 30,000 nucleotides in its genome with those of its cousin coronaviruses, of which the closest so far known is one called RaTG13. Compared with RaTG13, SARS2 has a 12-nucleotide insert right at the S1/S2 junction. The insert is the sequence T-CCT-CGG-CGG-GC. The CCT codes for proline, the two CGG’s for two arginines, and the GC is the beginning of a GCA codon that codes for alanine. There are several curious features about this insert but the oddest is that of the two side-by-side CGG codons. Only 5 percent of SARS2’s arginine codons are CGG, and the double codon CGG-CGG has not been found in any other beta-coronavirus. So how did SARS2 acquire a pair of arginine codons that are favored by human cells but not by coronaviruses? Proponents of natural emergence have an up-hill task to explain all the features of SARS2’s furin cleavage site. They have to postulate a recombination event at a site on the virus’s genome where recombinations are rare, and the insertion of a 12-nucleotide sequence with a double arginine codon unknown in the beta-coronavirus repertoire, at the only site in the genome that would significantly expand the virus’s infectivity. For the lab escape scenario, the double CGG codon is no surprise. The human-preferred codon is routinely used in labs. So anyone who wanted to insert a furin cleavage site into the virus’s genome would synthesize the PRRA-making sequence in the lab and would be likely to use CGG codons to do so. A third scenario of origin. There’s a variation on the natural emergence scenario that’s worth considering. This is the idea that SARS2 jumped directly from bats to humans, without going through an intermediate host as SARS1 and MERS did. A leading advocate is the virologist David Robertson who notes that SARS2 can attack several other species besides humans. He believes the virus evolved a generalist capability while still in bats. One problem with this idea, though, is that if SARS2 jumped from bats to people in a single leap and hasn’t changed much since, it should still be good at infecting bats. And it seems it isn’t. The bat coronaviruses of the Yunnan caves can infect people directly. In April 2012 six miners clearing bat guano from the Mojiang mine contracted severe pneumonia with COVID-19-like symptoms and three eventually died. A virus isolated from the Mojiang mine, called RaTG13, is still the closest known relative of SARS2 Researchers could have gotten infected during their collecting trips, or while working with the new viruses at the Wuhan Institute of Virology. The virus that escaped from the lab would have been a natural virus, not one cooked up by gain of function. The direct-from-bats thesis is a chimera between the natural emergence and lab escape scenarios. It’s a possibility that can’t be dismissed. But against it are the facts that 1) both SARS2 and RaTG13 seem to have only feeble affinity for bat cells, so one can’t be fully confident that either ever saw the inside of a bat; and 2) the theory is no better than the natural emergence scenario at explaining how SARS2 gained its furin cleavage site, or why the furin cleavage site is determined by human-preferred arginine codons instead of by the bat-preferred codons. Proponents of lab escape can explain all the available facts about SARS2 considerably more easily than can those who favor natural emergence. Wade (2021) The origin of COVID: Did people or nature open Pandora’s box at Wuhan? 34.2 How the West lost Covid Aside from the three Nordic outliers of Finland, Norway, and Iceland, no European state has managed the coronavirus well by global standards — or by their own much higher ones. For decades, the richest nations of the world had told themselves a story in which wealth and medical superiority offered, if not total immunity from disease, then certainly a guarantee against pandemics, regarded as a premodern residue of the underdeveloped world. That arrogance has made the coronavirus not just a staggering but an ironic plague. Invulnerability was a myth, of course, but what the pandemic revealed was much worse than just average levels of susceptibility and weakness. It was these countries that suffered most, died most, flailed most. Gave up most easily, too, acquiescing to so much more disease that they might have been fighting a different virus entirely. For nearly the entire year, the COVID epicenter was not in China, where the pathogen originated, or in corners of South Asia or sub-Saharan Africa, where limited state capacity and medical infrastructure seemed, at the outset, especially concerning, but either in Europe or the United States — places that were rated just one year ago the best prepared in the world to combat infectious disease. One distinct pattern stands out, with national outcomes falling into three obvious clusters. In Europe, North America, and South America: nearly universal failure. In sub-Saharan Africa and South Asia: high caseloads and low death rates, owing largely to the age structure of populations. In East Asia, South-East Asia and Oceania: inarguable success. By damage, the coronavirus has not been a “Chinese flu” but a western malady. The virus originated in China, but the true focus of the epidemic that spread to the world was actually in northern Italy. There’s a huge gap between the reality of globalization and our ability to actually apprehend what that means. “One of the common features is that we are a medical-centric group of countries,” says Michael Mina, a Harvard epidemiologist who has spent the pandemic advocating for mass rollout of rapid testing on the pregnancy-kit model — only to meet resistance at every turn by those who insisted on a higher, clinical standard for tests. “We have an enormous focus on medicine and individual biology and individual health. We have very little focus as a group of nations on prioritizing the public good. We just don’t. It’s almost taboo — I mean, it is taboo. We have physicians running the show — that’s a consistent thing, medical doctors across the western European countries, driving the decision-making.” The result, he says, has been short-sighted calculations that prioritize absolute knowledge about everything before advising or designing policy about anything. Devi Sridhar 34.3 Pantent Protection Waiver The Biden administration announced Wednesday that it supports waiving intellectual property protections for Covid-19 vaccines, as countries struggle to manufacture the life-saving doses. “This is a global health crisis, and the extraordinary circumstances of the COVID-19 pandemic call for extraordinary measures. The Administration believes strongly in intellectual property protections, but in service of ending this pandemic, supports the waiver of those protections for COVID-19 vaccines,” United States Trade Representative Katherine Tai wrote in a statement. Stocks of major pharmaceutical companies that have produced vaccines, including Moderna, BioNTech and Pfizer, dropped sharply after news of the potential waivers first broke. CNBC "],["patents.html", "35 Patents", " 35 Patents Tooze To measure the indirect influence of science, the yellow line shows the scientific citations of the vaccine’s “parent” patents—other patents referenced in the five original vaccine patents. These peak in the early 2000s, tracking discoveries in editing genetic codes. Earlier advances in reading genetic codes drove a similar wave of citations from “grandparent” patents in the early 1990s. These waves of scientific influence illustrate how policies that help incentivize advances in basic science today influence the building blocks of future technologies and yield long-lasting economic payoffs. Developing mRNA vaccines relied on a broad base of scientific knowledge. On average, the Moderna vaccine patents are in the same technological category as only 55 percent of their parent patents—a number that falls further as citation chains lengthen. Tooze (2022) Chartbook 77 "],["populism.html", "36 Populism", " 36 Populism Kundnani It obscures the heterogeneity among all the figures, movements and parties (and, in the case of Brexit, a decision!) that are categorised as “populist” and implies they are all the same. In particular, it implies that the far left and the far right are the same. It is not a neutral, analytical term – rather, it is used by the centre right and “radical” centrists to imply that there is a clear line between the centre right and the far right (and thus obscures the convergence between the two in Europe during the last decade) It misses the point – the problem with the far right is not its style or “thin” ideology (i.e. talking about a pure people vs a corrupt elite etc.), or at least that is not the main problem with it. Rather, the problem is its “thick” ideology, i.e. its actual ideas. The obsession with the idea of “populism” makes it seem as if, if you drop this style or “thin” ideology, far-right ideas are no longer a problem – which, again, is in the interest of the centre right, so it can cooperate with the far right or just copy these ideas. Kundnani (2023) Twitter Thread "],["quantum-mechanics.html", "37 Quantum Mechanics 37.1 Entropy 37.2 Many-World Interpretation", " 37 Quantum Mechanics 37.1 Entropy Ludwig Boltzmann’s grave in Vienna bears his famous entropy formula, S = k log W, which relates the entropy of a system to the number of possible microscopic states. Ludwig Boltzmann was an Austrian physicist and philosopher who made significant contributions to the fields of statistical mechanics and thermodynamics including the formulation of the H-theorem- which explains the behavior of gases in terms of statistical mechanics, the concept of the Boltzmann constant (k)- a fundamental constant in physics that relates temperature to the average kinetic energy of particles in a gas, the Boltzmann equation- which describes the behavior of a gas by incorporating statistical probabilities. The concept of a “Boltzmann Brain” was introduced by cosmologists, inspired by Boltzmann’s work. It suggests that there is a minuscule possibility of a conscious entity, like a human brain, spontaneously fluctuating into existence in an otherwise empty and chaotic universe. This idea sparked debates and discussions about the nature of reality and the interpretation of probabilities in cosmology. An under appreciated fact of history is that quantum physics implicitly started with Boltzmann. Entropy, energy per degree kelvin, is essential quantized because the number of possible micro states is an integer. Planck, in his 1901 paper, “On the distribution of energy in the normal spectrum” — in what Planck called “an act of desperation” — derived his quantized equation for black body radiation directly from Boltzmann’s equation for entropy. It is arguable that the father of quantum mechanics is truly Boltzmann, not Planck. Physics in History (Twitter Thread) 37.2 Many-World Interpretation Ball The idea that the universe splits into multiple realities with every measurement has become an increasingly popular proposed solution to the mysteries of quantum mechanics. But this “many-worlds interpretation” is incoherent, Philip Ball argues in this adapted excerpt from his new book Beyond Weird. Phillip Ball (2018) Why the Many-Worlds Interpretation Has Many Problems "],["vaccines.html", "38 Vaccines 38.1 Origin of diseases 38.2 mRNA", " 38 Vaccines 38.1 Origin of diseases Pendergrass &amp; Vettese Abstract Edward Jenner took the long view. His 1798 treatise on vaccination, which reported a revolutionary new method of preventing smallpox, opened with a medical philosophy of history rather than a description of symptoms or a review of existing treatments. “The deviation of Man from the state in which he was originally placed by Nature seems to have proved to him a prolific source of Diseases,” he explained. By this he meant that infectious disease ultimately resulted from human and animal intermingling since the agricultural revolution, an insight anthropologists and epidemiologists have since confirmed. The majority of human pathogens are ultimately zoonoses, originating not at the dawn of the human species but in the relatively recent past. Measles likely evolved from the bovine disease rinderpest seven thousand years ago. Influenza may have started about forty-five hundred years ago with the domestication of waterfowl. Jenner’s own specialty, smallpox, probably originated four thousand years ago in eastern Africa when a gerbil virus jumped to the newly domesticated camel and then to humans. The New World’s Indigenous nations cultivated countless crops but practiced little animal husbandry, allowing them to live relatively free of disease before 1492. European conquest succeeded in a large part thanks to the invaders’ pathogenic armory of measles, typhus, tuberculosis, and smallpox, which decimated Indigenous populations by 90 percent over the succeeding centuries. Pendergrass and Vettese (2021) 38.2 mRNA DW Claim: mRNA vaccines manipulate human DNA DW: False It is easy to get DNA and RNA confused, two similar abbreviations that relate to genetic material. But they’re very different. DNA contains the genetic blueprint that determines our bodies’ various traits. Viruses such as SARS-CoV-2 have RNA that stores their genetic material. But RNA is also found in the human body, and plays a role in protein synthesis. Viruses tap into this mechanism to reproduce in human cells. The human body, however, recognizes these intruders by their protein spike, producing antibodies and t-cells to fight off the virus. RNA vaccinations inject only one element of the SARS-CoV-2 virus into the human body, namely mRNA, containing the blueprint to produce its spike protein. The human immune system then kicks into action, forming antibodies against the pathogen. No human or virus RNA, however, ever enters the cell nucleus. This means it does not get in contact with our genetic material. After serving its purpose, human cells then break down the RNA. A scientific study published in December 2020 claims the genetic material from the SARS-CoV-2 virus could manipulate human DNA through the reverse transcriptase, an enzyme that transcribes RNA into DNA, which can enter the cell nucleus. The study in question has not yet been peer-reviewed and is hotly debated. Virologist David Baltimore from the California Institute of Technology won the Nobel Prize for his role in discovering reverse transcriptase. Science magazine quoted him describing the new work and findings as “impressive” and “unexpected.” However, he noted that the work showed only that fragments of the COVID-19 virus genome integrate that couldn’t produce infectious particles and represented a biological “dead end.” “It is also not clear if, in people, the cells that harbor the reverse transcripts stay around for a long time or they die,” Baltimore said. The work raises a lot of interesting questions.” Waldemar Kolanus, who headsLife &amp; Medical Sciences Institute (LIMES) at Bonn University, doubts the findings are relevant for the actual vaccine. Speaking to DW, he said the structure of mRNA has been deliberately altered for vaccines so as to prevent cells instantly breaking them down.”Most likely, it cannot be reverse transcribed.” As such, mRNA vaccines are much safer with regard to such processes than actual virus genomes, he says. DW Factcheck Zhang Prolonged SARS-CoV-2 RNA shedding and recurrence of PCR-positive tests have been widely reported in patients after recovery, yet these patients most commonly are non-infectious. Here we investigated the possibility that SARS-CoV-2 RNAs can be reverse-transcribed and integrated into the human genome and that transcription of the integrated sequences might account for PCR-positive tests. In support of this hypothesis, we found chimeric transcripts consisting of viral fused to cellular sequences in published data sets of SARS-CoV-2 infected cultured cells and primary cells of patients, consistent with the transcription of viral sequences integrated into the genome. To experimentally corroborate the possibility of viral retro-integration, we describe evidence that SARS-CoV-2 RNAs can be reverse transcribed in human cells by reverse transcriptase (RT) from LINE-1 elements or by HIV-1 RT, and that these DNA sequences can be integrated into the cell genome and subsequently be transcribed. Human endogenous LINE-1 expression was induced upon SARS-CoV-2 infection or by cytokine exposure in cultured cells, suggesting a molecular mechanism for SARS-CoV-2 retro-integration in patients. This novel feature of SARS-CoV-2 infection may explain why patients can continue to produce viral RNA after recovery and suggests a new aspect of RNA virus replication. From Comment Section Mellissa Booth Chimeric sequence reads that the researchers found in the sequence databases are likely artifacts from the preparation of the RNA libraries. These chimera artifacts are a common phenomenon in total RNA sequence library construction from complex samples whether the source material is from humans, soil, sewage, ocean water, etc., and these artifacts end up in sequence databases. The other lab bench experiments DO NOT show that SARS-CoV-2 RNA is reverse transcribed to DNA, transported into the nucleus and then integrated into the host genome under NORMAL conditions. However, there are assays that could answer the question about viral integration. The authors could collect samples from infected patients that are still shedding virus but are non-infectious (as they mention in their summary) and perform Southern Blot analysis to determine if viral sequences have indeed been integrated into the host genome. And ultimately, IF the researchers find integration under normal conditions, the next questions follow: Are infected cells persistent? Do these integrated elements propagate in host cells? These are the questions that get to their hypothesis about purported viral integration elements being responsible for persistent detection of virus in non-infectious, post-COVID patients. Mar Your argument only addresses their initial screening of public datasets. They do show integration in vitro when they extract cell genomic DNA and do PCR. Southern blot is not necessary. Zhang (2020) NIH Pubmed Zhang (2020) SARS-CoV-2 RNA reverse-transcribed and integrated into the human genome (Preprint Not Peered) (pdf) Viral Shedding When an individual gets infected by a respiratory virus like SARS-CoV-2, the virus particles will bind to the various types of viral receptors, particularly the angiotensin-converting enzyme 2 (ACE2) receptors in the case of SARS-CoV-2, that line the respiratory tract. Throughout this ongoing process, infected individuals, who may not yet be experiencing any of the viral symptoms, are shedding viral particles while they talk, exhale, eat, and perform other normal daily activities. Under normal circumstances, viral shedding will not persist for more than a few weeks; however, as researchers gain a more in-depth understanding of the viral clearance of SARS-CoV-2, they have found that certain populations will shed this virus for much longer durations. In fact, a growing amount of evidence indicates that the viral shedding of SARS-CoV-2 begins before a patient is symptomatic, peaks at the point of or shortly after symptom onset and can continue to be released even after the individual’s symptoms have been resolved. The duration of viral shedding can be used to categorize the infectivity of a person; therefore, this information is crucial in implementing effective infection prevention strategies, such as appropriate quarantine durations and mask requirements. Currently, SARS-CoV-2 infection is confirmed with a positive polymerase chain reaction (PCR) test that can be conducted regardless of whether an individual is experiencing symptoms. Through such PCR tests, viral shedding of SARS-CoV-2 has been found to have a median duration of 12 to 20 days, with a persistence that can reach up to 63 days after initial symptom onset. The viral shedding of SARS-CoV-2 from the gastrointestinal (GI) tract does not appear to have any correlation with disease severity. There remains uncertain information on the proportion of SARS-CoV-2 cases that are asymptomatic. It is unclear as to whether these “asymptomatic” cases are truly asymptomatic in the sense that these infected individuals will never experience any of the viral symptoms, or are rather presymptomatic, meaning that these individuals had no symptoms at the time of their positive PCR test but eventually developed symptoms later. During both the SARS outbreak of 2002 and 2003, as well as during the current pandemic, researchers hypothesized that live viral particles present within fecal matter moving through sewage pipes could infect individuals through aerosols or droplets. SARS-CoV-2 RNA has been detected commonly in patient feces. However, as of yet more evidence is needed to assess the quantities of virus in fecal matter and its replication abilities in order to determine whether fecal-oral viral transmission is possible. Cuffari. What is Viral Shedding Viral Integration and consequences on Host Gene Expression Upon cell infection, some viruses integrate their genome into the host chromosome, either as part of their life cycle (such as retroviruses), or incidentally. While possibly promoting long-term persistence of the virus into the cell, viral genome integration may also lead to drastic consequences for the host cell, including gene disruption, insertional mutagenesis and cell death, as well as contributing to species evolution. This review summarizes the current knowledge on viruses integrating their genome into the host genome and the consequences for the host cell. Viral genome integration into the host genome is a hallmark of retroviruses, as it is a mandatory step in the retroviral life cycle and a prerequisite for productive infection. Upon integration, the retrovirus will persist in the infected cell for its entire lifespan, and will affect host gene expression depending on the integration site. Furthermore, if retroviral infection and integration occurs in the germline, the provirus will be transmitted to the progeny, and will thus contribute shaping the genome of future generations. This is the case of the so called “endogenized” retroviruses or endo­genous retroviruses (ERV). The site of the viral integration event can have multiple consequences for the host, as well as for the virus itself. Indeed, viral integration can lead to cell death or proliferation as a result of insertional mutagenesis. However, integration can also lead to consequences for the virus, i.e. active production or transcriptional silencing, a process also called latency that is key to establish viral persistence. Finally, integration in the germline can contribute shaping the host genome and participate in species evolution. Desfarges (2012) Retrovirus Retroviruses have undergone quite an explosion in our knowledge in about the last 40 years. The term “retrovirus” means it behaves backwards from the original way that we all think about genetics, which is that DNA makes RNA, and RNA makes protein. So retroviruses have an RNA genome, and when they get into cells that RNA is reverse-transcribed into DNA, so it goes backwards. The DNA is then inserted into the genome of the cell, so when the cell divides, it copies this, and it begins to express RNA. Some of that RNA is translated into proteins, which are needed to package the retrovirus. And another of those RNAs is the RNA genome that goes into those packaging materials and is excreted from the cell and goes on to infect other cells. So there are many different kinds of retroviruses. Now, the most famous one right at the moment is the human immunodeficiency virus which causes acquired immunodeficiency syndrome, or AIDS. But there are many different kinds of retroviruses that are associated with diseases, including cancer, leukemia, and AIDS, obviously. Finally, retroviruses have been tamed for use in gene therapy, so it is possible to take out all of the genes that allow the retrovirus to replicate itself and replace that with a gene that the particular cell that you’re interested in is missing. And so using the integrating ability of a retrovirus, you can actually take something that could ordinarily harm people and turn it into something that can be used as a therapeutic vehicle to make them better. Bodine "],["academia.html", "39 Academia 39.1 Academic Snobbery 39.2 Academic Publishing", " 39 Academia 39.1 Academic Snobbery On David Graeber He was one of the most important anthropologists of our time. It is a bitter paradox that the best anthropologist theorist of his generation never felt quite at home in the established anthropology circles. He hated academic conferences with a passion. It wasn’t just because of Yale’s shameful decision to get rid of him because of his political activism; David was a working-class person who detested, with every fiber of his being, any hint of academic elitism, networking, and schmoozing. Much to his personal cost, he rejected these strange sectarian rituals of academic life. He was the most generous friend and colleague one could hope to have, and the most formidable opponent of academic snobbery. After he was fired from Yale, David applied to more than twenty academic jobs in the US. He hasn’t been shortlisted for a single one. Grubacic on Graeber 39.2 Academic Publishing It should take however long as it takes to get it right. this is one of the reasons that the model of academia should be rethought from the ground up. too many mediocre books. it should be okay to just not say anything until you have something to say, which can takes decades (Colin Drumm (twitter)) "],["professionalism.html", "40 Professionalism", " 40 Professionalism One of the bizarre contradictions of contemporary capitalism is the label “essential worker.” As most forms of commerce, business, production, and exchange shut down to minimize the spread of the virulent coronavirus, certain sectors of society have been designated as essential to the skeleton crew keeping a quarantined society afloat. Incidentally, many of these “essential workers” turn out to be some of the worst paid and most precariously employed. However, rather than, say, bump up their pay and benefits as befitting of their essential status within society, capitalists have turned to calling them heroes. One clue may be a paper written by sociologist Harold Wilensky titled “The Professionalization of Everyone?” published in 1964. Wilensky considered the idea that eventually, every job would become a profession and every worker a professional. In the end, he concluded that the idea of the professionalization of everyone “is a bit of sociological romance”. However, he did mention, “It may also be true that the empirical, critical, rational spirit of science finds its way into an increasing number of occupations” that leads to “a happy integration of professional and civil culture”. The world Wilensky lived and worked within was on the cusp of a fundamental, traumatic transformation, the consequences of which we now grapple with today. To understand the origins of professionalism, we need to understand capitalism. Capitalists argues that whoever owns the means of production owns whatever the means of production produces even if they are not the ones who produces it. Our entire society — laws, culture, infrastructure — are designed around this simple fact. And it is here, in the empty spaces within the framework of this basic economic relationship where we will introduce the early professional Taclott Parsons and Harold Wilensky saw professionals as an outgrowth or natural conclusion from the development of rationalism, capitalism, and science. Magali Larson sees professionalism as an economic strategy where workers in a field get together and decide to create a mini-monopoly by restricting who gets to work in it and building prestige around their knowledge and skills through scarcity. Hannes Siegrist saw professionalism as a new service class whose clients were primarily bourgeois and so the professionals imitated the bourgeois to gain their sympathy and patronage. Others saw professionals as evolving from medieval guilds, using scarcity to justify insulation from the constant shocks of the free market. But one thing was certain, professionals didn’t exactly fit into the neat picture of capitalism as the owners of production (the bourgeois) or the laborers whose work is exploited for profit (the proletariat). There are some agreed upon traits found among most of the original professions (generally considered medicine and law, sometimes military, teaching, clergy, and engineering). They include: an esoteric body of knowledge that professionals claimed mastery over some form of occupational enclosure (restricted access to being a member of the profession)3. some formalized or institutionalized process that regulated membership within the profession (usually through credentials and education) a camaraderie or collegiality among peers recognized as fellow professionals autonomy as a worker (generally self-employed) and autonomy as a field to self-regulate itself A service-oriented ethic, usually one that asserts the profession serves a higher societal good above mere commerce which often leads to the profession developing a sense of purpose beyond profit Already, there’s some characteristics about professionals that seem anti-capitalist. For one, there’s the idea of occupational enclosure; the concept of a free and unregulated market is anathema to the idea of a monopoly or oligopoly, and professions were essentially that. Most professions allied with the newly formed and growing nation-states of the 1700s and 1800s, relying on the state to regulate their markets through legal systems that made it illegal to practice without some kind of license or degree. The collective identity of professionals belonging to a single profession (and often creating powerful organizations and interest groups to protect and promote their agendas) also flies in the face of capitalist ideas of self-interest, enlightened greed, and individualism. There is an idea that professionals work for a higher calling than profit. ** Unions vs Professions**: As capitalism and labor markets grew more complex and workers became more aware of their exploited status, there were two options a workforce could essentially take in the 1800s-1900s: they could either unionize or professionalize. Unions and professions basically had the same goals — create regulations that would bring more profit to their work — but allied with different agendas. Unions veered towards a proletariat identity while professionals veered towards a bourgeois identity. You can see this in how they celebrate themselves and how they’re perceived in society. Unions emphasize the trades and craftsmanship, solidarity amongst the working class regardless of occupation and organizing against the owners of the means of production. Professions emphasize their skill but through mastery of knowledge and education, their status as individual experts rather than a collective group, and internalizing bourgeois values of apolitical neutrality, a disdain for conflict, and Victorian ideas of discretion and comportment. And then neoliberalism happened. During the 1970s, a series of economic crises such as stagflation and recessions and oil price spikes disrupted the capitalist dream. With the ever- looming threat of communism, capitalists began to push for a more aggressive, even more distilled version of capitalism. If the post-Great Depression era was leaders like Franklin Delano Roosevelt saving capitalism from itself by making it softer and kinder (to an extent), then the new neoliberal capitalists saw the answer in pushing capitalism to its limits. Because of their belief in the invisible hand and virtuous self- interest, they sought to make everything in society governed by the free market. This often meant drastic deregulation and the elimination of any kind of government-run program or centralized structures and institutions. It meant subjecting everyone to the full whims of the market, with all of its booms and busts, believing that the it would sort out the winners and the losers and that both of them would be deserving of their respective fates. Perhaps one of the most pervasive (and perverse) effects of neoliberalism, however, is how it changed the very concept of the self. Because neoliberals worshipped capital, they also believed people should act like capital itself — fluid, easilytransformed, fungible, mobile, infinitely transmutable, and self-generating in profits. We should be entrepreneurs, not workers. Neoliberalism rooted the means of production in the individual. It (falsely) redefined the means of production as labor, and everyone could provide some kind of labor. Everyone was redefined as a capitalist. It was all a slight of hand to get us to look away from the actual structural problems of capitalism and focus on the individual instead. Neoliberals did not like professions at all. Capitalists broke up unions much more aggressively and violently; to break the power-base of professionals, who were often wealthier and held more status and power, more subtle strategies were necessary. Capitalism began to purposefully exploit that labor more and more by “de- skilling” certain types of work and then using the excuse that a once valued skilled work is now unskilled. As women and people of color entered the professions, those who paid the professionals used it as an excuse to denigrate professional work and thus professional pay. This not only changed the nature of work but the nature of the worker: “people bend towards an adaptable/sacrificial /oblative position. Precarization: governing a population through creating precarity. Neoliberalism, by leaning hard into capitalism’s roots, rebranded precarity as freedom: Convince people that precarity was self- empowering. Thus, the gig economy isn’t exploiting workers by giving them unstable, hard-to-predict-or-plan-for working hours; it’s giving them the freedom to work whenever they wanted. Precarity, isn’t something to insulate yourself from; it is a virtue to embrace. Precarity means freedom and empowerment. As labor became more fluid and decentralized, it became more profitable and easier to exploit but also made it harder for employers to control. But then neoliberals figured something out. What does this new worker sound like? Someone who works autonomously and individually as opposed to in a collective or corporate group setting, someone who has to self-regulate themselves, who sees their economic value in their diverse set of skills rather than what they directly produce or own, and often wants a higher purpose in their work outside of just getting enough money to survive? Oh. They sound like professionals, suddenly, the professional, that oddball of capitalism, found the perfect slot to fit into in neoliberalism. Professionalism was a ready-made, already existing pool of ideas and beliefs and aspirations and emotions that employers could tap into to control the increasing margin of indeterminacy or flexibility in work. It turns your work into your identity, and you into your work. To put it more bluntly, you didn’t have to obsessively watch over your employees anymore if you could get your employees to do it for you by surveilling themselves. And so, we have all for the most part accepted the idea that we must all act like professionals even if we enjoy the complete opposites of the privileges professionals traditionally enjoyed. And, perhaps most chilling of all, most of us want to be a professional, desire to be professional, and judge those who are not, regardless of whether we’re treated as such. The phrase: “socially essential.” Paramedics, ambulance drivers, grocery store clerks, truck drivers, cleaning staff, cooks — all of these jobs are “socially essential” but aren’t treated or paid as such. To be professional is to be self-sacrificing, and if there’s one thing capitalists and the bourgeois middle class love to do, it’s sacrificing others for themselves. Why do we pay the most worthless of our society the most and the most valuable members of our society the least? Because we’re all professionals now. And professionals are not supposed to be in it for the money but for the love of the game. Except we didn’t make the rules for the game. The capitalists did. Lee (2021) We Are All Professionals Now: Or, How Did Grocery Workers Become Essential “Heroes”? (pdf) "],["geopolitics.html", "41 Geopolitics 41.1 Post-Westphalian Order 41.2 Rules-based order 41.3 Economic Convergence as Peace Strategy 41.4 Western Development Bankruptcy 41.5 Eur-Asian Primacy 41.6 Military Arsenals 41.7 Military Matching", " 41 Geopolitics 41.1 Post-Westphalian Order Vergerio The Westphalian order refers to the conception of global politics as a system of independent sovereign states, all of which are equal to each other under law. The most popular story about this political system traces its birth to the Peace of Westphalia in 1648, follows its strengthening in Europe and gradual expansion worldwide, and finally, near the end of the twentieth century, begins to identify signs of its imminent decline. On this view, much of the power that states once possessed has been redistributed to a variety of non-state institutions and organizations—from well-known international organizations such as the UN, the EU, and the African Union to violent non-state actors such as ISIS, Boko Haram, and the Taliban along with corporations with global economic influence such as Facebook, Google, and Amazon. This situation, the story often goes, will result in an international political order that resembles medieval Europe more than the global political system of the twentieth century. Over the last two decades, scholars working on the history of the global order have painstakingly shown the complete mismatch between the story of Westphalia and the historical evidence. The nation-state is not so old as we are often told, nor has it come to be quite so naturally. Getting this history right means telling a different story about where our international political order has come from—which in turn points the way to an alternative future. The post–Cold War period has indeed seen the growth of non-state organizations, but in more recent years a range of right-wing leaders has only buttressed the influence of the nation-state. The spectacular resurgence of nationalism—from Brexit and Donald Trump to the ascendence of Narendra Modi, Jair Bolsonaro, and Viktor Orbán—has led some to speculate that the hour of the Westphalian order may not have passed after all, while others stick to their guns and suggest that this phenomenon is a mere spasm of a dying system. Getting the history of the states-system right has critical implications for both of these positions. Generations of international relations students have absorbed the idea of the 1648 Peace of Westphalia as a pan-European charter that created the political structure that now spans the entire globe: a system of legally (if not materially) equal sovereign states. Along with this political structure, this story goes, came other important features, from the doctrine of non-intervention, respect of territorial integrity, and religious tolerance to the enshrinement of the concept of the balance of power and the rise of multilateral European diplomacy. In this light, the Peace of Westphalia constitutes not just a chronological benchmark but a sort of anchor for our modern world. With Westphalia, Europe broke into political modernity and provided a model for the rest of the world. Sovereign statehood only became the default within Europe in the nineteenth century, with entities like the Holy Roman Empire gradually giving way to sovereign states like Germany. While often overlooked in this regard, Latin America also transitioned into a system of sovereign states during that period as a result of its successive anti-colonial revolutions. This system then became the default of the international order through decolonization in the 1950s through the 1970s, when independent sovereign states replaced empires worldwide. Throughout this transition various alternatives were considered, including—up until the 1950s—forms of federations and confederations that have since been largely forgotten. Over the past several decades, the state has not only triumphed as the only legitimate unit of the international system, but it has also rewired our collective imagination into the belief that this has been the normal way of doing things since 1648. As late as 1800, Europe east of the French border looked nothing like its contemporary iteration. As historian Peter H. Wilson describes in his recent book Heart of Europe (2020), the Holy Roman Empire, long snubbed by historians of the nation-state, had been in existence for a thousand years at that point; at its peak it had occupied a third of continental Europe. It would hold on for six more years, until its dissolution under the strain of Napoleonic invasions and its temporary replacement with the French-dominated Confederation of the Rhine (1806–1813) and then the German Confederation (1815–1866). The latter mirrored the Holy Roman Empire in many ways; it hardly looked like a nation-state at all. Much of its territory still overlapped—in so-called “pre-modern” fashion—with the territory of the Habsburg monarchy, another composite state that began its centralization process earlier than the Holy Roman Empire but did not look much like a nation-state either until the late nineteenth century. It solidified into the Austrian empire (1804–1867) and then the Austro-Hungarian empire (1867–1918), but the 1867 power-sharing deal granted Hungary considerable autonomy and essentially allowed it to run its own mini-empire. Meanwhile, to the south, what we think of as modern-day Italy was still a patchwork of kingdoms (Sardinia, the Two Siciles, Lombardy-Venetia under the Austrian Crown), Duchies (including Parma, Modena, and Tuscany), and Papal States, while territory further east was ruled by the Ottoman Empire. The map of Europe did not begin to look more like a collection of nation-states until the middle of the nineteenth century: Belgium and Greece appeared in 1830, while Italian and German unification were completed in 1871. We are accustomed to thinking of Europe as the first historical instance of a full-blown system of sovereign states, but Latin America actually moved toward that form of political organization at just about the same time. After three centuries of imperial domination, the region saw a complete redrawing of its political geography in the wake of the Atlantic Revolutions of the late eighteenth and early nineteenth century. Following in the footsteps of the United States (1776) and Haiti (1804), it witnessed a series of wars of independence which, by 1826 and with only a few exceptions, had essentially booted out the Spanish and Portuguese empires. Of course, Britain promptly gained control of trade in the region through an aggressive combination of diplomatic and economic measures often referred to as “informal empire,” but its interactions were now with formally sovereign states. Over the remainder of the century, the sovereign federative structures that had emerged in the aftermath of independence—Gran Colombia (1819–1831), the Federal Republic of Central America (1823–1841), and the United Provinces of the Río de la Plata (1810–1831) collapsed through bloody civil wars that lasted for decades, pitting regions against centralized governments and including multiple attempts to reconstitute these larger political conglomerates. Thus, much as with Western Europe, the region did not stabilize into a system of nation-states that looks like its contemporary iteration until the end of the nineteenth century. Empires, of course, continued to thrive despite the growing popularity of nation-states. Until World War II the world was still dominated by empires. A mere seventy years ago, what we now consider to be the self-evident way of organizing political communities was still just one of the options available to our collective imagination. Even the most powerful contemporary multinational corporations—Facebook, Google, Amazon, Apple, and the rest—are drastically more limited in their formal powers than were the famous mercantile companies who were central actors in the international order until the mid-nineteenth century. The two largest, the British and the Dutch East India Companies, founded in 1600 and 1602 respectively, amassed spectacular amounts of power over their two-hundred-year existence, becoming the primary engine of European imperial expansion. While these companies started off as merchant enterprises seeking to get in on Asia’s lucrative trading network, they gradually turned into much more ambitious endeavors and grew from their original outposts in India and Indonesia into full-on polities of their own. They were, as various scholars now argue, “company-states”—hybrid public-private actors that were legally entitled to rule over subjects, mint money, and wage wars. From this perspective, contemporary non-state actors are still relatively weak compared to states, who still monopolize far more formal power than all other actors in the international system. The so-called “Westphalian order” begins to look much more like an anomaly than the status quo. To think about how decentering the state might matter, consider one historical example. Only sixty years ago, the challenges that colonized peoples faced in their struggle for freedom were exacerbated by the fact that, since they were not states themselves, they had almost no international legal rights against the states they sought to defy. Most importantly, they were not allowed to use force against their occupiers; if they did so, both domestic and international law considered them to be criminals rather than combatants. This applied regardless of the legitimacy of their cause, or of their ability to organize themselves into a complex network of national liberation movements. While in the majority of cases, they eventually managed to achieve freedom and found a place at the global diplomatic table through the formation of their own independent states, they would have found a much less obstructed path to freedom if early on they had enjoyed a baseline of rights in their capacity as collective actors. Engaging with this history makes the current centrality of the states-system as a basis for organizing the globe look recent and in fairly good shape, not centuries-old and on the verge of collapse. The layering of sovereignty within polities like the EU, the rising power of corporations, the prominence of violent groups not considered “states”—none of these developments is fundamentally at odds with how international relations operated over the past 373 years. What is truly new, from a longue durée perspective, is the triumph of the state worldwide, and our inability to think of ways of organizing the world that do not involve either nation-states or organizations of nation-states. Much more is a stake in our talk about international order, then, than quibbles over historical periodization. Misrepresenting the history of the states-system plays into the hands of nationalist strongmen, who depict themselves as saving the world from a descent into stateless anarchy, controlled by globalist corporations who couldn’t care less about national allegiance. Today the norm is that states enjoy far more rights than any other collectivity—ranging from indigenous peoples to transnational social movements—simply because they are states. But it is not at all clear why this should be the only framework available to our collective imagination, particularly if its legitimacy rests on a history of the states-system that has long been debunked. The myth of Westphalia has ultimately inflicted serious damage to our ability to think creatively about how to tackle the pressing global challenges that transcend both borders and levels of governmental organization, ranging from neighborhoods, villages, and towns all the way up to international institutions. Vergerio (2022) Beyond the Natuion-State 41.2 Rules-based order Milanovic There are three constituent parts of the “imperial” order. First, it holds without the hegemon needing to directly appoint the leaders or control the politics of the subimperial and lower-level (vassal) states although at times economic coercion or brute force can be used. Economic dependence is assured through the false doctrine of comparative advantage which, indeed, was not believed by the United States in the period of its rise. (“Had the Americans followed the principle of comparative advantage they would be exporting furs and bison meat”). Second, the public opinion has to be fashioned in a way that the most important and tricky issues are never raised, but the host of other topics can be debated at will. The third component is a political and ideological justification of the hierarchical system “embodied” in the vague notion of the “rules-based global order”. The rules are flexibly defined to fit the dominant power’s interest at any point in time. Thus, the “rules” can be the ones of self-determination as they can be also against self-determination; they can be in favor of interference in domestic affairs or non-interference. They differ from the UN-based rules of international order which are not as “flexible”. One can very easily replace Australia with countries such as the UK, France, Germany and even Japan to see that they occupy positions similar to that of Australia. UK is, thanks to its still far-flung empire, a “senior” subimperial power, while France and Germany are given the “proconsular” rights in respectively Françafrique and Eastern Europe. One can even see the current conflict between Russia and “the collective West” as a conflict explained by American unwillingness to grant Russia similar “proconsular” rights over the former Soviet republics. And one can wonder how the current hierarchical system could accommodate China that certainly shows a strong desire to be an independent pole of influence and to dominate East Asia. Finally, the current system has trouble including the rising non-European powers (India, Brazil, Nigeria, South Africa, Indonesia) that for historical and cultural reasons do not enjoy the level of affinity with the US equal to that of the present subimperial powers. Fernandes’ book thus, while entirely dedicated to Australia, has a much greater resonance. It addresses the key issues regarding the current arrangement of the world: a unipolar “imperial” system; or a multipolar system, each hierarchically organized within; or perhaps unlikely, but most desirably, a system of approximately equal rights of big and small states as defined by the UN charter. Milanovic (2024) Powerful, but within the orbit of the empire 41.2.1 Conquest is back Smith Venezuela just voted to claim two-thirds of the territory of neighboring Guyana. Guyana has vowed to defend itself. Brazil has moved troops to the border in case Venezuela tries to invade via Brazilian territory. This is one more sign of the unraveling of Pax Americana; the U.S. is overburdened with numerous other conflicts, and doesn’t have much attention or power to spare. But it also demonstrates another crucial fact about geopolitics in the 21st century: The norm of territorial integrity is breaking down, and conquest is coming back into vogue. The end of World War 2 was supposed to put a stop to the practice of seizing bits of territory from neighboring countries. Article 2 of the United Nations Charter states: All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the Purposes of the United Nations. This echoes the language in Woodrow Wilson’s Fourteen Points, in which he suggested that the League of Nations should provide “guarantees of political independence and territorial integrity to great and small nations alike.” And it hearkens back to the idea of Westphalian sovereignty, which was established gradually after Europe’s disastrous Thirty Years’ War. Something changed in 2014. When Russia conquered Crimea by force, it violated the principle of territorial integrity that great powers had largely upheld since World War 2. Crimea was an internationally recognized part of the territory of Ukraine, a sovereign nation. Obviously Russia claimed that Crimea belonged to it by historical right — that Khruschev had made a mistake by giving Crimea to Ukraine — but when have conquerors ever failed to make such claims? Everyone who conquers their neighbors land says “This land rightfully belongs to us.” It’s conquest all the same. Russia followed its conquest of Crimea with a full-blown invasion and attempted conquest of Ukraine. China, meanwhile, has made more incremental moves — slowly taking land in Bhutan, pressing India along their disputed border, seizing reefs from the Philippines, and claiming the South China Sea as its own. Smaller would-be conquerors are getting in on the game as well; Venezuela’s claim to Guyana is the most obvious example, but Azerbaijan also just claimed 8 Armenian villages. Meanwhile, a popular nationalist leader in Romania campaigns with maps of a “Greater Romania”. Conquest has returned to our world. I believe that territorial integrity should be the U.S.’ central guiding principle in Cold War 2. Avoiding a return to a jungle-like world of empires and conquerors is a top priority, both for human freedom and for the health of the global economy. Territorial integrity is sure to be a popular rallying cry as well, which will help the U.S. win the contest for allies. And since Russia and China have shown their willingness to grab land from smaller, weaker nations, upholding territorial integrity helps the U.S. draw a bright distinction between our values and theirs. China believes Taiwan is a breakaway province, and that conquering it by force would be upholding territorial integrity rather than violating it. And in the Israel-Gaza conflict, the borders between Palestine and Israel are not well-defined. China’s disputed border with India is…well, disputed. Border disputes should be hashed out by negotiation and compromise, not by force of arms. Alternatives to territorial integrity There are two basic alternatives to the principle of territorial integrity: 1) liberal interventionism, and 2) indigenism. Both are deeply flawed concepts, but only the second one is going to present a real challenge to territorial integrity in the next decade or two. Liberal interventionism basically says that if a sovereign state is doing something very bad within its own borders — a genocide or other mass atrocity — it’s acceptable for powerful countries to invade that state in order to put a stop to the atrocity. Indigenism (sometimes called autochthonism) will be a much tougher challenge for the principle of territorial integrity. Basically, indigenism is the idea that a group of people — a specific race, religion, ethnicity, tribe, etc. — collectively have permanent rights to a piece of land. Basically, it’s a claim of “We were here first, so this is our land.” The idea that every racial or religious or ethnic group has some inalienable homeland to which they can always return confers a sense of security and permanence. The idea that a specific race or religion or ethnicity collectively owns a piece of land very easily lends itself to institutionalized racism. Indigenism thus inherently weakens the concept of individual equality and human rights. An even bigger problem with indigenism is the question of who is actually “indigenous”. National mysticism — the belief that different groups of humans were created separately on different plots of land — is just flat-out false. All land is stolen land — in fact, it has been stolen many, many times. In the end, the victory will simply go to the side with the army powerful enough to back up their claims. Again, we emerge back into the law of the jungle. Territorial rights can be enforced at the level of the nation-state, rather than the racial or religious or ethnic group. So rejecting indigenism doesn’t mean that any act of conquest or expulsion must be respected thenceforth as “facts on the ground”. In any case, both liberal internationalism and indigenism are fatally flawed as principles for determining who should invade whom. Territorial integrity — the idea that nobody should invade anybody — has much greater potential to make all the people of Earth feel secure in their homes. Smith (2023) Uphold territorial integrity 41.3 Economic Convergence as Peace Strategy Tooze Zionism has to be understood as a product of its era i.e. as a settler-colonial project, typical of European global thinking in the late 19th and early 20th century. What is distinctive about it, is that the Israelis are the last group of (mainly) Europeans to engage in the wholesale arrogation of non-European land, justified in their mission by theology, claims to civilizational superiority and nationalism. Of course, land grabs go on, all over the world, all the time. But, in the present day, the Israeli project is uniquely coherent and uniquely unapologetic as an instance of “classic” settler-colonial ideology. Due to the relatively limited resources initially at the disposal of the Zionists and the relative size of the Palestinian population, the removal of Palestinians was incomplete. The extension of the zone of Israeli settlement and the displacement, confinement and fragmentation of the Palestinian population stretches is ongoing. The most illuminating thing I have recently read on the logic of Zionism as settler colonialism is Alon Confino’s brilliant article in the History Workshop Journal of the Spring of 2023. It spells out how something that was initially considered implausible by most Zionists i.e. a Jewish Palestine with fewer Palestinians, became the achieved reality of the new state of Israel. This did not need to be the initial grand design of Zionism, or peculiar or particular to it, because it was such a common place and accepted vision at the time. As Confino points out: When Ben-Gurion considered a homogenous Jewish state a dream fulfilled following the Peel Commission (1936-7), when he wondered in 1941 what sort of transfer he could and should contemplate, his Zionist imagination fitted within the nationalist imagination of the period as well as within a postwar international context that looked favourably on forced population movements. In the 1940s massive ethnic cleansing became the order of the day in eastern Europe, creating homogenous nation-states condoned by the international community. The expulsion of ethnic groups continued until 1948 and expanded beyond Europe to India and Pakistan. These expulsions were part of a larger European process whereby the borderlands of the Austro-Hungarian, German, Russian, and Ottoman Empires, geographical areas of multiethnic co-existence, turned in the first half of the twentieth century into a locus of ethnic cleansing and genocides in state-authorized suppression of ethnoreligious difference. Palestine was part of this wider process of nationalist state-formation, from the Baltic Sea to the shores of the Mediterranean. The scale of the more or less coercive population movements in the 1940s - the historical context within which the state of Israel (and modern Pakistan, India, Germany, Poland, Czechia to name just the most prominent cases) was formed - is staggering. World War II forced perhaps 60 million people out of their homes. At the end of the war, border rearrangements and ethnic cleansing set tens of millions more on the move. In Europe perhaps as many as 20 million people were newly displaced after the war or in its very final stages. Often this took the form of chain migrations, with Poles displaced to the West by a Soviet landgrab moving into the homes vacated by Germans who had been shipped to the West. It was everywhere a violent process, driven by resentments and anger, but also haunted by moral qualms, guilt and a sense of risk and fear of impermanence. The right of return is not a question limited to Palestine. In many parts of Eurasia the conflicts of the 1940s echo down to the present day. Burma became independent as a nation state in 1948, but remains a patchwork of ethnicities and cultures. The Taiwan dispute dates to the Chinese civil war and the nationalist evacuation of 1949. The two Koreas remains technically at war. Japan is at peace with all its neighbors, but relations with both Korea and China are extremely fraught. The fact that the Israel-Palestine conflict has continued since the 1940s makes it no exception. What is exceptional is the intensity of the violence and the complicity of Western powers with Israel’s ongoing settler-colonialism. In a world in which deep continuities of conflict are common, the grand exception is Europe where two ultra-violent wars (1914-1918 and 1939-1945) and an extremely violent process of peace-making after 1945, followed by the dangerous Cold War standoff, were transmuted, first into peaceful integration of Western Europe and then the extension of the EU to much of the former Warsaw Pact as well. Even German unification was achieved without unleashing the demons of resentful nationalist irredentism, demons that were still being indulged by the German right-wing in the 1980s. By the time of the 2+4 talks, a conservative German government was finally willing to accept the postwar boundaries of Eastern Europe. German leaders and their counterparts in the rest of Europe showed considerable courage in coming to terms with the past. Czech leaders even apologized for the violence done to 3 million Germans expelled after World War II. Of course, this pacification also rested on economic success. Political integration in turn enabled economic growth in a virtuous circle. Superintending the entire process was American power and money. It was these conditions that allowed Europe’s precarious success story to congeal into a cliché for export. It is not without irony that it was precisely in the Middle East in the 1990s that the European example was taken up. This vision was derailed by the resistance of the Second Intifada, the rise of Hamas in Gaza and the lurch to the right in Israel. But regardless of events on the ground, Europe and the US clung to the Two State vision of the 1990s. The horror of October 7 consists in no small part in the fact that that vista has now been blown apart. And the shock of 2023 was that this disillusionment was not confined to the Middle East alone. This is clearly also the deep realization with regard to Russia and China: The model of economic convergence leading to geopolitical and political alignment, is dead. Tooze (2024) War, peace and the return of history in 2023 41.4 Western Development Bankruptcy Tooze The West now faces the bankruptcy of the vision of global development on which it anchored itself in the wake of the victorious end to the Cold War in Europe. We now know that the European experience of successful integration under American leadership does not generalize. The question is how it responds to this defeat. The risk, all too obvious in the Biden administration, is that the US faced with this shock, goes back to the future. Casting off the platitudes of the 1990s and neoliberalism, American strategists are tempted to return to the mid 20th century as their reference point, to the moment of America’s rise to globalism, to the heroic narrative of World War II and the Cold War. This is an important part of Biden’s rock-jawed commitment to Israel. In so doing the political class of the West misunderstand not only the world as it has developed in the 80 years since, but what the West and the United States have become as well. That misrecognition is a recipe for further frustration, shock and likely for violent confrontation. It means that history will return as catastrophe rather than constructive and deliberate change. Tooze (2024) War, peace and the return of history in 2023 41.5 Eur-Asian Primacy Policy Tensor In the history of modern great power relations, the central role played by Russo-German relations has been obscured by daydreams about dashing young Anglo-Saxons defeating Nazi forces in a heroic midcentury struggle. It’s not just that it was the Red Army that crushed both the Wehrmacht and the Kwantung Army, the entire geopolitical history of the twentieth century needs to be reconceived. In order to appreciate the secret history of the past century, instead of putting US primacy at the center of the frame, we need to center Russo-German relations. In this frame, the cycles of war and peace over the past century, at least, are governed less by the passing of the baton between the Anglo-Saxon sea powers, than by the relations of the land powers in the western half of what Mackinder called the “world island.” Mackinder delivered his lecture on “The Geographical Pivot of History” at the Royal Geographical Society in 1904. His central insight was that the geopolitical position of the power that occupied the central position in Eurasia may permit the control of the entire supercontinent; and with it would come world control; simply because Eurasia contains more power potential than the rest of the world combined. Mackinder’s genius allowed him to predict the emergence of the Soviet Union as a superpower. As a British policymaker, he tried to destroy the monster in the crib by arming the Whites in the Russian civil war. American strategists have long understood that there are three potential seats of world power: on the north American continent and on the eastern and western extremities of Eurasia. As a corollary, it is recognized by classical and modern geopolitical analysts that control of Eurasia is tantamount to ‘world control’. Of course, Russia is the only power in the system that could possibly bring both extremities of Eurasia under the command of a single center of authority. The Anglo-Saxon geopolitical position is based on what Barry Posen called ‘command of the global commons’ — a modern version of sea power envisioned by Mahan as the key to world power. But not only. The Anglo-Saxon world position is also based on the geopolitical orientation of Europe and Japan. The difference between American hegemony, and the mere fact of American maritime primacy, is the orientation of Europe and Japan — otherwise, it would be ‘dominance without hegemony’, as threatened to obtain in the mid-2000s and again under Trump. The US has tried to and will continue to prevent either extremity of Eurasia from falling under the control of a single power. The exception may be a unified Europe “led from behind” by Germany. An independent line by Europe, especially in a future Trumpist administration, cannot easily be thwarted by the Americans. Policy Tensor (2022) The Moscow-Berlin Line 41.6 Military Arsenals Smith The phrase “Arsenal of Democracy” doesn’t refer to the size of our existing military, nor does it refer to our military spending. It refers to our military production potential — how much military equipment we could make in the event of a long conventional war against a major power like China. The United States has fought multifront wars before. But in past conflicts, it was always able to outproduce its opponents. That’s no longer the case: China’s navy is already bigger than the United States’ in terms of sheer number of ships, and it’s growing by the equivalent of the entire French Navy (about 130 vessels, according to the French naval chief of staff) every four years. By comparison, the U.S. Navy plans an expansion by 75 ships over the next decade. Defending U.S. allies won’t be possible unless the United States gets its defense-industrial base in order. Since the start of the Russia-Ukraine war, total U.S. defense production has increased by a mere 10 percent—even as the war demonstrates the staggeringly high consumption of military ammunition in a major conflict. Manufacturing isn’t the only thing you need for a strong military — software, logistics, and other things also help. But most of the economic output that goes into fighting a war consists of physical weapons and supplies. For years, pundits in the U.S. dismissed the importance of manufacturing, arguing that service industries like health care and education were more important sources of the good jobs of the future. Whether or not that was true, it completely neglected the military importance of manufacturing. This blase attitude toward deindustrialization has now come back to haunt us. Smith (2023) The Arsenal of Democracy is gone 41.7 Military Matching 41.7.1 Yemens ReadSea Blockade Wagenen The embarrassment for Secretary Austin and White House advisor Jake Sullivan was swift. Shortly after the coalition’s announcement, key US allies Saudi Arabia and Egypt declined participation. European allies Denmark, Holland, and Norway provided minimal support, sending only a handful of naval officers. France agreed to participate but refused to deploy additional ships to the region or place its existing vessel there under US command. Italy and Spain refuted claims of their participation, and eight countries remained anonymous, casting doubt on their existence. Ansarallah has therefore destroyed another pillar of the White House National Security Strategy, which seeks “to promote regional integration by building political, economic, and security connections between and among US partners, including through integrated air and maritime defense structures.” Revolutions in naval warfare The Pentagon plans to defend commercial ships using missile defense systems on US and allied naval carriers deployed to the region. But the world’s superpower, now largely on its own, does not have the military capacity to counter attacks from war-torn Yemen, the poorest country in West Asia. This is because the US relies on expensive and difficult to manufacture interceptor missiles to counter the inexpensive and mass-produced drones and missiles that Ansarallah possesses. Austin made his announcement shortly after the USS Carney destroyer intercepted 14 one-way attack drones on just one day, the 16th of December. The operation appeared to be a success, but Politico swiftly reported that according to three US Defense Department officials, the cost of countering such attacks “is a growing concern.” The SM-2 missiles used by the USS Carney cost roughly $2.1 million each, while Ansarallah’s one-way attack drones cost a mere $2,000 each. This means that to shoot down the $28,000 worth of drones on 16 December, the US spent at least $28 million in just one day…. As Fortis Analysis observed, the US has eight guided missile cruisers and destroyers operating in the Mediterranean and Red Seas, with a total of 800 SM-2 and SM-6 interceptor missiles for ship defense between them. Fortis Analysis further notes that production of these missiles is slow, meaning any ongoing campaign to counter Ansarallah will quickly deplete US interceptor missile stocks to dangerously low levels. Meanwhile, the US weapons manufacturer Raytheon can produce less than 50 SM-2 and fewer than 200 SM-6 missiles annually…. While the US military is successful at producing expensive, technologically complex weapons systems that provide excellent profits for the arms industry, such as the F-15 warplanes, it is not capable of producing enough of the weapons needed to actually fight and win real wars on the other side of the world, where supply chains become even more critical…. Moscow has the industrial base and the supply chains in place to produce hundreds of thousands of the low-cost, rudimentary 152mm artillery shells – two million annually – needed for success in a multi-year war of attrition fought largely in trenches. The US, quite simply, does not. Washington’s war industrial complex is currently, at best, manufacturing 288,000 shells annually and seeks to manufacture one million shells by the year 2028, still only half of the Russian manufacturing ability. Additionally, one Russian 152mm artillery round costs $600 dollars according to western experts, whereas it costs a western country $5,000 to $6,000 to produce a comparable 155mm artillery shell. Wagenen (2023) How Yemen is blocking US hegemony in West Asia "],["belgium.html", "42 Belgium", " 42 Belgium Across the Channel, Belgium was the second country after Britain to undergo a precocious industrial revolution. The process was partly driven by capital transfers from the English North: the Lancashire blacksmith and toolmaker William Cockerill arrived in Liège in 1799, when the Southern Low Countries—formerly a Habsburg possession—had been annexed to the French Republic under Napoleon’s forces. Cockerill found the social pre-conditions for mechanizing the Verviers woollen-handicraft industry not so different from those at home. His son John Cockerill expanded the family’s machine-building firm into a massive ironworks in the Seraing basin, turning the small post-Napoleonic buffer state into a global leader in steel production. As de Gaulle later quipped, the Kingdom of Belgium was always a country ‘created by the British to annoy the French’. In 1815 the (largely Catholic) Southern Low Countries had been gifted to the (Protestant) Dutch monarch by the Congress of Vienna, but Brabant radicals rose against Dutch rule in 1830, with the tacit backing of the clergy and landowning nobility. In the heat of the 1830 revolt, Palmerston’s Cabinet engineered the new state as a perfect replica of the Westminster model, installing as its first king the uncle of the future Queen Victoria. The Belgian ruling bloc equally united a wealthy aristocratic landowning class with rising industrial strata, tightly knit around an arriviste royal house, and soon strutting an empire in the Congo built on raw-resource extraction. The family resemblance was remarkable—although Belgian elites, inured against external absolutist rule, always tolerated a higher degree of provincial and municipal autonomy. As Marx remarked, Belgium was ‘the snug, well-hedged, little paradise of the landlord, the capitalist and the priest’. The development of 19th-century Belgian capitalism also offered, in microcosm, a parallel to the deep regional divisions of its larger overseas neighbour. Western Europe’s bourgeois enclave was linguistically divided between a poor agrarian Dutch-speaking north—the province of Flanders, with its North Sea estuary port at Antwerp—and the industrializing, mainly Francophone, southern province of Wallonia, oriented south and east towards France and Germany. Like the English North, the Belgian South developed into a smokestack landscape of steel mills, textile factories and mines, the ‘industrial furrow’ of the Sambre–Meuse valley, running from the Borinage coalfields to Charleroi, Liège and Verviers. Meanwhile the large agricultural hinterland of Flanders, populated by peasant families and putting-out households, was the equivalent of Britain’s internal Irish colony. In the 1840s, overtaken by a potato famine of Irish proportions, and as domestic weaving collapsed in face of international competition, the Flemish countryside discharged hundreds of thousands of impoverished cottagers into Wallonian mines and mills. There, their immigrant children quickly grew Francophone, shedding Dutch-speaking roots. As late as 1904, Rosa Luxemburg could speak of Flemish workers as ‘also dispossessed of their language’. Wallonia’s industrial capital was never locally sourced. Instead, it was sponsored by financiers and landowners from splendid villas in Francophone Brussels, operating through holding companies structured by giant investment banks. An administrative centre under the Habsburgs, well-staffed with lawyers and bankers, Brussels was, like London, an essentially cosmopolitan city, its gaze always directed outward, and embedded in international capital flows that made it more beholden to foreign debtors than to workers in its own hinterland. These qualities also made French-speaking Brussels an essential Fremdkörper, situated within a Dutch-speaking North, overseeing a rapidly industrializing South, each either territorially or linguistically distinct from the Belgian capital. Capitalist development only deepened this outsider status. Far more than London, Brussels would be characterized by the absence of manufacturing, and consequently of an urban proletariat. Instead, as noted by the Flemish-nationalist historian Antoon Roosens, a high concentration of bourgeois and petty-bourgeois consumers made the capital ‘by far the most important market in the country for all finished industrial products.’ This ‘abnormal social composition’ also explained the city’s persistent provincialism, populated by citizens who had ‘made bourgeois mimesis their very mode of thinking and living.’footnote26 More an administrative centre than a megacity, Brussels lacked the ‘red belts’ which gave suburban Paris and London their municipal radicals (even if Marx penned the Communist Manifesto here in 1847). As with Britain, the head-start provided by Belgium’s early industrialization had turned into a disadvantage by the 1900s. Like England’s North, Wallonia was overtaken by the rise of more dynamic manufacturing centres in the Ruhr and beyond. After 1914, Belgian decline accelerated precipitously: World War One brought devastation, deepened by the Depression and Nazi occupation. In the post-war era, Belgium’s position was weaker still than Britain’s, with a much smaller and more exposed domestic market. At this point, however, the trajectories of the two economies diverged—with important consequences for their regional outcomes. A number of factors were involved. First, while post-war British leaders struggled to maintain the uk’s world-imperial privilege, Belgium’s political elites were ready for a new start. Threatened by international competition, they recognized a small economy at the centre of Europe could only survive as an open transit point for neighbouring economies. Churchill and Eden were happy to watch European integration from afar, priding themselves on the special relationship with Washington. With this imperial hangover, Britain never produced an equivalent to Paul-Henri Spaak, who played a central role in drafting the Treaty of Rome and succeeded in getting both the eec and nato headquartered in Brussels. Unlike de Gaulle, Spaak took care to hitch his country’s wagon to European integration without angering the American allies, making clear that Belgium would never plan to build a rival pole to Washington; quite the contrary.footnote27 Europeanization and internal modernization then went hand in hand. Unexpectedly, the Flemish region reaped the primary fruits of this modernizing strategy. A Ten-Year Plan re-tooled the port of Antwerp to meet the needs of American multinationals. It soon transcended mere transit status, providing a penumbra of assembly plants and light-industrial complexes around the docks to finish and repackage us goods for inland destinations. At the same time, relying on its maritime pivot, Flemish policy makers were able to turn the region into an export power for an eager German neighbour. Unlike its Irish counterpart, Flanders did not rest content as a simple safe haven for offshore capital. Instead, the Flemish elite began an ambitious drive towards a West-European knowledge economy, preparing an under-educated workforce for the era of high-value-added production. Petrochemical industries were propped up by a state-led university system, joined by world-class research clusters in bioengineering and medical sciences. By the 1960s, the Anglo-Belgian divergence was becoming plain to see. Between 1950 and 1985, Belgium’s growth rates were 50 per cent higher than the uk’s, driven mainly by modern light-industrial development around Antwerp, assisted by the regulatory machine in Brussels. When uk growth rates did recover, from 1985 to 2008, the expansion was concentrated in the Southeast, driven by crisis-prone financial expansion and asset-price inflation. Here lay another contrast: the role played by London in England’s North–South divide had no parallel in Belgium. First, Brussels has never been an organic part of either region; it is seen from Wallonia as a citadel of industrial exploiters, while for Flanders it is a mere ‘oil stain’ of francophonie. In this sense, Belgium could never be Brussellized, in the way that the uk had Londonized. On the contrary, Brussels had to watch the growth of its rival, Antwerp, as a multinational business centre, while it became mainly a supplier of regulatory services, helping American companies navigate the eec. Second, while the City of London expanded relentlessly on the basis of Eurodollar trading, the post-war retreat of Belgium’s old holding bourgeoisie demoted Brussels as a financial hub. Unlike the uk, Belgium was able to shuffle a redundant rentier class off-stage and kickstart a new developmental trajectory. And Wallonia? In 1960–61, galvanized by a massive strike wave, support grew for a regionalist breakaway movement as proposed by the charismatic metalworkers’ leader André Renard. Flemish support for the return of the Nazi-collaborationist King Leopold iii in 1950, bitterly opposed in Wallonia, helped to cast the Flemish North as a drag on the South’s socialist ambitions. Rather than accept Flemish cohabitation in a house tended by Belgium’s bourgeoisie, Wallonia’s proletariat should contemplate a proper jailbreak. The escape was to be both economic and political: autonomy for the country’s two linguistic communities, and a socialization of industry in the South. Although never a majority force in the Parti Socialiste (ps), Renardists assembled a lively cohort for socialism in one region. Before too long, however—compounding the shock loss of the Congo in 1960—Europe’s oldest steel sector was hit by the consequences of global overcapacity. Suddenly, there was no industry left to nationalize. By the early 1970s, Renard’s followers were left with a desiccated industrial landscape, only meagrely irrigated by state coffers. Meanwhile, in 1968 Flemish students had followed their Parisian counterparts by demanding an end to the Francophone dominance at the country’s oldest academic institution, the Catholic University of Leuven. Regionalization was now continuing at cruising speed, but hardly to the South’s benefit. Instead, Liège and Charleroi became the ruined temples of Belgian manufacturing, Manchesters without the sea, Pittsburghs on the Meuse. These developments gave the final push to a tottering Belgique à papa.footnote28 From 1970 onwards, Belgium’s old guard relaxed its grip on the unitary state as it initiated a series of reforms to regionalize and de-centralize the political system. Three official language communities, Dutch (59.6 per cent), French (40 per cent) and German (0.4 per cent), were established through the talentelling (language count); they would eventually acquire a council each, charged with education. Three political regions—the Brussels Capital Region, Flanders and Wallonia—were also given their own parliaments. An intricate system of financial transfers was set in place—disparagingly known as centenfederalisme, or ‘cash federalism’, by Flemish nationalists—through which regions and communities would receive the bulk of their budgets from the central government. Step by step, in the 1980s, 90s and 2000s, new institutions began to operate and the Constitution was amended to define Belgium as ‘a federal state composed of communities and regions’. Wallonia’s leaders decided to swim with the tide. During the crisis years of the 1970s, they picked at the carcass of the unitary state and secured emergency funding for Wallonia. It was clear that the centre of gravity of the Belgian economy had shifted dramatically northward: two economic poles—the port delta around Antwerp and a Brussellian metropole welcoming lobbyists into a growing eu bureaucracy—had replaced the South’s industrial magnets. The shift left behind a self-determining Wallonia that now had little to determine for itself. Subsequent generations of Walloon Socialists vacillated between performative unionism in government, to assure revenue for their region, and an assertive regionalism when stuck in opposition.footnote29 The Flemish bias towards export strategies, coupled with the North’s voting power, further marginalized the Renardist tendency. In the 1980s, the Walloon Socialist leader André Cools tried to counter regional decline by promoting municipal sharing schemes known as intercommunales: local councils could jointly manage public services and safeguard the country’s welfare gains.footnote30 Here was a further difference between Wallonia’s post-industrial status and that of England’s North. Compared to Thatcher’s onslaught, the neoliberal medicine administered by her Belgian admirer Wilfried Martens was relatively mild. The Catholic Party leader was partly checked by the stiff opposition of the Christian-Democrat trade-union wing. Federalism certainly helped to cushion the blow, albeit more through a Hegelian cunning of unreason: Belgium’s byzantine set-up has given Francophone Socialists veto power over a neoliberal push from the export-oriented North, despite the latter’s greater voting strength. With conservatives permanently unable to gain a unicameral majority à la Thatcher, it has been much easier to maintain Belgium’s corporatist structures—union control of social-security finances, enforced social bargaining, wage indexation, generous insurance mechanisms. In a small country with a relatively well-organized working class—in 2019, union membership surpassed 50 per cent—Thatcher’s Blitzkrieg on the miners never was a practical possibility. Unlike Italy or France, Belgian elites were also less eager to instrumentalize the eu to implement capitalist policy by stealth. That option required a greater degree of elite closure anyway, something Belgium’s fractious ruling bloc could never muster. Flanders became the luckiest legatee of Belgium’s regional partition. Fusing the institutions of the Flemish ‘community’ with those of the Flanders ‘region’, it achieved full parliamentary devolution in 1995. For the Flemings, the Belgian house has been uitgeleefd—out-lived, or perhaps out-grown. As with any separatist squabble, the ‘transfer debate’ remains rife with acrimony; in 2005, Flemish nationalists drove a lorry full of fake euro bills to the south of their language border. The man who performed this stunt, Bart De Wever, is now Mayor of Antwerp. He has become only slightly less histrionic in his advocacy for the city’s export interests. No regionally unified Flemish capitalist class has cohered around this transition—yet.footnote31 Both the port of Antwerp and the Brussels metropolitan region are domains where foreign companies call the shots, ‘facilitated’ by Flemish and local authorities. Attempts to grant a Flemish-separatist project real political-economic depth remain breathless at best, mostly ruses to normalize the region’s far right. Nevertheless, visions of a regionally anchored neoliberalism have enjoyed a resurgence since 2010 with the rise of the free-market n-va (Nieuw-Vlaamse Alliantie, or New Flemish Alliance), currently the dominant party in Flanders—and led by the same Bart De Wever. Opting for a gradualist line—first confederalism, then full independence—the n-va is without doubt the most vocal of all separatist formations. The other contender, Vlaams Belang, has always stuck to a more chauvinist line, preferring to save money by keeping the foreigners out. English Northerners undoubtedly have reasons to be envious of their Walloon cousins. Though companions of the same post-industrial fate, Belgian deindustrialization has treated its working classes more fairly and less punitively. Walloon clientelism has proved less financialized, with social housing keeping up a steady pace of growth, in contrast to the council-housing sell-offs granted by Thatcher to the North’s ex-factory workers—said by some to be a key indicator of the Brexit vote. Contemporary Belgium is certainly no corporatist Eden, untouched by the market turn. But it has resisted many of the trends that have scarred countries in the developed world and boasts a better Gini-coefficient than other early industrializers. Anglo-Belgian divergences should not be overstated, however. Regionalization has hardly been a benediction to Belgium’s South. While Britain has six of the ten poorest regions in Northwest Europe, the Walloon regions of Hainaut, Liège and Charleroi are little better off. And just as South Yorkshire is only a few hours from inner London—still Europe’s richest district—so Liège also lies conspicuously closely to Luxembourg. Federalization has helped Wallonia, but it has hardly saved it. The cinema of the Dardenne brothers, with its focus on ‘poverty’ rather than class, provides aesthetic backing for a ps project of federally funded regional poverty management for the South that has given up hopes of reindustrialization altogether. The Dardennes’ oeuvre, from Rosetta to Two Days, One Night and The Unknown Girl, makes a striking contrast to the class confrontations depicted in the electrifying 1934 documentary Misère au Borinage by Henri Storck and Joris Ivens. Flemish neoliberals remain hopeful about a separatist free-trade breakthrough, letting the ‘best student in the Belgian class’ flourish next to competitors in Poland or Latvia. To no avail, however: anno 2021, the Belgian state is still here, badly mismanaging the covid crisis. But managing, nonetheless. It has to be said that the regional response to covid was just as shambolic as the federal one. Beneath Belgium’s so-called ‘communitarian’ crisis smoulders not only a medical or logistical crisis but above all a political one, affecting Belgium’s party democracy at its core. Recently leaked memos of the 2019 governmental negotiations indicated Francophone Socialists’ willingness to split between regions not only social security, but also labour-market policy and fire services. Hoping to secure its baronies in Brussels with a final pay out, the ps appeared willing to trade in the national achievements of the Belgian labour movement. Some tough questions follow. As the Walloon example shows, behind the question of regionalization stands the more intimidating one of capital investment. The English North never acquired a form of proto-statehood that would allow it to practice a properly local developmentalism; it was forced instead into an amorphous form of rebellion, within a topsy-turvy electoral geography that never provided a platform for regional consciousness. A look at post-industrial regions that did gain this form of statehood, however, is not comforting. In 2016, Walloon prime minister and Socialist Party leader Paul Magnette garnered laurels from the European left for his rebellion in the federal parliament against the imposition of the neoliberalizing eu–Canada trade deal. This act of resistance hid a structural dependency of the Walloon region on Flemish transfers, which have grown precipitously in the wake of federalization. Behind this lies the secular decline of Wallonian industry, unable to profit from containerization and shut out from the German-led Central European export boom. Jäger "],["china.html", "43 China 43.1 Education 43.2 Leadership", " 43 China Roberts 43.1 Education The chinese are smarter than us Though we eagerly compare heritable traits like height, appearance, and athletic ability, we rarely compare intelligence, yet we boast that America’s Ashkenazi Jews, one percent of the population, have won forty percent of our Nobel Prizes. Chinese women are unambivalent. For millennia, they have refused to marry unintelligent men,’dry branches’, and thus culled their DNA from the gene pool. So ancient is this process that Ron Unz 28 found Chinese intelligence almost immune to socio-economic factors. Healthy Swiss are fifty times richer and receive twice the schooling of ill-nourished rural Chinese children who have carried a heavy disease burden for generations, yet Chinese IQs are consistently higher, “The reported Chinese PISA scores are far above those of the United States and nearly every European country, many of which are almost totally urbanized and have incomes ten times that of China. It is almost unimaginable that any non-East Asian population of rural villagers with annual incomes in the $1,000 range would have tested IQs very close to 100 [the US median]. We would certainly expect Chinese numbers to rise further as the country continues to develop. Still, my point is that East Asian IQs seem to possess a uniquely high floor compared with those of any other population”. What does that mean in practice? Seventy percent of us have IQs between 85-115: we are smart enough to lead happy, productive lives. Five percent of us IQs are above 125 are smart enough for medical school, but only one of us out of two hundred, with an IQ of 140, can handle a Ph.D. in theoretical physics. And one in ten thousand have 160 IQs and can do groundbreaking work in any discipline. The US, with an average IQ of 100, has thirty-six thousand of these super-geniuses while China, with four times more people and a national IQ of 105, has three hundred thousand 29 . Anatoly Karlin predicts 30 that a combination of the Flynn effect 31 , poverty elimination and improved rural education and nutrition will lift the national IQ average to Shanghai’s 108, thus doubling the pool of super-geniuses. Since virtually all Chinese super-geniuses work for the government, it is not surprising that Henry Kissinger 32 concluded, “The Chinese are smarter than us”. Basic literacy in Chinese demands a working memory capable of handling at least three thousand characters, and mastery requires the use of contextual variables to extract their meaning from millions of possible combinations. Children’s working memory skills at five years of age were the best predictor of reading, spelling, and math outcomes six years later. Analogous to computer RAM; working memory is the part of short-term memory devoted to immediate conscious perceptual and linguistic processing. 43.2 Leadership As has been the case since the birth of Christ, China is governed by a Confucian just hierarchy that is designed to weed out sociopaths and recruit able, compassionate people willing to sacrifice their lives in service to the people. So it is not surprising that, instead of viewing the State as intrusive, untrustworthy, and threatening, the Chinese see themselves belonging to a family-state and view their politicians as family patriarchs. Under such leadership, they value collective over individual wellbeing, the future over the present, pragmatism above ideology, and outcomes over promises–a value system that provides social cohesion and has made China the richest, strongest nation on earth for most of its existence. Roberts (2022) Here comes China (pdf) "],["columbia.html", "44 Columbia 44.1 Macroeconomic Policy", " 44 Columbia 44.1 Macroeconomic Policy n April 2021, Ivan Duque’s administration presented a tax reform bill labeled “Law of Sustainable Solidarity” to Congress. The bill contemplated an increment of the VAT on basic goods in conjunction with an increase in the marginal tax rates on the income of the so-called Colombian middle class. The vast majority of whom earns monthly less than 4,000,000 Colombian pesos (around 1,065 U.S. dollars). Although the bill put on the table contained some crucial elements for discussion, such as implementing a “basic monthly income” of 21 U.S. dollars (by far less than the current minimum wage). It contained little or nothing to effectively tackle Colombia’s high social and income inequality (with an official GINI of 0.526 for 2019). Since 2016 after the peace deal between the Colombian government and the FARC, which used to be the oldest and biggest guerrilla in Colombia, the government hasn’t implemented most of the elements contemplated in the peace agreement. Also, although Colombia has had macroeconomic stability for more than 20 years, an indicator such as the official unemployment rate has consistently been above 10%. The level of poverty before the COVID-19 shock was near 32%. Developing Economics (2021) For a new macroeconomic policy in Colombia "],["denmark.html", "45 Denmark", " 45 Denmark Schwartz Danish industrial policy with regard to pharmaceuticals illustrates the tensions around both Mark I and Mark II types of innovation strategy. Recent Danish governments have promoted Denmark as an “innovation country” and formed a “Disruption Council” intended to preserve the country’s economic position. Danish R&amp;D is highly concentrated in the pharmaceutical and biomedical sector (Vestas’ windmill production and servicing provides a side bet on alternative energy, but Chinese competition squeezes profitability). A government-run public goods strategy supports the emergence of Mark I type firms in biotechnology, thus providing a “feedstock” for Mark II oriented Novo Nordisk. This strategy continues a long tradition of “extension” type services designed to bring new technology and best practices from universities and high performing firms to more traditional firms including manufacturing SMEs. It is supplemented with venture capital from the Danish Growth Fund (Vækstfonden). The policy has helped keep Danish firms largely at the technology frontier, producing a narrower gap between leading and lagging firms than in the rest of the OECD. But the challenge in pharmaceuticals is anticipating the bio-genomics revolution rather than adapting to it. In cooperation with Novo Nordisk, the Danish government runs a cluster of data collection and dissemination organizations: the Danish National Genome Center (Nationalt Genom Center), the National Biobank (Danmarks Nationale Biobank under the State Serum Institute), and the overarching Royal Bio and Genome Bank. These centers leverage the comprehensive and diachronic data the state collects from all hospitals and practitioners on a large range of conditions and patient variables. The databanks also enable smaller biotech firms to access large volumes of data on relatively smaller (in terms of incidence) diseases and thus help them overcome one major research hurdle. Put simply, they generate and hold the data that machine-learning and AI driven R&amp;D projects need to function. Novo Nordisk meanwhile acts as a central contractor with many smaller research-oriented firms—though these contracts generally choke off growth from Mark I innovation in favor of pre-emption by the lead firm. Overall, however, Denmark lacks indigenous AI and ML firms that might supply expertise to the entire sector, as compared with either Israel or Sweden. Schwartz (2023) The Nokia Risk "],["england.html", "46 England", " 46 England Tom Hazeldine’s The Northern Question imposes a much-needed historical lens on the discussion.footnote4 Rather than trade in essentialism about a North hesitant to change, Hazeldine deploys a Marxist method to explain the region’s woes. His first point of reference is Antonio Gramsci, whose reflections on the ‘Southern Question’ inflect the opening pages of his book. The Italian Marxist saw his party as the challenger to a timid Northern bourgeoisie that had failed to rally the peninsula around a popular-democratic Jacobin programme; instead, it brokered deals with Southern landowners and ecclesiastical classes, burdening the unified Italian nation-state with its typically hybrid character. Only a party with Machiavellian ambitions for national renewal could complete the task shirked by Italy’s Northern leaders, unwilling and unable to bury the old order. Hazeldine proposes a measured projection of this Gramscian frame onto Britain. The Northern Question takes as its epigraph the words of Gramsci’s prodigious Scottish pupil, Tom Nairn: The lamented ‘growing abyss’ between North and South should not really be a subject for mere figures, nor for moral outrage, nor for futile retreads of Westminster-inspired ‘modernization’: it can’t be tackled within the existing State, because it is the existing State, the dominance of the Crown (or ‘anti-industrial’) culture, the thriving pseudo-nationalism of the Old Regime. Any analysis of the North must of course begin with the question of whether ‘it’ actually exists. Hazeldine is clear that there is more to the region than cultural affect, an aggregate of accents and music scenes. Geographic definitions have varied: north of the River Trent, the Mersey, the Ribble—or the Severn–Wash divide, which would include the economically blighted Midlands? Hazeldine’s answer is structural. He wants to explore the relation of the rise and fall of the North, as an industrial powerhouse, to the rise and rise of London, as a capital of empire and high finance. As he notes, deindustrialization has meant that contemporary regional disparities in England have been blurred; they are now characterized less by a national division of labour than by ‘the positional superiority of London in a services-dominated national economic space’. In this context, the northern rustbelt acts as the ‘senior representative’ of a much larger left-behind England.footnote6 Yet the North has never achieved the ‘escape velocity’ needed to free itself from its industrial past: where the mills and coal-pits started, the North begins. It was the Industrial Revolution that transformed the region from ‘an obscure, ill-cultivated swamp’ (Engels) into the hub of an industrial-capitalist system that would be emulated the world over. Hazeldine argues that the North has been the launch pad for three successive attempts to challenge the hegemony of the Southern-based regime of landed-finance capital. Jäger "],["finland.html", "47 Finland", " 47 Finland Schwartz In the early 2000s, Finland was the darling of industrial and employment policy analysts everywhere. This small country with a population of 5.5 million and a GDP roughly equal to the state of Oregon experienced what looked like a high tech-led productivity revolution. Real GDP per capita in local currency terms rose 55 percent from 1995 to 2007—nearly double the US increase and close to the pinnacle of the twenty-one richest OECD industrialized economies. Yet spectacular growth abruptly halted after 2008. GDP continued to rise with population growth, but from 2008 to 2019 real Finnish per capita income declined. The European Central Bank’s dilatory response to the eurozone crisis, and the austerity policies that followed, undoubtedly explain part of this abysmal performance. But an equally large part is due to Finland having many of its growth eggs in a single basket: Nokia. Nokia’s handsets and related telephony equipment accounted for 20 percent of Finnish exports at peak, driving Finland’s current account surplus to nearly 7 percent of GDP. When the Apple iPhone launched in 2007, Nokia’s handset market collapsed, exports fell by half, Finland’s current account swung into deficit, and a decade plus of economic stagnation began. Finland is not the only economy facing “Nokia risk.” A larger group of seven countries—all of them relatively small, rich, and with stable governments—are similarly exposed. In Denmark, Israel, South Korea, Sweden, Switzerland, and Taiwan a handful of firms account for a hugely disproportionate share of both profits and R&amp;D spending. Schwartz (2023) The Nokia Risk "],["israel.html", "48 Israel 48.1 Gaza War 48.2 Zionism 48.3 Shimon Perez Peace Policy", " 48 Israel Schwartz Israel’s huge defense related investment in software and sensor capacity created a vibrant Mark I-type tech sector. But Israelis doing that Mark I innovation typically sell their firms or technology to US Mark II type firms. The relative absence of big domestic tech firms explains Israel’s deviation from the broader pattern of profit share being above GDP share It also explains why so many Israelis—an estimated 100,000—simply migrate to Silicon Valley even though Israel’s so-called Silicon Wadi usually houses more start-up firms per capita than any other country. Both trends potentially inhibit a response to the AI and ML revolution in software. Schwartz (2023) The Nokia Risk 48.1 Gaza War AI - mass assassination factory In one case discussed by the sources, the Israeli military command knowingly approved the killing of hundreds of Palestinian civilians in an attempt to assassinate a single top Hamas military commander. “The numbers increased from dozens of civilian deaths [permitted] as collateral damage as part of an attack on a senior official in previous operations, to hundreds of civilian deaths as collateral damage,” said one source. “Nothing happens by accident,” said another source. “When a 3-year-old girl is killed in a home in Gaza, it’s because someone in the army decided it wasn’t a big deal for her to be killed — that it was a price worth paying in order to hit [another] target. We are not Hamas. These are not random rockets. Everything is intentional. We know exactly how much collateral damage there is in every home.” Another reason for the large number of targets, and the extensive harm to civilian life in Gaza, is the widespread use of a system called “Habsora” (“The Gospel”), which is largely built on artificial intelligence and can “generate” targets almost automatically at a rate that far exceeds what was previously possible. This AI system, as described by a former intelligence officer, essentially facilitates a “mass assassination factory.” The increasing use of AI-based systems like Habsora allows the army to carry out strikes on residential homes where a single Hamas member lives on a massive scale, even those who are junior Hamas operatives. In the majority of cases, the sources added, military activity is not conducted from these targeted homes. The goal was to “kill as many Hamas operatives as possible,” for which the criteria around harming Palestinian civilians were significantly relaxed. As such, there are “cases in which we shell based on a wide cellular pinpointing of where the target is, killing civilians. This is often done to save time, instead of doing a little more work to get a more accurate pinpointing. The emphasis is on damage and not on accuracy. The Israeli army estimates that it has killed between 1,000 and 3,000 armed Palestinian militants. This deadly policy continues today — thanks in part to the use of destructive weaponry and sophisticated technology like Habsora, but also to a political and security establishment that has loosened the reins on Israel’s military machinery. Fifteen years after insisting that the army was taking pains to minimize civilian harm, Gallant, now Defense Minister, has clearly changed his tune. “We are fighting human animals and we act accordingly,” he said after October 7. Abraham (2023) A mass assassination factory’: Inside Israel’s calculated bombing of Gaza Tooze Benjamin Netanyahu, at least in private, has been blunt about what Israel needs most to destroy Hamas: a steady supply of more US bombs. “We need three things from the US: munitions, munitions, and munitions,” the Israeli prime minister told a group of local government officials, according to a recording obtained by the Israel Hayom newspaper. “There are huge demonstrations in western capitals,” added Netanyahu, who is concerned political pressure overseas might threaten the US arms shipments. “We need to apply counter-pressure . . . There have been disagreements with the best of our friends.” Israel has expended vast amounts of ammunition in its war against Hamas in Gaza. The modern western weaponry used, from satellite-guided “bunker busting” bombs to pinpoint-accurate laser-guided missiles, have eroded Hamas’s military capabilities and, according to the Israel Defense Forces, killed more than 5,000 of the group’s estimated 30,000 fighters. Citing estimates of damage to urban areas, military analysts say the destruction of northern Gaza in less than seven weeks has approached that caused by the years-long carpet-bombing of German cities during the second world war. “Dresden, Hamburg, Cologne — some of the world’s heaviest-ever bombings are remembered by their place names,” said Robert Pape, a US military historian and author of Bombing to Win, a landmark survey of 20th century bombing campaigns. “Gaza will also go down as a place name denoting one of history’s heaviest conventional bombing campaigns.” Tooze (2023) Munitions Coppola Israel’s actions are not only brutal beyond belief, but utterly stupid. It has played right into Hamas’s hands. All Hamas had to do was sit tight and watch the bodies pile high, knowing that eventually Netanyahu would have to choose between losing the war or committing genocide. Israel, for its part, is desperately begging the world to rescue it from having to commit a genocide by accepting the lesser crime of ethnic cleansing. Coppola (2024) Looking back at 2023 - a personal view 48.1.1 South Africa ICJ accusation of genocide South Africa (2023) Israels Genocide in Gaza to ICJ (pdf) Mearsheimer The 84-page “application” that South Africa filed with the International Court of Justice (ICJ) on 29 December 2023, accusing Israel of committing genocide against the Palestinians in Gaza. It maintains that Israel’s actions since the war began on 7 October 2023 “are intended to bring about the destruction of a substantial part of the Palestinian national, racial and ethnic … group in the Gaza Strip.” That charge fits clearly under the definition of genocide in the Geneva Convention, to which Israel is a signatory. The application is a superb description of what Israel is doing in Gaza. It is comprehensive, well-written, well-argued, and thoroughly documented. The application has three main components. First, it describes in detail the horrors that the IDF has inflicted on the Palestinians since 7 October 2023 and explains why much more death and destruction is in store for them. Second, the application provides a substantial body of evidence showing that Israeli leaders have genocidal intent toward the Palestinians. (59-69) Indeed, the comments of Israeli leaders – all scrupulously documented – are shocking. One is reminded of how the Nazis talked about dealing with Jews when reading how Israelis in “positions of the highest responsibility” talk about dealing with the Palestinians. (59) In essence, the document argues that Israel’s actions in Gaza, combined with its leaders’ statements of intent, make it clear that Israeli policy is “calculated to bring about the physical destruction of Palestinians in Gaza.” (39) Third, the document goes to considerable lengths to put the Gaza war in a broader historical context, making it clear that Israel has treated the Palestinians in Gaza like caged animals for many years. It quotes from numerous UN reports detailing Israel’s cruel treatment of the Palestinians. In short, the application makes clear that what the Israelis have done in Gaza since 7 October is a more extreme version of what they were doing well before 7 October. There is no question that many of the facts described in the South African document have previously been reported in the media. What makes the application so important, however, is that it brings all those facts together in one place and provides an overarching and thoroughly supported description of the Israeli genocide. In other words, it provides the big picture while not neglecting the details. Even though the South African application focuses on Israel, it has huge implications for the United States. The Biden administration is complicitous in Israel’s genocide. Mearsheimer (2023) Genocide in Gaza 48.1.2 US Intervention Farooqi The problem, of course, is the absolute primacy of political economy. Bill Ackman is only the ugly tip of the iceberg. What Walt and Mearsheimer called the Israel Lobby is just the institutional expression of the will of megadoners who dictate US Israel policy. No politician in DC has the balls to stand up to them, at risk of being primaried and politically destroyed. This means that there is no political room whatsoever for the United States to pressure Israel on any issue—much less end the special relationship. The Gaza war is eroding America’s world position and undermining Biden’s reelection prospects. But Biden does not have the political capital to take on the megadonors who dictate US Israel policy. So what is the way out here? Israel is an American protectorate in a dangerous region. I have for a while been suggesting that the US should unilaterally interpose itself between the Israelis and Palestinians, and create a state for Palestine on its own authority. Israeli Defense Minister Gallant has suggested, as part of his “day after” proposal, that a US-led international force should maintain order in Gaza. The US should support this course of action. The US should intervene unilaterally and decisively. Simply put, we should take the Palestine problem off Israel’s hands. We should draw up a territorial map of the Palestinian state and impose it on the two parties. The Israelis will require some persuasion. But even they are looking for a serious solution to the problem. If the US moves in decisively, the Israelis will play along. The Palestinians would finally be getting a state—they’d be stupid to contest the imposition. The end goal would be create a viable state for the Palestinians. Palestine will remain a ward of the international community for some time. But over time, as the security situation stabilizes and the state is rebuilt, prosperity should return. We can give Palestine preferential access to Western markets and encourage foreign investment to help the process along. This is very much doable. This course of action would guarantee Israeli security, solve the Palestine problem once and for all, and restore America’s authority on the world stage. It might even save Joe Biden’s reelection campaign. Comments Pxx Pxx 19 hrs ago ·edited 19 hrs ago “If the US moves in decisively, the Israelis will play along” Interesting theory. Really, what’s more likely in the proposed scenario? That US forces in what’s supposedly a sovereign Palestine chase the Israeli settlers out of the West Bank? Or would US forces become the enforcer for a resumption of the slow-motion ethnic cleansing that was already taking place? There’s no fixing this. Best US can do (under the bipartisan “political reality” in Washington) is when Israel is done exterminating and/or expelling the Gazans, find a scapegoat for the domestic press. Probably Netanyahu personally. Then help bribe and/or threated the ICJ judges, to save the rest of the Israeli political establishment - who are all in on this in 9/11-hysteria style. Then USN can go back to doing FONOP’s in a part of the world that’s less hazardous to sail thru. Finally, hope the allies that stick with the US and Israel after all this (eg Brits, Germans) will pretend nothing actually happened. But even that’s optimistic. Here’s the flipped take: “If Israel moves decisively, the US will play along” Meaning, for the US, having a proper war against Yemen and Lebanon Thanks for having the integrity to discuss the issue, in any case. More than we can say about much of the “PMC” class nowadays Feral Finster The United States has lost any pretense of being an honest broker. Were it to intervene in Palestine this would be seen only as a more direct participation in Israel’s open ethnic cleansing, and, barring that, outright genocide. Emanuel Francis What record of successful American State-building in the M/E justifies your optimism about this plan’s viability? The Americans need to treat Israel like a normal country similar to, say, Eisenhower did during Suez Crisis. That must be the first priority. Farooqi (2024) US Intervention in Palestine Must Be Decisive 48.1.3 Etnic Cleansing Welsh Historically, Israelis have been very good at ethnic cleansing: after all, almost everything in Palestine was Palestinian less than a hundred years ago, and now almost nothing is. This is the last gasp, the attempt to finish the job. Welsh (2023) Let’s Discuss Israel And Gaza Again 48.1.4 End game Welsh But Israel has systematically trained up its enemies. Hezbollah is their creation, it would not exist if they had not invaded and occupied Lebanon. Hamas was actively supported by them in order to undercut the PLA and fragment resistance. Israel taught its enemies how to fight against overwhelming force; how to hide from assassinations and shrug them off when they inevitably happen; how to operate against air superiority and American style electronic intelligence. Israel has been a US dependency for a long time, but right now, if it weren’t for the US, it would probably collapse economically and militarily. This war an embarassment for the Israelis. It shows they can’t protect the settlers, they can’t win against Hamas and they can’t stop Hezbollah from striking settlements. It’s just embarassment all around. They want to widen the war and get Iran and the US into it, but they overestimate the US: the US can’t send ships into the Gulf and expect to leave. Most of its regional allies would be unwilling to let it run its airforce off their airfields, and the US can devastate Lebanon but it can’t stop Hezbollah from devastating Tel Aviv. Israel’s on its way down. I don’t know how this mess will turn out, in the end, but US and Israeli weakness has been made clear. Neither are what they were. Add this to the humiliation of Ukraine and it’s clear the American century is all but over and that its allies are no longer safe. As for a peace, if it’s negotiated, it’ll be done under Chinese auspices. Think about that a little. Welsh (2023) Let’s Discuss Israel And Gaza Again 48.1.5 Prospects Freedman Proposals tabled by Egypt just before Christmas envisaged three stages: an extendable two-week halt to the fighting which would see more hostages released in return for more Palestinian prisoners; an Egypt-sponsored ‘Palestinian national talk’ aimed at ending the division between the Fatah party (that dominates the PA) and Hamas, leading to the formation of a technocratic government in the West Bank and Gaza to oversee the reconstruction of the Strip and eventual Palestinian parliamentary and presidential elections; a comprehensive cease-fire, involving the release of all hostages in return for more prisoners, including some held for more serious terror offenses, Israeli withdrawal of its forces from Gaza and the return of those Gazans from the north to their homes. Whether or not Israel could be convinced to engage with this plan, one might have thought that Hamas could be tempted. Indeed Ismail Haniyeh, Chairman of Hamas’s political bureau, based in Qatar, did show interest. He is aware that Hamas will struggle to stay in sole charge of the Strip, not least because they lack the funds and capacity for the massive reconstruction task ahead. To retain influence it would make sense to come together with Fatah, even though that could mean in effect recognising Israel and abandoning thoughts of its elimination. But Hamas is also divided. Sinwar, the head of the military wing and de facto leader, has shown disdain for Haniyeh in the past (and did not inform him of what was planned for 7 October). He has rejected any new working arrangement with Fatah as ‘outrageous.’ He wants to be in charge of Gaza and, like Netanyahu, would prefer that all talk of Gaza’s future be put off until the fighting is over. As far as Sinwar is concerned the war is going fine and he will be content if it ends with Hamas still present and able to continue as before, whatever the wretched state of the territory. For now Hamas looks stronger than the ineffectual PA, daring to attack Israel and then getting prisoners released in return for hostages. In practice, if Hamas remains active but excluded, it can cause trouble for whoever tries to run Gaza, even if the Israelis have left. If the US, Europeans, and Arabs really want to get a grip on this situation they are going to have to go beyond attempts to mediate between recalcitrant parties and engage more directly with the situation. [Freedman (2024) Israel/Gaza: Retrospect and Prospect] 48.2 Zionism 48.2.1 Evilness Coppola “The settlers want to see the sea,” said Daniella Weiss, leader of a Zionist settler movement. “In order to see the sea, it is necessary that there are no homes. There will be no homes, there will be no Arabs. It’s just an elegant romantic way of saying I want to see the sea. How will you see the sea? There will be no homes, there will be no Arabs, look at the sea.” I was shocked by the contrast between her bright eyes and pretty face, and the ugliness of her words. Evil was wearing the face of a middle-aged Jewish woman. Figure: The Evil Face of Zionism I have no doubt that what I saw in her face and heard in her words was pure evil. In that video she called for the violent clearing of the whole of Gaza, for no other reason than that Jewish settlers want the land. Over two million people live in Gaza, but she was not remotely concerned about their fate. For her, they were simply vermin to be eradicated. I cannot call the desire to displace or wipe out over two million people anything other than evil. I hope you can’t either. Never in all my life had I thought evil could possibly appear in such a form. Coppola (2023) Evil wears an ordinary face 48.3 Shimon Perez Peace Policy Tooze It is not without irony that it was precisely in the Middle East in the 1990s that the European example was taken up. This is how Ari Krampf summarizes Israeli peace politics in the 1990s: The link between liberalized markets and a dovish perception of security issues was embodied in the political vision of Shimon Peres, the political figure who played a key role in the realization of the internationalist neoliberal vision. Peres, one of the patrons of the (1985Israel financial) Stabilization Plan, regarded the Plan as a central element in his geopolitical vision, encapsulated in the notion of the New Middle East (Peres, 1993; see also, Ben Porat, 2005a). One cannot avoid noticing that Peres was inspired by the process of European integration, where free markets had been endorsed as a regional pacifying mechanism: “Ultimately, the Middle East will unite in a common market—after we achieve peace. And the very existence of this common market will foster vital interests in maintaining the peace over the long term” (Peres, 1993, p. 99). Guy Ben Porat describes Peres’ book as a “blue- print for the future of the region based on economic rationality, peace, democracy, cooperation, mutual gain and general prosperity.” The Middle East, according to Peres’ vision, argues Ben Porat, needs to choose between “peace, global integra- tion and progress” and “continuing conflicts and backwardness” (Ben Porat, 2005a, p. 39). The link between economic and national security interests was also based on the interest of the Israeli private sector, which was expected to benefit from the realization of the New Middle East vision. It was also supported by the Israeli intellectual and professional elites (Keren, 1994). Economic cooperation and economic development of the Palestinian Authority were (supposed to be) an essential element in the New Middle East policy agenda. The internationalist neoliberal agenda was manifested in the “Paris Protocol” signed in April 1993 between Israel and the Palestinian Authority, which specified that “The two parties view the economic domain as one of the cornerstone in their mutual relations with a view to enhance their interest in the achievement of a just, lasting and comprehensive peace” (Gaza-Jericho Agreement, 1994). As Krampf goes on to argue, this vision was derailed by the resistance of the Second Intifada, the rise of Hamas in Gaza and the lurch to the right in Israel. But regardless of events on the ground, Europe and the US clung to the Two State vision of the 1990s. The horror of October 7 consists in no small part in the fact that that vista has now been blown apart. Tooze (2024) War, peace and the return of history in 2023 "],["nicaragua.html", "49 Nicaragua", " 49 Nicaragua Economist Mr Ortega lost an election in 1990 and stepped down. Since coming back to power in 2007, he has vowed not to lose it again. On returning to office he cosied up to businessmen and won over the Roman Catholic church with one of the world’s strictest abortion bans. Between 2008 and 2015 Nicaragua bought $4.5bn of Venezuelan oil at discounted prices. The proceeds from selling it were funnelled into banks owned by the ruling party and lavished on Mr Ortega’s supporters. The economy grew on average by 5% a year between 2010 and 2017. The share of people living on less than $3.20 per day fell from 27% in 2005 to 10% by 2017. With potential critics silenced or bought off, Mr Ortega took over all branches of the state. The Supreme Court (widely considered to be controlled by Mr Ortega) abolished term limits and expelled the leader of the opposition and 16 of his supporters from Congress. More recently the regime has become bloodier and more brazen. When students and pensioners peacefully protested against the government in 2018 (over cuts to social security), police and Sandinista goons killed over 350 people. Before a general election last year, in which Mr Ortega won a fourth consecutive term, all seven opposition presidential candidates were locked up. Economist "],["palestine.html", "50 Palestine 50.1 Oslo Accords", " 50 Palestine 50.1 Oslo Accords MacArthur For a long time now, my sympathies have been with the more than 700,000 Palestinians who were driven from their lands in 1948 by Zionist militants–despite my visceral sympathy for the more than 100,000 Jews who survived the Holocaust, were trapped in displaced-persons camps, and who broke through the British blockade in 1945-1948 to enter Palestine and help establish the state of Israel. Today, I feel isolated, not unlike two writers whom I consider to be my guiding lights on the Israeli-Palestinian conflict, I.F. Stone and Edward Saïd, who were also isolated because of the positions they took. Stone, a brilliant journalist, wrote Underground to Palestine, a masterful, essential text for understanding the founding of Israel and its moral justification. Even before the war of “independence” that established the Jewish state in 1948, Stone was already advocating for a single and “binational” country that would respect both “nations,” Arab and Jewish, and two languages, Arabic and Hebrew, without taking into account a majority that could change over time. A left-wing Jew, Stone suffered many insults, including that of “self-hating Jew.” Saïd, a great literary scholar from a wealthy Palestinian-American family, pushed me further in my respect for the demands of his exiled people. An English professor at Columbia, Saïd influenced an entire generation of students through his mastery of modern British literature and his cultural critique of imperialism. I met him in 1977 when I was a student at Columbia and a freelancer for a major newspaper that had sent me to interview him during his appointment to the Palestinian National Council, a kind of “parliament in exile,” as he called it. In retrospect, it is remarkable how right Saïd was to treat as a capitulation Yasser Arafat’s adherence to the 1993 Oslo Accords and the creation of the Palestinian Authority. Saïd had understood that Oslo was a ploy that would mark “the end of the peace process,” rather than progress toward a Palestinian state. Like I.F. Stone, his unorthodox point of view earned him attacks from his Arab compatriots, supported by President Clinton, whom Saïd described as “a twentieth-century Roman emperor shepherding two vassal kings through rituals of reconciliation and obeisance.” The Oslo Accords provided no agreement on the status of Jerusalem; no curbs on illegal Jewish settlements in the West Bank and Gaza; no recognition of Palestinian sovereignty. What were the accords for if not for the construction of a weak and corrupt government of seven disconnected enclaves under Israeli stewardship? Arafat ended up banning Saïd’s books in his strongholds, and Saïd ended up adopting I.F. Stone’s ideal: “the one-state solution.” Absurd? Utopian? Not more so than “the two-state solution,” which has become a cynical trap advocated by hypocrites. No matter what, Arabs and Jews intermingle demographically–there are already 2 million Palestinians who are Israeli citizens. No other practical solution appears to be within sight. Where are the Stones and Saïds of our day to provide us with realistic and principled points of view? Who is there to reach out to across this bloody trench? MacArthur (2023) One-State Solution Wikrent (2023) Week-end Wrap – Political Economy – December 24, 2023 "],["spain.html", "51 Spain 51.1 Civil War Reconciliation", " 51 Spain 51.1 Civil War Reconciliation Jones José Celda – Pepe to his friends – was shot dead against a wall in the small Valencian town of Paterna at five in the afternoon on 14 September 1940. The 45-year-old farmer, whose body was buried in a mass grave, was one of the thousands of represaliados, or victims of reprisals, who were murdered by the Franco regime well after the end of the civil war in April 1939. El abismo del olvido, a collaboration between the graphic artist Paco Roca and the journalist Rodrigo Terrasa, examines the atrocities and the generational agonies they inflicted, and continue to inflict, on the families of the dead. As well as telling Celda’s story, the comic chronicles the tireless and solitary struggle that his daughter Pepica waged to fulfil her mother’s wishes for his bones to be found and reinterred with hers. Pepica Celda, who was eight when her father was murdered, began her quest after the socialist government of José Luis Rodríguez Zapatero introduced its landmark 2007 historical memory law that was intended to bring a measure of justice and comfort to Franco’s victims. She made headlines a few years later for becoming the last person in Spain to secure a government subsidy for her search before the conservative People’s party (PP) took power in 2011 and ended the funding. History, however, has taught them not to expect too much. “Take the place in Paterna where they shot José Celda and 2,000 other people,” Terrasa said. “There’s no plaque or memorial at all like you’d find in any other civilised European country. If you go there now, you might find a bunch of now-rotten republican wreaths that were left by a memorial association, but the rest is rubbish and bottles.” The book ends with a picture of that refuse-strewn mass murder scene, and with a rhetorical question: “You can tell a lot about a society from the way it buries its dead. What would they say about ours?” Jones (2024) ‘It’s about being able to say goodbye’: Spanish graphic novel explores early Franco-era reprisals "],["sweden.html", "52 Sweden", " 52 Sweden Schwartz Sweden has an abundance of large, mature Mark II type firms. Yet these have gradually hollowed out actual production in favor of simply generating intellectual property. The extreme case here is Volvo. Its old automobile capacity was sold first to Ford and then to the Chinese holding company Zhejiang Geely.5 Geely relies on Swedish engineering talent to design the new Volvo electric vehicle line (including Polestar), but production has largely shifted outside of Sweden. Similarly, Swedish pharmaceutical giants Pharmacia and Astra respectively ended up controlled by Pfizer (USA) and AstraZeneca (UK). As noted above, the big Swedish manufacturing firms have steadily reduced employment. Like Israel, Sweden is increasingly a hunting ground for foreign multinational firms looking for talented individuals. This sustains high-wage employment—at least for some. But it also means the profits end up somewhere else, and large Mark II firms that might anchor a research network are harder to form. Schwartz (2023) The Nokia Risk "],["usa.html", "53 USA 53.1 (The Lack of) American Ideology 53.2 Kissinger’s Legacy 53.3 New Confederacy", " 53 USA 53.1 (The Lack of) American Ideology Welsh The problem with American leaders is that they don’t believe in anything enough to die for it. Oh, they have beliefs, the beliefs of a leech (which is unfair to leeches, which are, unlike ticks, largely beneficial to their hosts.) They really, really believe in neoliberalism, because it has made them filthy rich. But die for it, except in the sense of “destroy the world for profit?” No. The leadership of Hamas, Hezbollah, even Iran to a lesser extent, have beliefs they are willing to die for, personally, not just send other people to die for. Further, Hamas, Hezbollah and the Iranian army (especially the Revolutionary Guards) are ideological organizations. From top to bottom, they believe in more or less the same things. You could kill the top 99 leaders of those orgs, and Mr. #100 would not be that much different. In our society and our organizations, corporate or military or civil service, the people at the top have significantly different beliefs from the people in the middle, who have different beliefs from those at the bottom. Further, because in our organizations there is vast infighting, because there isn’t any consensus beyond “make money” or “get power”. In organizations where, in fact, everyone isn’t pulling in more or less the same direction (if perhaps fighting a bit over “how to get there) leadership matters. The interests of employees in corps are not the same as executive interests. They don’t want the same things, or benefit from the same policies. None of this applies significantly to Hezbollah or Hamas, to Ansar Allah (the Houthis) or (to a lesser extent) to the Revolutionary Guard. You could kill Nasrallah, the leader of Hezbollah tomorrow and it would make very little difference. Leaders of genuine ideological organizations (we’re going to discuss this more in the future) do not have the calculus of late capitalists “leaders.” They do not think the same way. they do not feel the same way. And the organizations they run have genuine missions that the leaders and followers both believe in. It’s been so long since we had almost any of that in our society that we don’t get how it works. Even NGOs aren’t like that: I know NGO workers and professional staff: they believe, but the people who run the NGOs don’t, actually, and don’t act in alliance with their values, morals and ethics. Welsh (2023) Assassination Will Not Help Israel 53.2 Kissinger’s Legacy Ackerman The Yale University historian Greg Grandin, author of the biography Kissinger’s Shadow, estimates that Kissinger’s actions from 1969 through 1976, a period of eight brief years when Kissinger made Richard Nixon’s and then Gerald Ford’s foreign policy as national security adviser and secretary of state, meant the end of between three and four million people. “The Cubans say there is no evil that lasts a hundred years, and Kissinger is making a run to prove them wrong,” Grandin told Rolling Stone not long before Kissinger died. “There is no doubt he’ll be hailed as a geopolitical grand strategist, even though he bungled most crises, leading to escalation. He’ll get credit for opening China, but that was De Gaulle’s original idea and initiative. He’ll be praised for detente, and that was a success, but he undermined his own legacy by aligning with the neocons. And of course, he’ll get off scot free from Watergate, even though his obsession with Daniel Ellsberg really drove the crime.” Ackerman (2023) Henry Kissinger, War Criminal Beloved by America’s Ruling Class, Finally Dies Cirincione Kissinger was uniquely culpable. He not only helped kill the peace talks in 1968 that would have stopped the war, promoted a “secret plan” to end the war that actually extended it and broadened it, but he was the major advocate for the carpet bombing that killed hundreds of thousands in Southeast Asia and radicalized a Cambodian resistance, leading directly to the murderous Pol Pot regime that killed a million more. And this is just one of his crimes. As others have documented during Kissinger’s life and in the week since he died, Kissinger is directly responsible for the murder of hundreds of thousands of people in Southeast Asia, Bangladesh, East Timor, Argentina, Chile and many other countries. He swung U.S. power behind dictators around the world, explicitly endorsing their brutal military campaigns against domestic opponents in the name of preserving American global dominance. Many intelligent, witty and successful people fall from power. Kissinger never did. The answer may be as simple as the basic motivations that dominate Washington and the upper reaches of American society. People on the rise want to be near those with power, money and prestige. Kissinger offered all that — with the veneer of Harvard scholarship. It is irresistible to those who are, as Shakespeare put it, “seeking the bubble reputation.” When former Secretary of State George Shultz convinced Kissinger to join him, former Senator Sam Nunn and former Secretary of Defense William Perry in a sweeping vision of “A World Free of Nuclear Weapons,” published in The Wall Street Journal in January 2007, I was willing to put aside my anger and criticism of Kissinger’s war crimes to use his prestige to advance a breakthrough in nuclear policy. I was happy to spend the next few years promoting their views. It was very powerful to be able to say that four men who had helped build the American nuclear arsenal now concluded that it should be eliminated. Kissinger, the most hardline of the group, gave it the most credibility. Nowhere in my writings or speeches did I note that he was a war criminal with the blood of millions on his hands. It was simply too useful to leverage Kissinger’s prestige for validation. While Kissinger enjoyed several major policy successes, including detente with China and the Soviet Union, most of these could have been accomplished by others. Most of his policies failed — at tremendous cost. They brought ruin to millions. They didn’t solve the problems they were supposed to address. The threats they were designed to counter were often exaggerated, making the brutal policies he advocated unnecessary and counter-productive. It wasn’t Vietnam, for example, that brought down Nixon, it was Watergate. When Nixon resigned, Gerald Ford kept Kissinger as secretary of state. Kissinger gave Ford credibility. The brutal dictator, Augusto Pinochet, that Kissinger brought to power in a violent September 1973 coup, ruled for twenty years with U.S. support, longer than any other leader of Chile, despite having killed, jailed and tortured tens of thousands of his fellow citizens. Kissinger and Pinochet were wrong. Salvadore Allende, the disposed Socialist president of Chile, didn’t represent a threat to the continent. There never was a genuine Soviet threat to Chile or Latin America. The policies Kissinger advocated and Pinochet implemented were completely unnecessary. But the image of strength, security and determination served both men well. Let us hope that we do not again see his like. Benjamin Netanyahu’s carpet bombing of Gaza is a direct descendent of Kissinger’s failed Vietnam strategy. Imagine a Military Targeting Bot with the Instincts of Henry Kissinger. Cirincione (2023) Why Do So Many Praise Henry Kissinger? 53.3 New Confederacy MacLean Frustrated by the surprise defeat of Mitt Romney in the 2012 presidential race, a group of breathtakingly rich and highly strategic actors on the radical right, including the Koch brothers, quietly launched an ambitious new campaign to lock in their political control once and for all. They had used their immense wealth and institution-building savvy to capture a majority of state legislatures in 2010, so the groundwork was already in place. This campaign would be spearheaded by a corporate pay-to-play group they had long funded to influence state laws—the American Legislative Exchange Council (ALEC)—and a dark money group with deep ties to Charles and the late David Koch (who died in 2019), as well as the Tea Party movement—Citizens for Self-Governance (CSG). When legislators arrived at ALEC’s annual meeting in August 2013, they were given detailed instructions and model text to bring back to their statehouses for a resolution demanding the first Constitutional convention since 1787…. In the decade since those first secretive meetings, Meckler’s Convention of States has managed to rack up wins in nineteen states for a convention that would address sweeping proposals to radically curtail the powers of the federal government. ALEC-led groups also claim to have twenty-eight states behind their call for a more limited convention to propose a balanced budget amendment to the U.S. Constitution. Should a convention be convened, what is it that the ultra-rich backers want? Their chosen so-called grassroots leaders mince no words when speaking to friendly audiences. Meckler has declared that the purpose is “to reverse 115 years of progressivism.” In fact, the endgame is even more consequential: to return this nation to its pre-Constitution roots under the Articles of Confederation, with a weak central government and sovereign states…. Indeed, most of what ALEC, CSG, and their billionaire backers want to achieve flies in the face of public opinion. And that’s what makes their plan so devious. “Voters have no role to play in the right’s vision of a Constitutional convention,” a report by the Center for Media and Democracy (CMD) concluded. Delegates would be handpicked by legislative leaders, and here’s the kicker: The votes taken at such a convention would be based not on population but on one vote per state in order to grossly underrepresent the majority of Americans. In audio obtained by CMD, former U.S. Senator Rick Santorum, Republican of Pennsylvania, told an ALEC audience in 2021 how this strategy could be used to circumvent what most Americans want. “Because their [Democrats’] population is concentrated and ours isn’t,” Santorum said, “rural voters [Republicans] . . . actually have an outsized power granted under this process.” He added, “We have the opportunity as a result of that to have a supermajority, even though . . . we may not even be in an absolute majority when it comes to the people who agree with us.” MacLean (2023) Constitution in the Crosshairs: The Far Right’s Plan for a New Confederacy "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 210102 Climate Finance Shadow Report 2020", " C NEWS C.1 210102 Climate Finance Shadow Report 2020 Oxfam has released this report with subtitle Asessing progress towards the $100 billion commitment Progress is NOT in line with need or pledges. Climate change could undo decades of progress in development and dramatically increase global inequalities. There is an urgent need for climate finance to help countries cope and adapt. Over a decade ago, developed countries committed to mobilize $100bn per year by 2020 to support developing countries to adapt and reduce their emissions. The goal is a critical part of the Paris Agreement. As 2020 draws to a close, Oxfam’s Climate Finance Shadow Report 2020 offers an assessment of progress towards the $100bn goal. Based on 2017–18 reported numbers, developed countries are likely to claim they are on track to meet the $100bn goal. And on their own terms, they may be. But how the goal is met is as important as whether it is met. The dubious veracity of reported numbers, the extent to which climate finance is increasing developing country indebtedness, and the enduring gap in support for adaptation, LDCs and SIDS, are grave concerns. Meeting the $100bn goal on these terms would be cause for concern, not celebration. Oxfam Report (pdf) "],["sitelog.html", "D Sitelog", " D Sitelog Latest Additions January 1, 2024 \\ January 1, 2024 oligarchy\\        Oligarchy should be understood as the politics of wealth defense January 1, 2024 \\ January 1, 2024 religion\\        modernist fall for the opiate of social science and stock market analysts January 1, 2024 geopolitics\\        yemens red sea blockade arsenals mismatch January 2, 2024 geopolitics\\        forced displacements of peoples isreal-gaza in historic context January 2, 2024 israel\\        israels 1990s peace policy perez January 2, 2024 geopolitics\\        economic convergence as peace strategy January 2, 2024 geopolitics\\        western development model bankruptcy January 4, 2024 civilization\\        fermi paradox drake equation climate change civilization collapse January 5, 2024 israel\\        south africa accusation to icj for israels genocide in gaza January 7, 2024 israel\\        us intervene between israel and gaza proposal January 7, 2024 israel\\        etnic cleansing and end game January 7, 2024 geopolitics\\        rules-based subimperial order January 8, 2024 israel\\        gaza prospects January 8, 2024 spain\\        el abismo del olvido civel war reconciliation "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
