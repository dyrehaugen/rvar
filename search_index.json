[["index.html", "Varia 1 Varia", " Varia Dyrehaugen Web Notebook 2021-06-27 1 Varia "],["entropy.html", "2 Entropy 2.1 Social Architecture - Ian Wright 2.2 Information Entropy - Shannon", " 2 Entropy 2.1 Social Architecture - Ian Wright Think of entropy as a number that measures the randomness of a distribution. The higher the entropy the more random the distribution. The most random distribution of all is the uniform distribution. It’s the most random distribution because every x is equally likely. There’s a constraint on entropy maximisation. That constraint is the conservation of energy. We can calculate the distribution that corresponds to maximum entropy subject to a total energy constraint. The answer to this optimisation problem is the exponential distribution. Ian Wright 2.2 Information Entropy - Shannon In a single groundbreaking paper, he laid the foundation for the entire communication infrastructure underlying the modern information age. The heart of his theory is a simple but very general model of communication: A transmitter encodes information into a signal, which is corrupted by noise and then decoded by the receiver. Despite its simplicity, Shannon’s model incorporates two key insights: isolating the information and noise sources from the communication system to be designed, and modeling both of these sources probabilistically. He imagined the information source generating one of many possible messages to communicate, each of which had a certain probability. The probabilistic noise added further randomness for the receiver to disentangle. Before Shannon, the problem of communication was primarily viewed as a deterministic signal-reconstruction problem: how to transform a received signal, distorted by the physical medium, to reconstruct the original as accurately as possible. Shannon’s genius lay in his observation that the key to communication is uncertainty. This single observation shifted the communication problem from the physical to the abstract, allowing Shannon to model the uncertainty using probability. This came as a total shock to the communication engineers of the day. First, Shannon came up with a formula for the minimum number of bits per second to represent the information, a number he called its entropy rate, H. This number quantifies the uncertainty involved in determining which message the source will generate. The lower the entropy rate, the less the uncertainty, and thus the easier it is to compress the message into something shorter. For example, texting at the rate of 100 English letters per minute means sending one out of 26100 possible messages every minute, each represented by a sequence of 100 letters. One could encode all these possibilities into 470 bits, since 2470 ≈ 26100. If the sequences were equally likely, then Shannon’s formula would say that the entropy rate is indeed 470 bits per minute. In reality, some sequences are much more likely than others, and the entropy rate is much lower, allowing for greater compression. Second, he provided a formula for the maximum number of bits per second that can be reliably communicated in the face of noise, which he called the system’s capacity, C. This is the maximum rate at which the receiver can resolve the message’s uncertainty, effectively making it the speed limit for communication. Finally, he showed that reliable communication of the information from the source in the face of noise is possible if and only if H &lt; C. Thus, information is like water: If the flow rate is less than the capacity of the pipe, then the stream gets through reliably. His theorems led to some counterintuitive conclusions. Suppose you are talking in a very noisy place. What’s the best way of making sure your message gets through? Maybe repeating it many times? That’s certainly anyone’s first instinct in a loud restaurant, but it turns out that’s not very efficient. Sure, the more times you repeat yourself, the more reliable the communication is. But you’ve sacrificed speed for reliability. Shannon showed us we can do far better. Repeating a message is an example of using a code to transmit a message, and by using different and more sophisticated codes, one can communicate fast — all the way up to the speed limit, C — while maintaining any given degree of reliability. Another unexpected conclusion stemming from Shannon’s theory is that whatever the nature of the information — be it a Shakespeare sonnet, a recording of Beethoven’s Fifth Symphony or a Kurosawa movie — it is always most efficient to encode it into bits before transmitting it. So in a radio system, for example, even though both the initial sound and the electromagnetic signal sent over the air are analog wave forms, Shannon’s theorems imply that it is optimal to first digitize the sound wave into bits, and then map those bits into the electromagnetic wave. This surprising result is a cornerstone of the modern digital information age, where the bit reigns supreme as the universal currency of information. Shannon’s general theory of communication is so natural that it’s as if he discovered the universe’s laws of communication, rather than inventing them. His theory is as fundamental as the physical laws of nature. In that sense, he was a scientist. Shannon invented new mathematics to describe the laws of communication. He introduced new ideas, like the entropy rate of a probabilistic model, which have been applied in far-ranging branches of mathematics such as ergodic theory, the study of long-term behavior of dynamical systems. In that sense, Shannon was a mathematician. Shannon’s theory has now become the standard framework underlying all modern-day communication systems: optical, underwater, even interplanetary. Shannon figured out the foundation for all this more than 70 years ago. How did he do it? By focusing relentlessly on the essential feature of a problem while ignoring all other aspects. Shannon (Quanta Magazine) "],["complexity.html", "3 Complexity 3.1 Wicked Problems", " 3 Complexity 3.1 Wicked Problems The Wicked Problem of COVID-19 The pandemic’s complexity transcends health, environment, economy, and social boundaries. Any intervention triggers responses in those various fields. From Oz Sahin, Hengky Salim, Emiliya Suprun, Shannon Rutherford , et al., “Developing a Preliminary Causal Loop Diagram for Understanding the Wicked Complexity of the COVID-19 Pandemic,” Griffith University, Australia Systems , May 2020 Wicked Complexity (pdf) "],["evolution.html", "4 Evolution 4.1 Organization of Life 4.2 Intentional Evolution 4.3 Metabolic Scaling 4.4 Hiarchies", " 4 Evolution 4.1 Organization of Life The conflict between lower-level selfishness and higher-level welfare pervades the biological world. Cancer cells selfishly spread at the expense of other cells within the body, without contributing to the common good, ultimately resulting in the death of the whole organism. In many animal societies, the dominant individuals act more like tyrants than wise leaders, taking as much as they can for themselves until deposed by the next tyrant. Single species can ravage entire ecosystems for nobody’s benefit but their own. But goodness has its own advantages, especially when those who behave for the good of their groups are able to band together and avoid the depredations of the selfish. Punishment is also a powerful weapon against selfishness, although it is often costly to wield. Every once in a great while, the good manage to decisively suppress selfishness within their ranks. Then something extraordinary happens. The group becomes a higher-level organism. Nucleated cells did not evolve by small mutational steps from bacterial cells but as groups of cooperating bacteria. Likewise, multi-cellular organisms are groups of highly cooperative cells, and the insects of social insect colonies, while physically separate, coordinate their activities so well that they qualify as super-organisms. Life itself might have originated as groups of cooperating molecular reactions. Only recently have scientists begun to realize that human evolution represents a similar transition. In most primate species, members of groups cooperate to a degree but are also each other’s main rivals. Our ancestors evolved to suppress self-serving behaviors that are destructive for the group, at least for the most part, so that the main way to succeed was as a group. Teamwork became the signature adaptation of our species. Extant hunter-gatherer societies still reflect the kind of teamwork that existed among our ancestors for thousands of generations. Individuals cannot achieve high status by throwing their weight around but only by cultivating a good reputation among their peers. Most of human moral psychology – including its other-oriented elements such as solidarity, love, trust, empathy, and sympathy, and its coercive elements such as social norms enforced by punishment – can be understood as products of genetic evolution operating among groups, favoring those that exhibited the greatest teamwork. From Genes to Culture Teamwork in our ancestors included physical activities such as childcare, hunting and gathering, and offense and defense against other groups. Human teamwork also acquired a mental dimension including an ability to transmit learned information across generations that surpasses any other species. This enabled our ancestors to adapt to their environments much more quickly than by the slow process of genetic evolution. They spread over the globe, occupying all climatic zones and hundreds of ecological niches. The diversity of human cultures is the cultural equivalent of the major genetic adaptive radiations in dinosaurs, birds, and mammals. The invention of agriculture initiated a positive feedback process between population size and the ability to produce food leading to the mega-societies of today. Cultural evolution differs from genetic evolution in important respects but not in the problem that lurks at every rung of the social ladder. Just like genetic traits, cultural traits can spread by benefitting lower-level units at the expense of the higher-level good – or by contributing to the higher-level good. There can be cultural cancers, no less so than genetic cancers. And for teamwork to exist at any given rung of the social ladder, there must be mechanisms that hold the wolves of selfishness at bay. A nation or the global village is no different in this respect than a human village, a hunter-gatherer group, an ant colony, a multi-cellular organism, or a nucleated cell. Modern nations differ greatly in how well they function at the national scale. Some manage their affairs efficiently for the benefit of all their citizens. They qualify at least as crude superorganisms. Other nations are as dysfunctional as a cancer-ridden patient or an ecosystem ravaged by a single species. Whatever teamwork exists is at a smaller scale, such as a group of elites exploiting the nation for its own benefit. The nations that work have safeguards that prevent exploitation from within, like scaled-up villages. The nations that don’t work will probably never work unless similar safeguards are implemented. Accomplishing teamwork at the level of a nation is hard enough, but it isn’t good enough because there is one more rung in the social ladder. Although many nations have a long way to go before they serve their own citizens well, a nation can be as good as gold to its own citizens and still be a selfish member of the global village. In fact, there are many examples in the international arena, where nations protect their own perceived interests at expense of the common global future. Wilson and Hessen 4.2 Intentional Evolution The Evolutionary Manifesto Evolutionary Manifesto Cooperative Evolutionary Transitions Abstract Stewart Major Cooperative Evolutionary Transitions occur when smaller-scale entities cooperate together to give rise to larger-scale entities that evolve and adapt as coherent wholes. Key examples of cooperative transitions are the emergence of the complex eukaryote cell from communities of simpler cells, the transition from eukaryote cells to multicellular organisms, and the organization of humans into complex, modern societies. A number of attempts have been made to develop a general theory of the major cooperative transitions. This paper begins by critiquing key aspects of these previous attempts. Largely, these attempts comprise poorly-integrated collections of separate models that were each originally developed to explain particular transitions. In contrast, this paper sets out to identify processes that are common to all cooperative transitions. It develops an alternative theoretical framework known as Management Theory. This general framework suggests that all major cooperative transitions are the result of the emergence of powerful, evolvable ‘managers’ that derive benefit from using their power to organize smaller-scale entities into larger-scale cooperatives. Management Theory is a contribution to the development of a general, “all levels” understanding of major cooperative transitions that is capable of identifying those features that are level-specific, those that are common across levels and those that are involved in trends across levels. Stewart: General Theory 4.3 Metabolic Scaling Hatton Metabolic scaling theory has had a profound influence on ecology, but the core links between species characteristics have not been formally tested across the full domain to which the theory claims to apply. We compiled datasets spanning all eukaryotes for the foremost body mass scaling laws: metabolism, abundance, growth, and mortality. We show that metabolism and abundance scaling only follow the canonical ±3/4 slopes within some taxonomic groups, but across eukaryotes reveal reciprocal near ±1 slopes, broadly supporting the “energetic equivalence rule.” In contrast to metabolism, growth follows consistent ∼3/4 scaling within many groups and across all eukaryotes. Our findings are incompatible with a metabolic basis for growth scaling and instead point to growth dynamics as foundational to biological scaling. Scaling laws relating body mass to species characteristics are among the most universal quantitative patterns in biology. Within major taxonomic groups, the 4 key ecological variables of metabolism, abundance, growth, and mortality are often well described by power laws with exponents near 3/4 or related to that value, a commonality often attributed to biophysical constraints on metabolism. However, metabolic scaling theories remain widely debated, and the links among the 4 variables have never been formally tested across the full domain of eukaryote life, to which prevailing theory applies. Here we present datasets of unprecedented scope to examine these 4 scaling laws across all eukaryotes and link them to test whether their combinations support theoretical expectations. We find that metabolism and abundance scale with body size in a remarkably reciprocal fashion, with exponents near ±3/4 within groups, as expected from metabolic theory, but with exponents near ±1 across all groups. This reciprocal scaling supports “energetic equivalence” across eukaryotes, which hypothesizes that the partitioning of energy in space across species does not vary significantly with body size. In contrast, growth and mortality rates scale similarly both within and across groups, with exponents of ±1/4. These findings are inconsistent with a metabolic basis for growth and mortality scaling across eukaryotes. We propose that rather than limiting growth, metabolism adjusts to the needs of growth within major groups, and that growth dynamics may offer a viable theoretical basis to biological scaling. Hatton (2019) Linking Scaling Laws across Eukaryotes (pdf) Blair Fix (twitter) 4.4 Hiarchies Turchin One of the greatest puzzles of human evolutionary history concerns the how and why of the transition from small-scale, ‘simple’ socie- ties to large-scale, hierarchically complex ones. Multilevel selection suggests that complex hierarchies can arise in response to selection imposed by intergroup conflict. People living in small-scale societies are fiercely egalitarian and use a variety of ‘leveling institutions to reduce inequality. Complex societies are vastly inegalitarian. Small-scale societies have simple structure. Thus, local communities may be grouped in larger units (‘tribes’), but usually there are no levels of organization above that, and there are no permanent control centers. Complex societies, on the other hand, are centralized and have many levels of hierarchical organization. Complex societies have states – coercion-wielding hierarchical organizations managed by administrative specialists (bureaucracies). There is a strong correlation between hierarchical complexity and state organization. Multilevel Selection The theory of multilevel selection provides insights into the evolu- tion of such traits as altruism that are subject to conflicting selec- tion pressures. In the pithy characterization of D. S. Wilson and E. O. Wilson (2007), ‘Selfishness beats altruism within groups. Altruistic groups beat selfish groups.’ Whether altruism spreads in the population, or not, depends on the balance of within-group (in- dividual level) and between-group (higher level) selection forces. Human groups need to be well-integrated by within- group cooperation in order to effectively compete against other groups. There are biological limits on the size of an egalitarian group, in which the basis of cooperation is face-to-face interactions. The main limit has to do with the size of the human brain. According to the ‘social brain’ hypothesis the evolution of human brain size and intelligence during the Pleistocene was largely driven by selective forces arising from intense competition between individuals for increased social and reproductive success. One can view language as a tool that originally emerged for simplifying the formation and improving the efficiency of coalitions and alliances. The huge and energetically demanding brains of humans, ac- cording to this theory, evolved in order to store and process large amounts of social data. To function well in a social group an indi- vidual needs to remember who did favors for whom and, alterna- tively, who cheated whom. One must be able to calculate the po- tential ramifications of one’s actions towards another individual and how it will affect the relationships with third parties. The prob- lem is, as the group increases in size, the potential number of rela- tionships that one must keep in mind grows exponentially. Once a human group attains the size of roughly 150 individuals, even the hypertrophied human brain becomes overwhelmed. For group size to increase beyond the few hundred individuals typical of small-scale human societies, evolution had to break through the barriers imposed by face-to-face sociality. Humans evolved the capacity to demarcate group membership with symbolic markers. Markers such as dialect and language, clothing, ornamentation, and religion allowed humans to determine whether someone personally unknown to them was a member of their cooperating group or, vice versa, an alien and therefore an enemy. The second evolutionary innovation was hierarchical organization. The elementary building block for hierarchical organizations is a bond between a superior and an inferior ‘agents.’ If agents are individual humans, then this relationship takes the form of one between a leader and a follower, or a lord and a vassal. The growth of hierarchies occurs primarily by adding extra levels of organization and, therefore, is not limited by social channel capacity. Any member of a hierarchy needs to have a face-to-face relationship only with, at most, \\(n + 1\\) persons: the maximum number of subordi- nates (the ‘span of control’), \\(n\\), plus an additional link to its own superior. Hierarchical societies are also not limited by social channel capacity, and can potentially reach any size, as long as it is possible to add new hierarchical levels. An acephalous tribe is the largest social scale a human group can achieve without the benefit of centralized organization. Greater social complexity requires leaders – chiefs, kings, caliphs, presidents, prime ministers, or politburo chairmen. Adding extra levels of social organization beyond a complex chiefdom usually requires transition to a more formal political or- ganization – the state. The state ischaracterized by a formal division of labor: separate organizations specializing in administration (a bureaucracy), coer- cion (an army), law (a judiciary), and religion (a church). Large-scale hierarchically complex societies arose as a result of evolutionary pressures brought on by warfare. As Charles Tilly (1975) famously said, ‘states made war, and war made states’ The key insight from evolutionary theory (and, specifically, multilevel selection) is that warfare is an extreme form of parochial altruism, driven by the ‘cooperate to compete’ evolutionary logic. Up until the present the force driving the evolution of increased social scale has always been competition/conflict in opposition to some other societies. If the global state were to arise, where will it find the external threat that would keep it unified? Unless (or until) the humanity experiences a major evolutionary breakthrough that will provide a different basis for large-scale cooperation, the rise of a stable state unifying all humanity is unlikely. The history of the European Union (EU), a most audacious and innovative experiment in building a supranational community, ap- pears to support this pessimistic conclusion. Two reasons for the apparent reversal of the integrative dynamic in Europe. First, adversarial relations with the Soviet block (the ‘Evil Empire’) helped to suppress internal bickering among the member states. Second, rapid expansion into Central and Eastern Europe, by simply adding new members in a completely unstructured way, was clearly a mistake, as lasting increase in social scale can be accomplished only by adding extra layers of hierarchical organization. Without an international authority possessing sufficient coercive power to hold individual states in check, great powers will continue their attempts to gain power at each other’s expense leading, inevitably, to interstate rivalry and war. All through the history, and for the foreseeable future, integration among humans required conflict against other humans. Even if a world-wide state were to arise, according to this logic, it would rapidly fission into multiple parts. On the other hand, neither history nor evolution is destiny. Humans have transcended their evolutionary limitations before. Turchin: Evolution of Complex Hierarchical Societies (pdf) "],["agency.html", "5 Agency", " 5 Agency Mumford’s mega-machine as biological metaphor All living things are essentially robots — bits of matter that have become animated. None of the fundamental constituents (molecules and atoms) have any agency. Nonetheless, through the miracle of complexity, matter somehow organizes into forms that at least appear (to us) to have agency. ‘Mmajor evolutionary transitions’ happens when a whole new level of natural selection emerges. Life started as replicating molecules, that then organized into larger proteins (RNA), that then organized into prokaryotic cells, that then merged into eukaryotic cells, that then organized into multicellular organisms, that then grouped into eusocial animals … At least in the context of human evolution, the emergence of civilization counts as a major evolutionary transition. We went from being a social primate to being ‘ultrasocial.’ At least in the context of human evolution, the emergence of civilization counts as a major evolutionary transition. We went from being a social primate to being ‘ultrasocial.’ Enter Mumford’s ‘megamachine.’ The emergence of civilization went hand in hand with the concentration of power. Humans, for the first time, organized in large-scale hierarchies. If you want to frame this transition in terms of evolutionary theory, then it’s just part of a longer story. With every major transition, units that were previously ‘autonomous’ became cogs in the emergent larger ‘machine.’ But in a strict scientific sense, it’s still ‘machines all the way down.’ There is no unit where you can distinguish between ‘living’ matter and ‘dead’ matter. It’s all just matter. Blair Fix "],["hegel.html", "6 Hegel 6.1 Science of Logic", " 6 Hegel Lefebvre: in contrast to Feurbach’s humanism, which is in fact a myth of a naturalism, Hegel’s idealism “saw that man is not given biologically, but produces himself in history, through life in society, that he creates himself in a process.” 6.1 Science of Logic Mathematical Interpretation Wright Hegel’s Science of Logic, written in the early 1800s, is a difficult book, to say the least. One difficulty is that Hegel’s project seems fantastical: he aims to discover the fundamental structure of everything from pure reflection alone. Materialists and empiricists will rightly hesitate. Another difficulty is Hegel’s methodology, which is shocking: doubting even Rene Descartes’ ‘I’ he claims to start his enquiry with zero assumptions – and yet derives multiple propositions. How is it possible to reason with no axioms or inference rules? Another difficulty is Hegel’s language: difficult ideas often necessitate technical terms and complex locutions, but Hegel twists the reader up, down, left and right and the effect is dizzying. So it’s a difficult book, and one which most readers, over the centuries, have either never picked up, or quickly put down. Hegel’s beginning is irreducibly dynamic in the sense that the core concepts – pure being and pure nothing – are not merely concepts but actual occurrent processes (that we intuit by self-referential philosophical reflection). I will begin to apply the mathematical language of the calculus to represent change. We simply have no choice but to use currently available languages to describe the phenomena, but, of course, the phenomena isn’t those languages. Change implies a sequence of different ‘states’ that the thing that changes exhibits. Pure being changes positively with respect to itself; in consequence, pure being increases exponentially. Pure nothing changes negatively with respect to itself; in consequence, pure nothing decreases exponentially. The coming-to-be of pure being and the ceasing-to-be of pure nothing are identical once we remove the idea of an arrow of time. So we can make sense of Hegel’s startling claim that pure being and pure nothing are ‘the same’ and also ‘they are not the same.’ They are not the same because pure being is about existence, whereas pure nothing is about non-existence, and hence they self-interact in different ways (pure being affirms itself, pure nothing denies itself). But they are also the same because the shape of these self-interactions are isomorphic. Their behaviour is identical. These different behaviours are isomorphic to each other, via the reciprocal map, \\(f(x)=1/x\\). Notably, the map is an involution, i.e. \\(f(f(x))=x\\), and therefore is its own inverse. In the language of calculus, the coming-to-be of pure being speeds towards infinity. In this sense, pure being explodes. For example, any natural or mechanical systems (which we cannot properly talk about here, but only mention by analogy) undergoing exponential growth quickly fall apart. They could only exist, at best, for a short period of time. Similarly, the ceasing-to-be of pure nothing collapses to zero. Again, any natural or mechanical systems that obeyed this exponential law of decrease would quickly cease and become entirely inert. In this sense, pure nothing implodes. In both cases, the systems ‘vanish,’ either by exploding or imploding. So although pure being and pure nothing are present at the metaphysical bedrock, and imply each other, as self-referential systems they are unstable. They cannot permanently exist in their pure states of self-reflection. Thought contemplating itself can never catch its own tail, but will endlessly chase it, caught forever in a self-referential loop. As thought contemplating nothing it can never eradicate its own existence, and therefore forever sustains some residual of thought in the self-referential loop. We can now, at last, explicate Hegel’s ‘causal’ sense of vanishing into an opposite. Consider pure being in the state epsilon close to zero (where we imagine epsilon is a really small magnitude as close to zero as we wish). This state is arbitrarily close to the asymptote of pure nothing. But since being is a coming-to-be this ’empty’ state rapidly vanishes towards the state of pure being (infinity). More prosaically: when we try to contemplate absolutely nothing there’s an irreducible element of our own existence, which when noticed, takes over, and flowers into pure being. We can make sense of Hegel’s beginning by interpreting this passage as essentially talking about isomorphic positive and negative feedback loops. Hegel’s philosophical reflection asks us to perform the following mental exercise: shed all your knowledge and assumptions, including your own existence as an individual person and simply contemplate the existence of thought. When you do that you’ll enter a self-referential feedback loop. This mental state has paradoxical properties (especially from the point-of-view of formal logic). First, it seems like you are contemplating pure being (that is existence itself) but also it seems like you are contemplating pure nothing (zero content, or non-existence). Both points-of-view make sense. But the interpretations are unstable, and don’t settle down, and spontaneously flip back-and-forth. And, furthermore, what the feedback loops seem to strive towards – the state of pure being or pure nothing – can never be reached. In this sense, they cannot exist, both ‘logically’ and ‘causally.’ Since the beginning doesn’t make sense and cannot exist it therefore cannot really be the beginning after all. The beginning must be something else. At this point Hegel introduces the concept of ‘sublation.’ Becoming is the sublated unity of being and nothing. They no longer self-relate but ‘interpenetrate’ each other Figure: Becoming is the sublated unity of being and nothing. They no longer self-relate but ‘interpenetrate’ each other The unity of being and nothing expressed as a system of coupled first-order differential equations. We need to refer to these equations. I’ll call them ‘Hegel’s equations‘, and sometimes ‘Hegel’s contradiction‘. ‘Determinations are of unequal value’ in the straightforward sense that, in general, \\(dx/dt\\) is not equal to \\(dy/dt\\). Hegels Sublation: Grasped as thus distinguished, each is in their distinguishedness a unity with the other. Becoming thus contains being and nothing as two such unities, each of which is itself unity of being and nothing; the one is being as immediate and as reference to nothing; the other is nothing as immediate and as reference to being; in these unities the determinations are of unequal value The coupled system mirrors Hegel’s natural language description of sublation remarkably well. The reference is such ‘that is to say, it passes over into it‘. In the coupled system, we have a ‘substance’ that actually flows from being into nothing, and the ‘substance’ leaves nothing and enters into being. Now we also have a typically Hegelian claim: becoming is both a ‘ceaseless unrest’ and a ‘quiescent result.’ So the ‘vanishing’ that previously implied that pure being and pure nothing could not exist, now, in this sublated state, ‘vanishes the vanishing itself’ such that we now have ceaseless unrest that paradoxically collapses into a stable result (that presumably doesn’t vanish by either exploding or imploding). Hegel’s equations are equivalent to two second-order differential equations that, in terms of their motion, are independent (although they are coupled in terms of their initial conditions). Now we have the acceleration of being (or nothing) changing negatively with respect to itself. We need to solve the second-order linear differential equations. Since this deduction applies to both being (x) and nothing (y) I’ll just solve for the temporary variable (z): In this mathematical interpretation of becoming, being changes according to cos(t) and nothing changes according to -sin(t). Being and nothing oscillate between -1 and 1, forever. When being realises its maximum (at an absolute value of 1) then nothing is at its absolute minimum of 0. When nothing realises its maximum, then being is at its minimum. What one ‘gains’ the other ‘loses,’ but neither ‘side’ ever wins. What is being gained, and what is being lost? It’s tempting to introduce familiar physics-based concepts, such as amplitude or energy etc. But Hegel employs the term ‘indeterminate being,’ which is the status of pure being and pure nothing prior to their sublation. I will use the slightly more evocative term ‘substance.’ Being and nothing continually exchange their substance with one another: at one time being is more substantial, at another time nothing is. Define the total substance contained within the unity as the sum of the squares of x and y (to handle the negative values). As soon as we do that, we immediately see that Hegel’s equations instantiate a simple conservation law: \\[x^{2} + y^{2} = cos^{2}t + sin^{2}t = 1\\] We can think of this perpetual trade-off between being and nothing as either eternal conflict, or an eternal dance of co-operation. Hegel, more simply, describes it as ‘a ceaseless unrest.’ The union of being and nothing is unstable, any equilibrium is immediately undermined, and the opposing concepts remain in perpetual contradiction. We’ve shown that Hegel’s contradiction does generate ceaseless unrest. But in typically Hegelian fashion, becoming is not merely a ‘ceaseless unrest’ but also a ‘quiescent result.’ Becoming as a ‘quiescent result’: being and nothing always vary but are bounded. The 2-D state-space of Hegel’s equations is a perfect circle. Hegel’s equations, that is the unity of being and nothing, trace a perfect circle in state-space. Becoming is indeed ceaseless unrest, but that unrest is always bounded. Pure being, which merely self-relates, explodes, and pure nothing, which also self-relates, implodes; in this sense, neither can exist. In contrast, Hegel’s equations define a stable dynamic system: their sublated unity neither explodes or implodes, but is a ‘quiescent result’ that reproduces itself indefinitely. In fact if we – somehow – managed to be outside observers and ‘measured’ the total substance of becoming we would notice no change whatsoever. Hegel’s equations, as we’ve seen, obey a conservation law. The ceaseless unrest on the inside conserves the total substance and so, on the outside, we would observe perfect calm, a truly quiescent result. So becoming both preserves its identity over time (conservation of substance) and changes (the internal oscillation). The unity of being and nothing determines a new kind of whole: a dynamic and contradictory unity. Pure being and nothing ‘sink from their initially represented self-subsistence’ and are turned into ‘moments’ of a bigger whole where they are ‘distinguished but at the same time sublated.’ Hegel resolves this paradox by a (logical? causal?) operator he calls sublation. Hegel remarks that sublation is ‘one of the most important notions in philosophy.’ A sublation, in typically Hegelian fashion, both preserves or maintains and puts an end to. Where did this operator come from? I think Hegel would argue that this operator is observable within the phenomenon itself. Becoming is the name Hegel gives to the sublation of pure being and pure nothing. Suddenly, everything changes: we ‘put an end to’ pure being and pure nothing as self-referential concepts (as uncoupled differential equations); and now they reciprocally refer to each other (as coupled differential equations). So each presupposes the other, and neither is a unique starting point. There cannot be being without nothing, or nothing without being. The logical paradox is resolved. In consequence, the unity of being and nothing determines a beginning that does make sense and can exist: the beginning is an irreducibly dynamic and contradictory unity. According to Hegel the fundamental structure of becoming must be present, as both a logical and natural necessity, in anything that exists at all. My mathematical interpretation covers only chapter 1 of Hegel’s monumental and obscure Science of Logic. In subsequent chapters, Hegel derives the necessary existence of further categories, such as quality, finitude, infinity, multiplicity, quantity, measure and the syllogisms of ‘ordinary’ logic. We should explore how far this new, mathematical interpretation of Hegel’s opening chapter extends to his later chapters. At some point, the semantics of Hegel’s metaphysical theory and the semantics of systems of differential equations must surely break down. But who knows? We might yield more insights into Hegel’s philosophy by pursuing this project. Regardless, Hegel – at least as far as he is concerned – derives and critiques the Kantian categories from his assumption free starting point, and, if this derivation is successful, then that would constitute evidence that fundamental aspects of our cognition are the manifestation of the contradiction between being and nothing. Those with a physics background will have already noticed that Hegel’s equations imply that the unity of being and nothing instantiates simple harmonic oscillators. Simple harmonic oscillators are the bread-and-butter of physics courses simply because harmonic oscillation is ubiquitous in nature, both in the microcosm (quantum) and the macrocosm (general relativity). As above, so below. Quantum field theory, the currently dominant theory of fundamental particles, is essentially simple harmonic motion taken to increasing levels of abstraction. In other words, simple harmonic motion is indeed a fundamental structure that appears, again and again, at all levels of physical reality. Hegel’s contradiction is not merely simple harmonic motion, but rather a 2-D, system of coupled harmonic oscillators with additional properties that relate to complex analysis and holomorphic functions. But here let’s simply note the following: it’s utterly remarkable that Hegel’s psychedelic, assumption-free starting point, which is resolutely conceptual and abstract – and makes no reference to physical reality or empirical knowledge whatsoever – nonetheless, according to the interpretation developed here, implies a structure of ‘becoming’ that is equivalent to the fundamental structure found everywhere in physical reality. Physicists might not ask, and perhaps could not answer, why oscillatory motion is ubiquitous in nature. Philosophy, in particular Hegel’s metaphysics, in contrast, provides a candidate explanation of this empirical phenomenon: according to Hegel, everything that exists necessarily is a unity of being and nothing and therefore – according to the mathematical interpretation developed here – must exhibit harmonic motion. Ian Wright "],["marxism.html", "7 Marxism 7.1 Transformation Problem", " 7 Marxism 7.1 Transformation Problem Ian Wright My paper, “A category mistake in the classical labour theory of value,” tackles Ricardo and Marx’s problematic in the context of a formal model of capitalist production. The formality is austere but has the advantage that it imparts precise semantics to some of the key concepts of the labour theory of value. This helps pinpoint a certain kind of logical error in the classical theory. Philosophers, such as Gilbert Ryle (1984 [1949]) and Ludwig Wittgenstein (1953), argue that the underlying cause of a long-lived and insoluble problem is often a hidden conceptual confusion or mistake. The problem is insoluble because the conceptual framework in which the problem is stated is itself faulty. Empirical study, or experimental activity, cannot resolve such problems. Rather, the problem must be deflated or dissolved by applying conceptual analysis. For instance, Ryle introduced the term “category-mistake” (Ryle 1984[1949], ch.1) to denote the conceptual error of expecting some concept or thing to possess properties it cannot have. For example, John Doe may be a relative, friend, enemy or stranger to Richard Roe; but he cannot be any of these things to the “Average Taxpayer.” So if “John Doe continues to think of the Average Taxpayer as a fellow-citizen, he will tend to think of him as an elusive an insubstantial man, a ghost who is everywhere yet nowhere” (Ryle 1984 [1949], p.18). In the paper, I argue that the contradictions of the classical labour theory of value derive from a “theoretically interesting category-mistake” (Ryle 1984 [1949], p.19), specifically the mistake of supposing that classical labour-values, which measure strictly technical costs of production, are of the same logical type as natural prices, which measure social costs of production, and in consequence labour-values and prices, under appropriate equilibrium conditions, are mutually consistent. Since this supposition is mistaken, Ricardo’s search for an invariable measure of value and Marx’s search for a conservative transformation attempt to discover a commensurate relationship between concepts defined by incommensurate cost accounting conventions. They therefore seek an impossible “elusive and insubstantial man” or “ghost.” Once the category mistake has been identified we can resolve the classical problems by “giving prominence to distinctions which our ordinary forms of language make us easily overlook” (Wittgenstein 1953, § 132). Such distinctions then solve, or more accurately, dissolve the problems. The key step is to notice that we can and should define a different measure of labour cost – total labour costs – that generalises the classical measure to include real costs induced by the institutional conditions of production. We then immediately possess a more general labour theory of value that includes both total and classical (i.e., technical) measures of labour cost. The general theory then applies the different measures in distinct, but complementary, theoretical roles, and in consequence separates issues normally conflated in the classical theories. The classical authors attempt to explain the structure of total costs of production – which include both technical costs due to the material conditions of production (e.g., the cost of physical capital and labour inputs) and additional social costs due to the institutional conditions of production (e.g., the cost of money-capital, state imposed taxes, etc.) – in terms of the structure of technical costs of production alone, which explicitly ignore institutional conditions. This conceptual error is the underlying cause of the almost two hundred year history of the “value controversy.” In the paper I explain why the more general theory has both an invariable measure of value and lacks a transformation problem. The main technical result is the theorem that natural prices are proportional to physical real costs of production measured in labour time. Hence, prices and labour costs, in appropriate equilibrium conditions, are “two sides of the same coin.” The measurement relation, missing from the classical theory, is therefore established, which implies that labour costs can in principle explain economic value. The more general theory therefore removes the primary theoretical obstacle that has hindered the development of the classical theory of value since its inception. Ian Wright "],["socialism.html", "8 Socialism", " 8 Socialism Contrary to conventional wisdom the defining characteristic of socialism is not the abolition of market relations and its replacement by centrally planned, top-down production. Economic planning has no bearing whatsoever on whether a set of social relations are exploitative or not. The essence of socialism is a hoped-for system of property relations, which we’ll call the “communal system.” In this system, the renting of people has been abolished (just as liberal democracy abolished the selling of people, i.e. slavery). People no longer are workers available to rent by the owners of firms. Instead, people are workers available to join as equal members of a democratic firm, who together lay claim on the residual income. A socialist firm is owned by its working members who hire-in capital at pre-agreed rental prices (compared to capitalism, the contracts are reversed). Capital, not labour, is now the ex ante cost of production. In consequence, the working members democratically distribute the firm’s residual income to themselves. Skip to content Home About Contact Twitter December 14, 2016 Ian Wright What’s this blog about? This blog is about getting from here to there … Where is here? “Here” is the social system we live in. We know we live in a capitalist system. But do we understand what this really means? Contrary to conventional wisdom the defining characteristic of capitalism is not market exchange. Market relations have existed since classical times. The essence of capitalism is a system of property relations, which I’ll call the “wage system.” Here, the capitalist firm hires-in labour at a pre-agreed rental price. The labour is mixed with other inputs and produces goods or services for sale in the market. Normally, firms sell at prices that exceed their costs of production, which includes the cost of used-up material inputs, rent, interest on capital loans, and labour costs etc. Here, labour is just another ex ante cost of production. Any remaining revenue — the residual income — then gets distributed as profits to the owners of the firm. What’s wrong with this? Essentially, the wage system is a theft-based system of property relations. The mere legal ownership of a firm is sufficient to lay claim on its residual income. The owner of a capitalist firm can, as John Stuart Mill, put it: “grow richer, as it were in their sleep.” Yet this residual income is the fruit of others’ labour. Taking resources from others, without giving anything back in exchange, is theft. This is why Marxists label capitalism an exploitative economic system. But wait. Isn’t profit a just reward for the risk of capital investment? Someone has to fund the firm. Surely owners deserve their returns too? Actually, no. There’s a big difference between advancing capital to a firm, and owning the firm. Let’s say you advance capital to a firm. You should expect repayment of your capital, plus a risk premium and collateral security, as a just exchange. But by lending capital you do not become an owner of the firm. Once your loan is repaid you have no further claims on the firm’s revenue. This contract is based on the principle of exchange: in essence, it allows the loaner and loanee to exchange the time when they consume a set of resources. There’s no theft here. But let’s say you want more. You advance capital to a firm, and in addition to the above, you expect joint ownership of the firm, i.e. equity capital. In this case you do become an owner of the firm. And so, once your initial loan and risk premium is repaid, you still retain a claim on the firm’s revenue. In fact you lay claim to the residual income, that is the fruits of others’ labour. This contract is not based on the principle of exchange. You now get to take resources from others, solely in virtue of the paper claim of holding “equity.” You do not have to give anything to the firm in return for your claim. The contract bestows the right to take wealth produced by others by fiat. This is theft. All ideological justifications of capitalism obscure this theft, and render it normal, almost entirely unnoticed, and socially acceptable. It’s largely an unquestioned and seemingly natural aspect of our economic relations. Capitalist property relations are not merely unjust, however. They are also the hidden and root cause of the major social ills of our day, in particular those caused by extreme income and wealth inequality. The capitalist firm minimises input costs, including wages, in order to maximise the residual income of the owners of the firm. This causes a two-class distribution of income and wealth, with a Pareto long tail of the super-rich. And it’s also the major cause of imperialism, which has a material foundation in the massive wealth disparities between countries. In consequence, capitalist property relations are also undesirable. Where is there? We might prefer to live in a system that avoids these widespread injustices and social ills. Traditionally this is the goal of socialism. But do we understand what socialism really means? Contrary to conventional wisdom the defining characteristic of socialism is not the abolition of market relations and its replacement by centrally planned, top-down production. Economic planning has no bearing whatsoever on whether a set of social relations are exploitative or not. The essence of socialism is a hoped-for system of property relations, which we’ll call the “communal system.” In this system, the renting of people has been abolished (just as liberal democracy abolished the selling of people, i.e. slavery). People no longer are workers available to rent by the owners of firms. Instead, people are workers available to join as equal members of a democratic firm, who together lay claim on the residual income. A socialist firm is owned by its working members who hire-in capital at pre-agreed rental prices (compared to capitalism, the contracts are reversed). Capital, not labour, is now the ex ante cost of production. In consequence, the working members democratically distribute the firm’s residual income to themselves. What’s right with this? The communal system is an inherently egalitarian system because all income from production is earned in essentially the same manner: by the contribution of labour. Absentee owners no longer lay claim to the fruit of others’ labour in virtue of a piece of paper (e.g. “equity”). The systematic economic theft, characteristic of capitalism, has been abolished. Socialism is not merely just, however. It also eradicates extreme income and wealth inequality for the simple reason that the entire working population earns the same kind of economic income, which is a kind of profit share. The split of the population into two main economic classes, that is workers (wage-earners) and capitalists (profits via ownership of firms), disappears along with the major social ills caused by extreme inequality. We need to formulate a political economy of socialism that simultaneously constitutes a political practice that crowds-out capitalist property relations. Ian Wright "],["anarchism.html", "9 Anarchism 9.1 Kropotkin’s evolutionary holism", " 9 Anarchism 9.1 Kropotkin’s evolutionary holism Sometimes—not very often—a particularly cogent argument against reigning political common sense presents such a shock to the system that it becomes necessary to create an entire body of theory to refute it. Such interventions are themselves events, in the philosophical sense; that is, they reveal aspects of reality that had been largely invisi-ble but, once revealed, seem so entirely obvious that they can never be unseen. Much of the work of the intellectual Right is identifying, and heading off, such challenges. Cooperation is just as decisive a factor in natural selection than competition. It is not love, and not even sympathy (understood in its proper sense) which induces a herd of ruminants or of horses to form a ring in order to resist an attack of wolves; not love which induces wolves to form a pack for hunting; not love which induces kittens or lambs to play, or a dozen of species of young birds to spend their days together in the autumn; and it is neither love nor personal sympathy which induces many thousand fallow-deer scattered over a territory as large as France to form into a score of separate herds, all marching towards a given spot, in order to cross there a river. It is a feeling infinitely wider than love or personal sympathy—an instinct that has been slowly developed among animals and men in the course of an extremely long evolution, and which has taught animals and men alike the force they can borrow from the practice of mutual aid and support, and the joys they can find in social life…. It is not love and not even sympathy upon which Society is based in mankind. It is the conscience—be it only at the stage of an instinct—of human solidarity. It is the unconscious recognition of the force that is borrowed by each man from the practice of mutual aid; of the close dependence of every one’s happiness upon the happiness of all; and of the sense of justice, or equity which brings the individual to consider the rights of every other individual as equal to his own. Upon this broad and necessary foundation the still higher moral feelings are developed. (Kropotkin: Mutual Aid) The only viable alternative to capitalist barbarism is stateless socialism, a product, as the great geographer never ceased to remind us, “of tendencies that are apparent now in the society” and that were “always, in some sense, imminent in the present.” To create a new world, we can only start by rediscovering what is and has always been right before our eyes. Andrej Grubacic &amp; David Graeber: Mutual Aid Kropotkin "],["history.html", "10 History 10.1 Personalities", " 10 History 10.1 Personalities 10.1.1 Jimmy Carter Bergman Carter, who lost his bid for re-election in a so-called landslide to Reagan in 1980, is often painted as a “failed president” – a hapless peanut farmer who did not understand how to get things done in Washington, and whose administration was marked by inflation, an energy crisis and the Iran hostage disaster. He was not in over his head or ineffective, weak or indecisive – he was a visionary leader, decades ahead of his time trying to pull the country toward renewable energy, climate solutions, social justice for women and minorities, equitable treatment for all nations of the world. He faced nearly impossible economic problems – and at the end of the day came so very close to changing the trajectory of this nation. He startled the globe by personally brokering the critical Middle East peace treaty between Anwar Sadat and Menachem Begin at Camp David. He ceded access to the Panama canal, angering conservatives who thought he was giving away an American asset. Through the Alaska Natural Interests Lands Conservation Act, he doubled the national park system and conserved over 100m acres of land – the most sweeping expansion of conserved land in American history. Carter was right on asking us to drive less, to reduce our dependence on foreign oil, to focus on conservation and renewable energy. Not only was Carter’s vision a path not taken, it was a path mocked. Reagan removed the solar panels from the White House, politicized the environmental movement and painted it as a fringe endeavor. Carter was a trained engineer who believed in science. He understood things on a global scale, and believed in forecasting. Preparing for the long run is rare in politics. Bergman on Carter (Guardian) "],["asteroids.html", "11 Asteroids", " 11 Asteroids Nasa has given Earth the all clear on the chances of an asteroid called Apophis hitting our planet any time in the next century, having worried space scientists for over 15 years. The 340-metre (1,100ft) chunk of space rock hit the headlines in 2004 after its discovery led to some worrying forecasts about its orbit. It became a “poster child for hazardous asteroids,” according to one Nasa expert. It was supposed to come frighteningly close in 2029 and again in 2036. Nasa ruled out any chance of a strike during those two close approaches a while ago, but a potential 2068 collision still loomed. But new telescope observations mean that collision has been ruled out and Apophis has been officially taken off the US space agency’s asteroid “risk list.” Apophis will come within 32,000km (20,000 miles) of Earth on Friday 13 April 2029, enabling astronomers to get a good look. That is about one-tenth of the distance to the moon and closer than the communication satellites that encircle the Earth at 36,000km. Although most asteroids are found in the belt of space between Mars and Jupiter, not all of them reside there. Apophis belongs to a group known as the Aten family. These do not belong to the asteroid belt and spend most of their time inside the orbit of the Earth, placing them between our planet and the sun. That makes them particularly dangerous because they spend the majority of their orbit close to the sun, whose overwhelming glare obscures them to telescopes on Earth – rather like a second world war fighter ace approaching out of the sun. Apophis (The Guardian) "],["antifragile-therapy.html", "12 Antifragile Therapy 12.1 Optimal Cancer Treatment", " 12 Antifragile Therapy 12.1 Optimal Cancer Treatment Abstract West Antifragility is a recently coined word using to describe the opposite of fragility. Sys- tems or organisms can be described as antifragile if they derive a benefit from systemic variability, volatility, randomness, or disorder. Herein, we introduce a mathematical framework to quantify the fragility or antifragility of cancer cell lines in response to treatment variability. This framework enables straightforward prediction of the optimal dose treatment schedule for a range of treatment schedules with identical cu- mulative dose. We apply this framework to non-small-cell lung cancer cell lines with evolved resistance to ten anti-cancer drugs. We show the utility of this antifragile framework when applied to 1) treatment resistance, 2) collateral sensitivity of sequen- tial monotherapies, and 3) combination therapies. Over the past decades it has become increasingly clear that the benefit of a cancer therapeutic agent is determined not only by its molecular action but also by its schedule. However, because of the costs associated with clinical trials and the combinatorial size of the potential search space, optimal treatment strategies remain elusive. As a result, most therapies are administered in a fashion to maximize cell kill, meaning they are given as frequently as is logistically feasible (weekly for chemotherapies, daily for orally available targeted therapies) and at the maximum dose patients can safely tolerate. At the same time, translating into the clinic alternative schedules which have been shown to perform better in vitro, in vivo, and/or in silico has been challenging, and has failed on several occasions. For example, even though “bolus-dosing” of EGFR inhibitor for EGFR-Mutant NSCLC, in which daily low dose treatment is supplemented with a weekly high dose of therapy, was shown to better control therapy resistance than the standard-of-care continuous schedule in a mathematical model 27 , as well as in in vitro 27 and in in vivo experiments 28 , it failed to do so in patients 37 . One reason for this discrepancy is the fact that it is often difficult to understand why a given schedule is optimal. In this paper, we have shown how that the theory of antifragility, pioneered in financial risk management, provides a general tool to compare schedules in an intuitive yet formal fashion. In particular, we have demonstrated that the curvature of the dose response curve determines whether regimens should seek to maintain a constant treatment level, or should induce fluctuations between high and low periods of exposure. Importantly, this assessment can be made graphically and does not require specialist knowledge of complex optimization techniques. Moreover, it is easily generalizable as it can be applied to dose response curves obtained from any experimental or theoretical model system. West (2020) Antifragile Therapy (bioRxiv) (pdf) "],["consciousness.html", "13 Consciousness", " 13 Consciousness The Economist IN AN IDEAL world, science would work by making unambiguous predictions based on a theory, and then testing those predictions in ways that leave no wiggle room about which are right and which wrong. In practice, it rarely happens quite like that, especially in biology. But, the coronavirus always permitting, a group of neuroscientists plan to apply this method over the course of the coming year to the most mysterious biological phenomenon of all: human consciousness. They are organising what is known as an “adversarial collab­oration competition” between two hypotheses about how consciousness is generated in brains. The contestants are Giulio Tononi’s integrated information theory (IIT) and Stanislas Dehaene’s global workspace theory (GWT). The competition was dreamed up at the Allen Institute for Brain Science, in Seattle, and is being paid for by the Templeton World Charity Foundation. The practical side of things is being led by Lucia Melloni of the Max Planck Institute for Empirical Aesthetics, in Frankfurt. Dr Tononi, of the University of Wisconsin, Madison, thinks consciousness is a direct consequence of the interconnectedness of neurons within brains. IIT argues that the more the neurons in a being’s brain interact with one another, and the more complex the resulting network is, the more the being in question feels itself to be conscious. Because the parts of a human brain where neuronal connectivity is most complex are the sensoryprocessing areas (in particular, the visual cortex) at the back of the organ, these, IIT predicts, are where human consciousness will be seated. Dr Dehaene, who works at the Collège de France, in Paris, reckons by contrast that the action, when it comes to consciousness, involves a network of brain areas—particularly the prefrontal cortex. This part of the brain receives sensory information from elsewhere in the organ, evaluates and edits it, and then sends the edited version out to other areas, to be acted on. It is the activity of evaluating, editing and broadcasting which, according to GWT, generates feelings of consciousness. One difference between IIT and GWT, accordingly, is that the former is a “bottom up” explanation, whereas the latter is “top down.” Supporters of IIT think consciousness is an emergent property of neural complexity that can exist to different degrees, and could, in principle, be measured as a number (for which they use the Greek letter phi). GWT-type consciousness, by contrast, is more of an all-or-nothing affair. Distinguishing between the two would be a big step forward for science. It would also have implications for how easy it might be to build a computer that was conscious. The competition’s experiments will be conducted on 500 volunteers at six sites in America, Britain, China and the Netherlands. Three techniques will be used: functional magnetic-resonance imaging (fMRI), magnetoencephalography (MEG) and electrocorticography (ECoG). fMRI measures blood flow, which in turn relates to the level of activity in the part of the brain being examined (the more blood that is flowing through an area, the more active it is). MEG records fluctuating magnetic fields produced by electrical activity in the brain. Neither of these is intrusive. ECoG, however, records electrical activity directly from the surface of the cerebral cortex. This part of the project will therefore rely on volunteers who are undergoing brain surgery for reasons, such as to treat epilepsy, which require the patient to remain conscious throughout the procedure. Half the data collected will be analysed immediately, by researchers independent of the protagonists, who have no axe to grind for either side. The other half will be locked away for future reference, in case confirmatory analyses need to be done. In the spirit of adversarial collaboration, the two sides have hammered out a set of tests that both agree should produce different results, depending on which theory is correct. These depend on the fact that GWT predicts brain activity only when attention is actively being paid to something, whereas mere conscious awareness of something is enough for IIT to predict activity. The tests’ details vary (some involve stationary letters, objects or faces on a screen while others have shapes moving across the screen). In all of them, though, the distinction between attention and awareness is clear—and so, therefore, are the predictions. Whatever emerges from the experiment will not be anywhere near a definitive explanation of consciousness. In particular, it will not address the “hard” problem of the phenomenon: the “feeling of what it is like to be something” that was raised in 1974 by Thomas Nagel, an American philosopher, in an essay titled “What is it like to be a bat?” It will, however, by providing what are known as neural correlates of conscious experience, point to directions in which future investigations might usefully travel. Geoffrey Carr, The Economist (Nov 17th 2020) Memo Burkeman The feeling of being inside your head, looking out, or of having a soul. One spring morning in Tucson, Arizona, in 1994, an unknown philosopher named David Chalmers got up to give a talk on consciousness, by which he meant the feeling of being inside your head, looking out – or, to use the kind of language that might give a neuroscientist an aneurysm, of having a soul. Though he didn’t realise it at the time, the young Australian academic was about to ignite a war between philosophers and scientists, by drawing attention to a central mystery of human life – perhaps the central mystery of human life – and revealing how embarrassingly far they were from solving it. Hard Problem of Consciousness – and it’s this: why on earth should all those complicated brain processes feel like anything from the inside? Why aren’t we just brilliant robots, capable of retaining information, of responding to noises and smells and hot saucepans, but dark inside, lacking an inner life? Philosophers had pondered the so-called “mind-body problem” for centuries. What the hell is this that we’re dealing with here? Conscious sensations, such as pain, don’t really exist, no matter what I felt as I hopped in anguish around the kitchen; or, alternatively, that plants and trees must also be conscious. In recent years, a handful of neuroscientists have come to believe that it may finally be about to be solved – but only if we are willing to accept the profoundly unsettling conclusion that computers or the internet might soon become conscious, too. Cartesian dualism Science had been vigorously attempting to ignore the problem of consciousness for a long time. The source of the animosity dates back to the 1600s, when René Descartes identified the dilemma that would tie scholars in knots for years to come. On the one hand, Descartes realised, nothing is more obvious and undeniable than the fact that you’re conscious. In theory, everything else you think you know about the world could be an elaborate illusion cooked up to deceive you – at this point, present-day writers invariably invoke The Matrix – but your consciousness itself can’t be illusory. On the other hand, this most certain and familiar of phenomena obeys none of the usual rules of science. It doesn’t seem to be physical. It can’t be observed, except from within, by the conscious person. It can’t even really be described. The mind, Descartes concluded, must be made of some special, immaterial stuff that didn’t abide by the laws of nature; it had been bequeathed to us by God. Cartesian dualism, remained the governing assumption into the 18th century and the early days of modern brain study. But it was always bound to grow unacceptable to an increasingly secular scientific establishment that took physicalism – the position that only physical things exist – as its most basic principle. And yet, even as neuroscience gathered pace in the 20th century, no convincing alternative explanation was forthcoming. So little by little, the topic became taboo. Few people doubted that the brain and mind were very closely linked: if you question this, try stabbing your brain repeatedly with a kitchen knife, and see what happens to your consciousness. But how they were linked – or if they were somehow exactly the same thing – seemed a mystery best left to philosophers in their armchairs. As late as 1989, writing in the International Dictionary of Psychology, the British psychologist Stuart Sutherland could irascibly declare of consciousness that “it is impossible to specify what it is, what it does, or why it evolved. Nothing worth reading has been written on it.” It was only in 1990 that Francis Crick, the joint discoverer of the double helix, used his position of eminence to break ranks. Neuroscience was far enough along by now, he declared in a slightly tetchy paper co-written with Christof Koch, that consciousness could no longer be ignored. “It is remarkable,” they began, “that most of the work in both cognitive science and the neurosciences makes no reference to consciousness” – partly, they suspected, “because most workers in these areas cannot see any useful way of approaching the problem.” They presented their own “sketch of a theory,” arguing that certain neurons, firing at certain frequencies, might somehow be the cause of our inner awareness – though it was not clear how. Consciousness can’t just be made of ordinary physical atoms. So consciousness must, somehow, be something extra – an additional ingredient in nature. It may be true that most of us, in our daily lives, think of consciousness as something over and above our physical being – as if your mind were “a chauffeur inside your own body,” to quote the spiritual author Alan Watts. But to accept this as a scientific principle would mean rewriting the laws of physics. Everything we know about the universe tells us that reality consists only of physical things. If this non-physical mental stuff did exist, how could it cause physical things to happen – as when the feeling of pain causes me to jerk my fingers away from the saucepan’s edge? Science has dropped tantalising hints that this spooky extra ingredient might be real. Daniel Dennett, the high-profile atheist and professor at Tufts University outside Boston, argues that consciousness, as we think of it, is an illusion: there just isn’t anything in addition to the spongy stuff of the brain, and that spongy stuff doesn’t actually give rise to something called consciousness. Common sense may tell us there’s a subjective world of inner experience – but then common sense told us that the sun orbits the Earth, and that the world was flat. Consciousness, according to Dennett’s theory, is like a conjuring trick: the normal functioning of the brain just makes it look as if there is something non-physical going on. To look for a real, substantive thing called consciousness, Dennett argues, is as silly as insisting that characters in novels, such as Sherlock Holmes or Harry Potter, must be made up of a peculiar substance named “fictoplasm”; the idea is absurd and unnecessary, since the characters do not exist to begin with. To Dennett’s opponents, he is simply denying the existence of something everyone knows for certain. The Hard Problem is nonsense, kept alive by philosophers who fear that science might be about to eliminate one of the puzzles that has kept them gainfully employed for years. Life is just the label we give to certain kinds of objects that can grow and reproduce. Eventually, neuroscience will show that consciousness is just brain states. Solutions have regularly been floated: the literature is awash in references to “global workspace theory,” “ego tunnels,” “microtubules,” and speculation that quantum theory may provide a way forward. But the intractability of the arguments has caused some thinkers, such as Colin McGinn, to raise an intriguing if ultimately defeatist possibility: what if we’re just constitutionally incapable of ever solving the Hard Problem? After all, our brains evolved to help us solve down-to-earth problems of survival and reproduction; there is no particular reason to assume they should be capable of cracking every big philosophical puzzle. There’s actually no mystery to why consciousness hasn’t been explained: it’s that humans aren’t up to the job. Panpsychism “Panpsychism,” the dizzying notion that everything in the universe might be conscious, or at least potentially conscious, or conscious when put into certain configurations. If humans have it, and apes have it, and dogs and pigs probably have it, and maybe birds, too – well, where does it stop? Physicists have no problem accepting that certain fundamental aspects of reality – such as space, mass, or electrical charge – just do exist. They can’t be explained as being the result of anything else. Explanations have to stop somewhere. The panpsychist hunch is that consciousness could be like that, too – and that if it is, there is no particular reason to assume that it only occurs in certain kinds of matter. Anything at all could be conscious, providing that the information it contains is sufficiently interconnected and organised. The human brain certainly fits the bill; so do the brains of cats and dogs, though their consciousness probably doesn’t resemble ours. But in principle the same might apply to the internet, or a smartphone, or a thermostat. Integrated information theory Unlike the vast majority of musings on the Hard Problem, moreover, Tononi and Koch’s “integrated information theory” has actually been tested. A team of researchers led by Tononi has designed a device that stimulates the brain with electrical voltage, to measure how interconnected and organised – how “integrated” – its neural circuits are. Sure enough, when people fall into a deep sleep, or receive an injection of anaesthetic, as they slip into unconsciousness, the device demonstrates that their brain integration declines, too. Among patients suffering “locked-in syndrome” – who are as conscious as the rest of us – levels of brain integration remain high; among patients in coma – who aren’t – it doesn’t. Gather enough of this kind of evidence, Koch argues and in theory you could take any device, measure the complexity of the information contained in it, then deduce whether or not it was conscious. But even if one were willing to accept the perplexing claim that a smartphone could be conscious, could you ever know that it was true? Surely only the smartphone itself could ever know that? Koch shrugged. “It’s like black holes,” he said. “I’ve never been in a black hole. Personally, I have no experience of black holes. But the theory [that predicts black holes] seems always to be true, so I tend to accept it.” It would be poetic – albeit deeply frustrating – were it ultimately to prove that the one thing the human mind is incapable of comprehending is itself. Burkeman: Mystery of Consciousness (Guardian 2015) "],["crime.html", "14 Crime 14.1 Free Will", " 14 Crime In the 1680s, a Huron-Wendat statesman named Kondiaronk, who had been to Europe and was intimately familiar with French and English settler society, engaged in a series of debates with the French governor of Quebec, and one of his chief aides, a certain Lahontan. In them he presented the argument that punitive law and the whole appa-ratus of the state exist not because of some fundamental flaw in human nature but owing to the existence of another set of institutions—private property, money—that by their very nature drive people to act in such ways as to make coercive measures necessary. Mutual Aid Kropotkin 14.1 Free Will Memo Burkeman A growing chorus of scientists and philosophers argue that free will does not exist. They argue that our choices are determined by forces beyond our ultimate control – perhaps even predetermined all the way back to the big bang – and that therefore nobody is ever wholly responsible for their actions. The experience of possessing free will – the feeling that we are the authors of our choices – is so utterly basic to everyone’s existence that it can be hard to get enough mental distance to see what’s going on. Suppose you find yourself feeling moderately hungry one afternoon, so you walk to the fruit bowl in your kitchen, where you see one apple and one banana. As it happens, you choose the banana. But it seems absolutely obvious that you were free to choose the apple – or neither, or both – instead. That’s free will: were you to rewind the tape of world history, to the instant just before you made your decision, with everything in the universe exactly the same, you’d have been able to make a different one. “This sort of free will is ruled out, simply and decisively, by the laws of physics,” says one of the most strident of the free will sceptics, the evolutionary biologist Jerry Coyne. Leading psychologists such as Steven Pinker and Paul Bloom agree, as apparently did the late Stephen Hawking, along with numerous prominent neuroscientists, including VS Ramachandran, who called free will “an inherently flawed and incoherent concept.” Arguments against free will go back millennia, but the latest resurgence of scepticism has been driven by advances in neuroscience during the past few decades. Now that it’s possible to observe – thanks to neuroimaging – the physical brain activity associated with our decisions, it’s easier to think of those decisions as just another part of the mechanics of the material universe, in which “free will” plays no role. And from the 1980s onwards, various specific neuroscientific findings have offered troubling clues that our so-called free choices might actually originate in our brains several milliseconds, or even much longer, before we’re first aware of even thinking of them. We would be forced to conclude that it was unreasonable ever to praise or blame anyone for their actions, since they weren’t truly responsible for deciding to do them “Illusionism,” the idea that although free will as conventionally defined is unreal, it’s crucial people go on believing otherwise Start with what seems like an obvious truth: anything that happens in the world, ever, must have been completely caused by things that happened before it. And those things must have been caused by things that happened before them – and so on, backwards to the dawn of time: cause after cause after cause, all of them following the predictable laws of nature, even if we haven’t figured all of those laws out yet. It’s easy enough to grasp this in the context of the straightforwardly physical world of rocks and rivers and internal combustion engines. But surely “one thing leads to another” in the world of decisions and intentions, too. Our decisions and intentions involve neural activity – and why would a neuron be exempt from the laws of physics any more than a rock? To have what’s known in the scholarly jargon as “contra-causal” free will – so that if you rewound the tape of history back to the moment of choice, you could make a different choice – you’d somehow have to slip outside physical reality. You just are some of the atoms in the universe, governed by the same predictable laws as all the rest. It was the French polymath Pierre-Simon Laplace, writing in 1814, who most succinctly expressed the puzzle here: how can there be free will, in a universe where events just crank forwards like clockwork? His thought experiment is known as Laplace’s demon, and his argument went as follows: if some hypothetical ultra-intelligent being – or demon – could somehow know the position of every atom in the universe at a single point in time, along with all the laws that governed their interactions, it could predict the future in its entirety. It’s true that since Laplace’s day, findings in quantum physics have indicated that some events, at the level of atoms and electrons, are genuinely random, which means they would be impossible to predict in advance, even by some hypothetical megabrain. But few people involved in the free will debate think that makes a critical difference. Those tiny fluctuations probably have little relevant impact on life at the scale we live it, as human beings. And in any case, there’s no more freedom in being subject to the random behaviours of electrons than there is in being the slave of predetermined causal laws. By far the most unsettling implication of the case against free will, for most who encounter it, is what it seems to say about morality: that nobody, ever, truly deserves reward or punishment for what they do, because what they do is the result of blind deterministic forces (plus maybe a little quantum randomness). Consider the case of Charles Whitman. Just after midnight on 1 August 1966, Whitman – an outgoing and apparently stable 25-year-old former US Marine – drove to his mother’s apartment in Austin, Texas, where he stabbed her to death. He returned home, where he killed his wife in the same manner. Later that day, he took an assortment of weapons to the top of a high building on the campus of the University of Texas, where he began shooting randomly for about an hour and a half. By the time Whitman was killed by police, 12 more people were dead, and one more died of his injuries years afterwards – a spree that remains the US’s 10th worst mass shooting. Within hours of the massacre, the authorities discovered a note that Whitman had typed the night before. “I don’t quite understand what compels me to type this letter,” he wrote. “Perhaps it is to leave some vague reason for the actions I have recently performed. I don’t really understand myself these days. I am supposed to be an average reasonable and intelligent young man. However, lately (I can’t recall when it started) I have been a victim of many unusual and irrational thoughts [which] constantly recur, and it requires a tremendous mental effort to concentrate on useful and progressive tasks … After my death I wish that an autopsy would be performed to see if there is any visible physical disorder.” Following the first two murders, he added a coda: “Maybe research can prevent further tragedies of this type.” An autopsy was performed, revealing the presence of a substantial brain tumour, pressing on Whitman’s amygdala, the part of the brain governing “fight or flight” responses to fear. It’s impossible to know if the brain tumour caused Whitman’s actions. What seems clear is that it certainly could have done so – and that almost everyone, on hearing about it, undergoes some shift in their attitude towards him. If you find the presence of a brain tumour in these cases in any way exculpatory, though, you face a difficult question: what’s so special about a brain tumour, as opposed to all the other ways in which people’s brains cause them to do things? A neurological disorder appears to be just a special case of physical events giving rise to thoughts and actions. What all this means is that retributive punishment – punishing a criminal because he deserves it, rather than to protect the public, or serve as a warning to others – can’t ever be justified. Retribution is central to all modern systems of criminal justice, yet ultimately it’s a moral injustice to hold someone responsible for actions that are beyond their control. It’s capricious. The “public health-quarantine” model of criminal justice would transform the institutions of punishment in a radically humane direction. You could still restrain a murderer, on the same rationale that you can require someone infected by Ebola to observe a quarantine: to protect the public. But you’d have no right to make the experience any more unpleasant than was strictly necessary for public protection. And you would be obliged to release them as soon as they no longer posed a threat. The main focus would be on redressing social problems to try stop crime happening in the first place – just as public health systems ought to focus on preventing epidemics happening to begin with. Burkeman: Is free will an illusion? (Guardian 2021) "],["energy-empires.html", "15 Energy Empires", " 15 Energy Empires Fix The history of empires’ is not written in the speeches and proclamations of elites. Instead, it’s written in the language of energy. Although the motivations for empire building differ between societies, the end result is always the same. A successful empire centralizes the flow of energy. This means that energy use (per person) in the empire’s core will dwarf energy use in the periphery. The degree that this is true marks the degree that the empire is successful. Energy use, then, provides a window into the rise and fall of empires. How can we estimate the energy use of early empires? We make an educated guess. That’s exactly what Ian Morris does in his book The Measure of Civilization. Keep in mind that Morris’ data is less of a measurement and more of a back-of-the-envelope guess. In his book Ages of Discord, Peter Turchin analyzes the long-waves of American social stability. His waves match (at least roughly) what’s written in the energy data. Turchin finds that social stability peaked after independence, but reached a trough by the Civil War. Stability rose again during the good times after World War II. But today, social stability is on the decline. After the quagmire of civil war, American fortunes rose again. By the turn of the 20th century, the American empire was in full swing. As revealed by energy consumption, the ‘American century’ lasted roughly 70 years (1900–1970). During this time, the typical American consumed about 6 times more energy than the world average. The peak of US supremacy came during World War II. At the height of its war machine, the US consumed roughly one third of the world’s energy. The end of US dominance came around 1970. The decline was halted, briefly, during the boom of the 1990s. But then the dotcom bubble burst, and the energy slide continued. The pinnacle of US empire has long passed. During the Middle Ages, China was the center of world civilization. But with the European renaissance, that would change. In the 19th century, Europe colonized China (although never as completely as it colonized neighboring India). Colonization eventually ended in the 20th century with the Chinese communist revolution. But this revolution didn’t end Chinese suffering. If anything, it exacerbated it. Still, China eventually emerged as an industrial power. In the last 40 years, its transformation has been remarkable. And the whole story is written in the ‘book of energy.’ When Mao died, the Chinese government abandoned hardline command and control. Instead, it let (big) firms take some of the economic reigns. After this reform, the growth of Chinese energy use was spectacular. Chinese energy consumption has only recently surpassed the world average. Today, the typical Chinese citizen consumes about 1.3 times more energy than the world average. While China is on the rise, it has yet to catch up to the fading imperial power, the United States. Today, the typical American consumes about 3.8 times more energy than the world average. Although the US is in relative decline, it’s still a far wealthier nation (on average) than China. China is marked by a great urban-rural divide. In 2012, Chinese urbanites earned about 300% more than their rural counterparts. In contrast, American urbanites earn only 36% more than their rural counterparts.4 This divide means that speaking about ‘average Chinese energy use’ is misleading. Some Chinese live much like modern Americans. Others live more like Americans of the 18th century. In the language of world-systems theory, we’d say that China ‘includes its own periphery.’ Blair Fix: Why America won’t be great again Thomson Nazi Energy Empire Energy has been central to geopolitics since the industrial revolution. Oil-fuelled American industrialisation induced frenzied fears in Europe. From the 1870s, the conjunction of American oil resources allied to a continental single market and a high-tariff customs union drove a European scramble for energy resources and land empires in Africa. After the discovery of oil in the Middle East at a time when the US was moving to an oil-based navy, Britain, France, and Germany competed over the crumbling Ottoman empire. Britain’s victory in that competition – helped by its ability to project land power from India into the Persian Gulf – left it with a Middle Eastern empire that was a pivotal feature of the Eurasian map until the late 1960s. Germany’s defeat was central to the Nazis’ expansionist ambitions. From the mid-1920s, Hitler became obsessed with American mass car society and the domestic oil supply that fuelled it. In retaining Ford and General Motors’ presence in Germany, Nazi industrial strategy used short-term technological dependency on the United States in order to bring American-level mass automated production to Germany. In Hitler’s geopolitical fantasy, the autobahns German producers had learned to build would stretch out across Eurasia under German control. During the Second World War, he conceived Operation Barbarossa as the German equivalent of the conquest of the American West, since Russia was where on Europe’s immediate borders there were large quantities of oil. Thompson "],["fertility.html", "16 Fertility", " 16 Fertility Abstract Roser The global average fertility rate is just below 2.5 children per woman today. Over the last 50 years the global fertility rate has halved. And over the course of the modernization of societies the number of children per woman decreases very substantially. In the pre-modern era of 4.5 to 7 children per woman were common. At that time the very high mortality at a young age kept population growth low. As health improves and the mortality in the population decreases we typically saw accelerated population growth. This rapid population growth then comes to an end as the fertility rate declines and approaches 2 children per woman. Max Roser: Fertility Rate "],["mathematics.html", "17 Mathematics 17.1 Riemann Zeta Function", " 17 Mathematics 17.1 Riemann Zeta Function Ian Wright The relation between Riemann zeroes and primes is provable but mysterious. A bridge from perfect continuity to perfect discreteness. Excitement here about a new interpretation of zeroes. I’ll attempt to understand it, but I’m a relative amateur and this work is highly technical. Baez Pólya suggested that the nontrivial zeros of the Riemann zeta function should correspond to the eigenvalues of some interesting self-adjoint operator. Since then there’s been a lot of work looking for this operator. This might give a physics-inspired proof of the RH Baez Twitter Thread "],["pandemics.html", "18 Pandemics 18.1 Origins of SARS2 18.2 How the West lost Covid 18.3 Pantent Protection Waiver", " 18 Pandemics 18.1 Origins of SARS2 Wade Virologists like Daszak had much at stake in the assigning of blame for the pandemic. For 20 years, mostly beneath the public’s attention, they had been playing a dangerous game. In their laboratories they routinely created viruses more dangerous than those that exist in nature. They argued that they could do so safely, and that by getting ahead of nature they could predict and prevent natural “spillovers,” the cross-over of viruses from an animal host to people. If SARS2 had indeed escaped from such a laboratory experiment, a savage blowback could be expected, and the storm of public indignation would affect virologists everywhere, not just in China. “It would shatter the scientific edifice top to bottom,” Some older methods of cutting and pasting viral genomes retain tell-tale signs of manipulation. But newer methods, called “no-see-um” or “seamless” approaches, leave no defining marks. Nor do other methods for manipulating viruses such as serial passage, the repeated transfer of viruses from one culture of cells to another. If a virus has been manipulated, whether with a seamless method or by serial passage, there is no way of knowing that this is the case. Both the SARS1 and MERS viruses had left copious traces in the environment. The intermediary host species of SARS1 was identified within four months of the epidemic’s outbreak, and the host of MERS within nine months. Yet some 15 months after the SARS2 pandemic began, and after a presumably intensive search, Chinese researchers had failed to find either the original bat population, or the intermediate species to which SARS2 might have jumped, or any serological evidence that any Chinese population, including that of Wuhan, had ever been exposed to the virus prior to December 2019. Natural emergence remained a conjecture which, however plausible to begin with, had gained not a shred of supporting evidence in over a year. As long as that remains the case, it’s logical to pay serious attention to the alternative conjecture, that SARS2 escaped from a lab. Why would anyone want to create a novel virus capable of causing a pandemic? Ever since virologists gained the tools for manipulating a virus’s genes, they have argued they could get ahead of a potential pandemic by exploring how close a given animal virus might be to making the jump to humans. And that justified lab experiments in enhancing the ability of dangerous animal viruses to infect people, virologists asserted. With this rationale, they have recreated the 1918 flu virus, shown how the almost extinct polio virus can be synthesized from its published DNA sequence, and introduced a smallpox gene into a related virus. These enhancements of viral capabilities are known blandly as gain-of-function experiments. With coronaviruses, there was particular interest in the spike proteins, which jut out all around the spherical surface of the virus and pretty much determine which species of animal it will target. In 2000 Dutch researchers, for instance, earned the gratitude of rodents everywhere by genetically engineering the spike protein of a mouse coronavirus so that it would attack only cats. Virologists started studying bat coronaviruses in earnest after these turned out to be the source of both the SARS1 and MERS epidemics. In particular, researchers wanted to understand what changes needed to occur in a bat virus’s spike proteins before it could infect people. Researchers at the Wuhan Institute of Virology, led by China’s leading expert on bat viruses, Shi Zheng-li or “Bat Lady,” mounted frequent expeditions to the bat-infested caves of Yunnan in southern China and collected around a hundred different bat coronaviruses. Shi then teamed up with Ralph S. Baric, an eminent coronavirus researcher at the University of North Carolina. Their work focused on enhancing the ability of bat viruses to attack humans so as to “examine the emergence potential (that is, the potential to infect humans) of circulating bat CoVs [coronaviruses].” In pursuit of this aim, in November 2015 they created a novel virus by taking the backbone of the SARS1 virus and replacing its spike protein with one from a bat virus (known as SHC014-CoV). This manufactured virus was able to infect the cells of the human airway, at least when tested against a lab culture of such cells. The SHC014-CoV/SARS1 virus is known as a chimera because its genome contains genetic material from two strains of virus. If the SARS2 virus were to have been cooked up in Shi’s lab, then its direct prototype would have been the SHC014-CoV/SARS1 chimera, the potential danger of which concerned many observers and prompted intense discussion. “If the virus escaped, nobody could predict the trajectory,” said Simon Wain-Hobson, a virologist at the Pasteur Institute in Paris. Baric and Shi referred to the obvious risks in their paper but argued they should be weighed against the benefit of foreshadowing future spillovers. Scientific review panels, they wrote, “may deem similar studies building chimeric viruses based on circulating strains too risky to pursue.” Given various restrictions being placed on gain-of function (GOF) research, matters had arrived in their view at “a crossroads of GOF research concerns; the potential to prepare for and mitigate future outbreaks must be weighed against the risk of creating more dangerous pathogens. In developing policies moving forward, it is important to consider the value of the data generated by these studies and whether these types of chimeric virus studies warrant further investigation versus the inherent risks involved.” That statement was made in 2015. From the hindsight of 2021, one can say that the value of gain-of-function studies in preventing the SARS2 epidemic was zero. The risk was catastrophic, if indeed the SARS2 virus was generated in a gain-of-function experiment. Shi set out to create novel coronaviruses with the highest possible infectivity for human cells. Her plan was to take genes that coded for spike proteins possessing a variety of measured affinities for human cells, ranging from high to low. She would insert these spike genes one by one into the backbone of a number of viral genomes (“reverse genetics” and “infectious clone technology”), creating a series of chimeric viruses. These chimeric viruses would then be tested for their ability to attack human cell cultures (“in vitro”) and humanized mice (“in vivo”). And this information would help predict the likelihood of “spillover,” the jump of a coronavirus from bats to people. It cannot yet be stated that Shi did or did not generate SARS2 in her lab because her records have been sealed, but it seems she was certainly on the right track to have done so. On December 9, 2019, before the outbreak of the pandemic became generally known, Daszak gave an interview in which he talked in glowing terms of how researchers at the Wuhan Institute of Virology had been reprogramming the spike protein and generating chimeric coronaviruses capable of infecting humanized mice. “And we have now found, you know, after 6 or 7 years of doing this, over 100 new SARS-related coronaviruses, very close to SARS,” Daszak says around minute 28 of the interview. “Some of them get into human cells in the lab, some of them can cause SARS disease in humanized mice models and are untreatable with therapeutic monoclonals and you can’t vaccinate against them with a vaccine. So, these are a clear and present danger…. Once you have generated a novel coronavirus that can attack human cells, you can take the spike protein and make it the basis for a vaccine. One reason for SARS1 being so hard to handle is that there were no vaccines available to protect laboratory workers. The long history of viruses escaping from even the best run laboratories. The smallpox virus escaped three times from labs in England in the 1960’s and 1970’s, causing 80 cases and 3 deaths. Dangerous viruses have leaked out of labs almost every year since. Coming to more recent times, the SARS1 virus has proved a true escape artist, leaking from laboratories in Singapore, Taiwan, and no less than four times from the Chinese National Institute of Virology in Beijing. The two closest known relatives of the SARS2 virus were collected from bats living in caves in Yunnan, a province of southern China. If the SARS2 virus had first infected people living around the Yunnan caves, that would strongly support the idea that the virus had spilled over to people naturally. But this isn’t what happened. The pandemic broke out 1,500 kilometers away, in Wuhan. The bats’ range is 50 kilometers, so it’s unlikely that any made it to Wuhan. For the lab escape scenario, a Wuhan origin for the virus is a no-brainer. Wuhan is home to China’s leading center of coronavirus research where, as noted above, researchers were genetically engineering bat coronaviruses to attack human cells. They were doing so under the minimal safety conditions of a BSL2 lab. If a virus with the unexpected infectiousness of SARS2 had been generated there, its escape would be no surprise. Researchers have documented the successive changes in its spike protein as the virus evolved step by step into a dangerous pathogen. After it had gotten from bats into civets, there were six further changes in its spike protein before it became a mild pathogen in people. After a further 14 changes, the virus was much better adapted to humans, and with a further four, the epidemic took off. But when you look for the fingerprints of a similar transition in SARS2, a strange surprise awaits. The virus has changed hardly at all, at least until recently. From its very first appearance, it was well adapted to human cells. For those who think SARS2 may have escaped from a lab, explaining the furin cleavage site is no problem at all. “Since 1992 the virology community has known that the one sure way to make a virus deadlier is to give it a furin cleavage site at the S1/S2 junction in the laboratory,” writes Steven Quay, a biotech entrepreneur interested in the origins of SARS2. “At least 11 gain-of-function experiments, adding a furin site to make a virus more infective, are published in the open literature, including [by] Dr. Zhengli Shi, head of coronavirus research at the Wuhan Institute of Virology.” Different organisms have different codon preferences. Human cells like to designate arginine with the codons CGT, CGC or CGG. But CGG is coronavirus’s least popular codon for arginine. Keep that in mind when looking at how the amino acids in the furin cleavage site are encoded in the SARS2 genome. Now the functional reason why SARS2 has a furin cleavage site, and its cousin viruses don’t, can be seen by lining up (in a computer) the string of nearly 30,000 nucleotides in its genome with those of its cousin coronaviruses, of which the closest so far known is one called RaTG13. Compared with RaTG13, SARS2 has a 12-nucleotide insert right at the S1/S2 junction. The insert is the sequence T-CCT-CGG-CGG-GC. The CCT codes for proline, the two CGG’s for two arginines, and the GC is the beginning of a GCA codon that codes for alanine. There are several curious features about this insert but the oddest is that of the two side-by-side CGG codons. Only 5 percent of SARS2’s arginine codons are CGG, and the double codon CGG-CGG has not been found in any other beta-coronavirus. So how did SARS2 acquire a pair of arginine codons that are favored by human cells but not by coronaviruses? Proponents of natural emergence have an up-hill task to explain all the features of SARS2’s furin cleavage site. They have to postulate a recombination event at a site on the virus’s genome where recombinations are rare, and the insertion of a 12-nucleotide sequence with a double arginine codon unknown in the beta-coronavirus repertoire, at the only site in the genome that would significantly expand the virus’s infectivity. For the lab escape scenario, the double CGG codon is no surprise. The human-preferred codon is routinely used in labs. So anyone who wanted to insert a furin cleavage site into the virus’s genome would synthesize the PRRA-making sequence in the lab and would be likely to use CGG codons to do so. A third scenario of origin. There’s a variation on the natural emergence scenario that’s worth considering. This is the idea that SARS2 jumped directly from bats to humans, without going through an intermediate host as SARS1 and MERS did. A leading advocate is the virologist David Robertson who notes that SARS2 can attack several other species besides humans. He believes the virus evolved a generalist capability while still in bats. One problem with this idea, though, is that if SARS2 jumped from bats to people in a single leap and hasn’t changed much since, it should still be good at infecting bats. And it seems it isn’t. The bat coronaviruses of the Yunnan caves can infect people directly. In April 2012 six miners clearing bat guano from the Mojiang mine contracted severe pneumonia with COVID-19-like symptoms and three eventually died. A virus isolated from the Mojiang mine, called RaTG13, is still the closest known relative of SARS2 Researchers could have gotten infected during their collecting trips, or while working with the new viruses at the Wuhan Institute of Virology. The virus that escaped from the lab would have been a natural virus, not one cooked up by gain of function. The direct-from-bats thesis is a chimera between the natural emergence and lab escape scenarios. It’s a possibility that can’t be dismissed. But against it are the facts that 1) both SARS2 and RaTG13 seem to have only feeble affinity for bat cells, so one can’t be fully confident that either ever saw the inside of a bat; and 2) the theory is no better than the natural emergence scenario at explaining how SARS2 gained its furin cleavage site, or why the furin cleavage site is determined by human-preferred arginine codons instead of by the bat-preferred codons. Proponents of lab escape can explain all the available facts about SARS2 considerably more easily than can those who favor natural emergence. Wade (2021) The origin of COVID: Did people or nature open Pandora’s box at Wuhan? 18.2 How the West lost Covid Aside from the three Nordic outliers of Finland, Norway, and Iceland, no European state has managed the coronavirus well by global standards — or by their own much higher ones. For decades, the richest nations of the world had told themselves a story in which wealth and medical superiority offered, if not total immunity from disease, then certainly a guarantee against pandemics, regarded as a premodern residue of the underdeveloped world. That arrogance has made the coronavirus not just a staggering but an ironic plague. Invulnerability was a myth, of course, but what the pandemic revealed was much worse than just average levels of susceptibility and weakness. It was these countries that suffered most, died most, flailed most. Gave up most easily, too, acquiescing to so much more disease that they might have been fighting a different virus entirely. For nearly the entire year, the COVID epicenter was not in China, where the pathogen originated, or in corners of South Asia or sub-Saharan Africa, where limited state capacity and medical infrastructure seemed, at the outset, especially concerning, but either in Europe or the United States — places that were rated just one year ago the best prepared in the world to combat infectious disease. One distinct pattern stands out, with national outcomes falling into three obvious clusters. In Europe, North America, and South America: nearly universal failure. In sub-Saharan Africa and South Asia: high caseloads and low death rates, owing largely to the age structure of populations. In East Asia, South-East Asia and Oceania: inarguable success. By damage, the coronavirus has not been a “Chinese flu” but a western malady. The virus originated in China, but the true focus of the epidemic that spread to the world was actually in northern Italy. There’s a huge gap between the reality of globalization and our ability to actually apprehend what that means. “One of the common features is that we are a medical-centric group of countries,” says Michael Mina, a Harvard epidemiologist who has spent the pandemic advocating for mass rollout of rapid testing on the pregnancy-kit model — only to meet resistance at every turn by those who insisted on a higher, clinical standard for tests. “We have an enormous focus on medicine and individual biology and individual health. We have very little focus as a group of nations on prioritizing the public good. We just don’t. It’s almost taboo — I mean, it is taboo. We have physicians running the show — that’s a consistent thing, medical doctors across the western European countries, driving the decision-making.” The result, he says, has been short-sighted calculations that prioritize absolute knowledge about everything before advising or designing policy about anything. Devi Sridhar 18.3 Pantent Protection Waiver The Biden administration announced Wednesday that it supports waiving intellectual property protections for Covid-19 vaccines, as countries struggle to manufacture the life-saving doses. “This is a global health crisis, and the extraordinary circumstances of the COVID-19 pandemic call for extraordinary measures. The Administration believes strongly in intellectual property protections, but in service of ending this pandemic, supports the waiver of those protections for COVID-19 vaccines,” United States Trade Representative Katherine Tai wrote in a statement. Stocks of major pharmaceutical companies that have produced vaccines, including Moderna, BioNTech and Pfizer, dropped sharply after news of the potential waivers first broke. CNBC "],["quantum-mechanics.html", "19 Quantum Mechanics 19.1 Many-World Interpretation", " 19 Quantum Mechanics 19.1 Many-World Interpretation Ball The idea that the universe splits into multiple realities with every measurement has become an increasingly popular proposed solution to the mysteries of quantum mechanics. But this “many-worlds interpretation” is incoherent, Philip Ball argues in this adapted excerpt from his new book Beyond Weird. Phillip Ball (2018) Why the Many-Worlds Interpretation Has Many Problems "],["vaccines.html", "20 Vaccines 20.1 mRNA", " 20 Vaccines 20.1 mRNA DW Claim: mRNA vaccines manipulate human DNA DW: False It is easy to get DNA and RNA confused, two similar abbreviations that relate to genetic material. But they’re very different. DNA contains the genetic blueprint that determines our bodies’ various traits. Viruses such as SARS-CoV-2 have RNA that stores their genetic material. But RNA is also found in the human body, and plays a role in protein synthesis. Viruses tap into this mechanism to reproduce in human cells. The human body, however, recognizes these intruders by their protein spike, producing antibodies and t-cells to fight off the virus. RNA vaccinations inject only one element of the SARS-CoV-2 virus into the human body, namely mRNA, containing the blueprint to produce its spike protein. The human immune system then kicks into action, forming antibodies against the pathogen. No human or virus RNA, however, ever enters the cell nucleus. This means it does not get in contact with our genetic material. After serving its purpose, human cells then break down the RNA. A scientific study published in December 2020 claims the genetic material from the SARS-CoV-2 virus could manipulate human DNA through the reverse transcriptase, an enzyme that transcribes RNA into DNA, which can enter the cell nucleus. The study in question has not yet been peer-reviewed and is hotly debated. Virologist David Baltimore from the California Institute of Technology won the Nobel Prize for his role in discovering reverse transcriptase. Science magazine quoted him describing the new work and findings as “impressive” and “unexpected.” However, he noted that the work showed only that fragments of the COVID-19 virus genome integrate that couldn’t produce infectious particles and represented a biological “dead end.” “It is also not clear if, in people, the cells that harbor the reverse transcripts stay around for a long time or they die,” Baltimore said. The work raises a lot of interesting questions.\" Waldemar Kolanus, who headsLife &amp; Medical Sciences Institute (LIMES) at Bonn University, doubts the findings are relevant for the actual vaccine. Speaking to DW, he said the structure of mRNA has been deliberately altered for vaccines so as to prevent cells instantly breaking them down.“Most likely, it cannot be reverse transcribed.” As such, mRNA vaccines are much safer with regard to such processes than actual virus genomes, he says. DW Factcheck Zhang Prolonged SARS-CoV-2 RNA shedding and recurrence of PCR-positive tests have been widely reported in patients after recovery, yet these patients most commonly are non-infectious. Here we investigated the possibility that SARS-CoV-2 RNAs can be reverse-transcribed and integrated into the human genome and that transcription of the integrated sequences might account for PCR-positive tests. In support of this hypothesis, we found chimeric transcripts consisting of viral fused to cellular sequences in published data sets of SARS-CoV-2 infected cultured cells and primary cells of patients, consistent with the transcription of viral sequences integrated into the genome. To experimentally corroborate the possibility of viral retro-integration, we describe evidence that SARS-CoV-2 RNAs can be reverse transcribed in human cells by reverse transcriptase (RT) from LINE-1 elements or by HIV-1 RT, and that these DNA sequences can be integrated into the cell genome and subsequently be transcribed. Human endogenous LINE-1 expression was induced upon SARS-CoV-2 infection or by cytokine exposure in cultured cells, suggesting a molecular mechanism for SARS-CoV-2 retro-integration in patients. This novel feature of SARS-CoV-2 infection may explain why patients can continue to produce viral RNA after recovery and suggests a new aspect of RNA virus replication. From Comment Section Mellissa Booth Chimeric sequence reads that the researchers found in the sequence databases are likely artifacts from the preparation of the RNA libraries. These chimera artifacts are a common phenomenon in total RNA sequence library construction from complex samples whether the source material is from humans, soil, sewage, ocean water, etc., and these artifacts end up in sequence databases. The other lab bench experiments DO NOT show that SARS-CoV-2 RNA is reverse transcribed to DNA, transported into the nucleus and then integrated into the host genome under NORMAL conditions. However, there are assays that could answer the question about viral integration. The authors could collect samples from infected patients that are still shedding virus but are non-infectious (as they mention in their summary) and perform Southern Blot analysis to determine if viral sequences have indeed been integrated into the host genome. And ultimately, IF the researchers find integration under normal conditions, the next questions follow: Are infected cells persistent? Do these integrated elements propagate in host cells? These are the questions that get to their hypothesis about purported viral integration elements being responsible for persistent detection of virus in non-infectious, post-COVID patients. Mar Your argument only addresses their initial screening of public datasets. They do show integration in vitro when they extract cell genomic DNA and do PCR. Southern blot is not necessary. Zhang (2020) NIH Pubmed Zhang (2020) SARS-CoV-2 RNA reverse-transcribed and integrated into the human genome (Preprint Not Peered) (pdf) Viral Shedding When an individual gets infected by a respiratory virus like SARS-CoV-2, the virus particles will bind to the various types of viral receptors, particularly the angiotensin-converting enzyme 2 (ACE2) receptors in the case of SARS-CoV-2, that line the respiratory tract. Throughout this ongoing process, infected individuals, who may not yet be experiencing any of the viral symptoms, are shedding viral particles while they talk, exhale, eat, and perform other normal daily activities. Under normal circumstances, viral shedding will not persist for more than a few weeks; however, as researchers gain a more in-depth understanding of the viral clearance of SARS-CoV-2, they have found that certain populations will shed this virus for much longer durations. In fact, a growing amount of evidence indicates that the viral shedding of SARS-CoV-2 begins before a patient is symptomatic, peaks at the point of or shortly after symptom onset and can continue to be released even after the individual’s symptoms have been resolved. The duration of viral shedding can be used to categorize the infectivity of a person; therefore, this information is crucial in implementing effective infection prevention strategies, such as appropriate quarantine durations and mask requirements. Currently, SARS-CoV-2 infection is confirmed with a positive polymerase chain reaction (PCR) test that can be conducted regardless of whether an individual is experiencing symptoms. Through such PCR tests, viral shedding of SARS-CoV-2 has been found to have a median duration of 12 to 20 days, with a persistence that can reach up to 63 days after initial symptom onset. The viral shedding of SARS-CoV-2 from the gastrointestinal (GI) tract does not appear to have any correlation with disease severity. There remains uncertain information on the proportion of SARS-CoV-2 cases that are asymptomatic. It is unclear as to whether these “asymptomatic” cases are truly asymptomatic in the sense that these infected individuals will never experience any of the viral symptoms, or are rather presymptomatic, meaning that these individuals had no symptoms at the time of their positive PCR test but eventually developed symptoms later. During both the SARS outbreak of 2002 and 2003, as well as during the current pandemic, researchers hypothesized that live viral particles present within fecal matter moving through sewage pipes could infect individuals through aerosols or droplets. SARS-CoV-2 RNA has been detected commonly in patient feces. However, as of yet more evidence is needed to assess the quantities of virus in fecal matter and its replication abilities in order to determine whether fecal-oral viral transmission is possible. Cuffari. What is Viral Shedding Viral Integration and consequences on Host Gene Expression Upon cell infection, some viruses integrate their genome into the host chromosome, either as part of their life cycle (such as retroviruses), or incidentally. While possibly promoting long-term persistence of the virus into the cell, viral genome integration may also lead to drastic consequences for the host cell, including gene disruption, insertional mutagenesis and cell death, as well as contributing to species evolution. This review summarizes the current knowledge on viruses integrating their genome into the host genome and the consequences for the host cell. Viral genome integration into the host genome is a hallmark of retroviruses, as it is a mandatory step in the retroviral life cycle and a prerequisite for productive infection. Upon integration, the retrovirus will persist in the infected cell for its entire lifespan, and will affect host gene expression depending on the integration site. Furthermore, if retroviral infection and integration occurs in the germline, the provirus will be transmitted to the progeny, and will thus contribute shaping the genome of future generations. This is the case of the so called “endogenized” retroviruses or endo­genous retroviruses (ERV). The site of the viral integration event can have multiple consequences for the host, as well as for the virus itself. Indeed, viral integration can lead to cell death or proliferation as a result of insertional mutagenesis. However, integration can also lead to consequences for the virus, i.e. active production or transcriptional silencing, a process also called latency that is key to establish viral persistence. Finally, integration in the germline can contribute shaping the host genome and participate in species evolution. Desfarges (2012) Retrovirus Retroviruses have undergone quite an explosion in our knowledge in about the last 40 years. The term “retrovirus” means it behaves backwards from the original way that we all think about genetics, which is that DNA makes RNA, and RNA makes protein. So retroviruses have an RNA genome, and when they get into cells that RNA is reverse-transcribed into DNA, so it goes backwards. The DNA is then inserted into the genome of the cell, so when the cell divides, it copies this, and it begins to express RNA. Some of that RNA is translated into proteins, which are needed to package the retrovirus. And another of those RNAs is the RNA genome that goes into those packaging materials and is excreted from the cell and goes on to infect other cells. So there are many different kinds of retroviruses. Now, the most famous one right at the moment is the human immunodeficiency virus which causes acquired immunodeficiency syndrome, or AIDS. But there are many different kinds of retroviruses that are associated with diseases, including cancer, leukemia, and AIDS, obviously. Finally, retroviruses have been tamed for use in gene therapy, so it is possible to take out all of the genes that allow the retrovirus to replicate itself and replace that with a gene that the particular cell that you’re interested in is missing. And so using the integrating ability of a retrovirus, you can actually take something that could ordinarily harm people and turn it into something that can be used as a therapeutic vehicle to make them better. Bodine "],["academia.html", "21 Academia 21.1 Academic Snobbery 21.2 Academic Publishing", " 21 Academia 21.1 Academic Snobbery On David Graeber He was one of the most important anthropologists of our time. It is a bitter paradox that the best anthropologist theorist of his generation never felt quite at home in the established anthropology circles. He hated academic conferences with a passion. It wasn’t just because of Yale’s shameful decision to get rid of him because of his political activism; David was a working-class person who detested, with every fiber of his being, any hint of academic elitism, networking, and schmoozing. Much to his personal cost, he rejected these strange sectarian rituals of academic life. He was the most generous friend and colleague one could hope to have, and the most formidable opponent of academic snobbery. After he was fired from Yale, David applied to more than twenty academic jobs in the US. He hasn’t been shortlisted for a single one. Grubacic on Graeber 21.2 Academic Publishing It should take however long as it takes to get it right. this is one of the reasons that the model of academia should be rethought from the ground up. too many mediocre books. it should be okay to just not say anything until you have something to say, which can takes decades (Colin Drumm (twitter)) "],["belgium.html", "22 Belgium", " 22 Belgium Across the Channel, Belgium was the second country after Britain to undergo a precocious industrial revolution. The process was partly driven by capital transfers from the English North: the Lancashire blacksmith and toolmaker William Cockerill arrived in Liège in 1799, when the Southern Low Countries—formerly a Habsburg possession—had been annexed to the French Republic under Napoleon’s forces. Cockerill found the social pre-conditions for mechanizing the Verviers woollen-handicraft industry not so different from those at home. His son John Cockerill expanded the family’s machine-building firm into a massive ironworks in the Seraing basin, turning the small post-Napoleonic buffer state into a global leader in steel production. As de Gaulle later quipped, the Kingdom of Belgium was always a country ‘created by the British to annoy the French.’ In 1815 the (largely Catholic) Southern Low Countries had been gifted to the (Protestant) Dutch monarch by the Congress of Vienna, but Brabant radicals rose against Dutch rule in 1830, with the tacit backing of the clergy and landowning nobility. In the heat of the 1830 revolt, Palmerston’s Cabinet engineered the new state as a perfect replica of the Westminster model, installing as its first king the uncle of the future Queen Victoria. The Belgian ruling bloc equally united a wealthy aristocratic landowning class with rising industrial strata, tightly knit around an arriviste royal house, and soon strutting an empire in the Congo built on raw-resource extraction. The family resemblance was remarkable—although Belgian elites, inured against external absolutist rule, always tolerated a higher degree of provincial and municipal autonomy. As Marx remarked, Belgium was ‘the snug, well-hedged, little paradise of the landlord, the capitalist and the priest.’ The development of 19th-century Belgian capitalism also offered, in microcosm, a parallel to the deep regional divisions of its larger overseas neighbour. Western Europe’s bourgeois enclave was linguistically divided between a poor agrarian Dutch-speaking north—the province of Flanders, with its North Sea estuary port at Antwerp—and the industrializing, mainly Francophone, southern province of Wallonia, oriented south and east towards France and Germany. Like the English North, the Belgian South developed into a smokestack landscape of steel mills, textile factories and mines, the ‘industrial furrow’ of the Sambre–Meuse valley, running from the Borinage coalfields to Charleroi, Liège and Verviers. Meanwhile the large agricultural hinterland of Flanders, populated by peasant families and putting-out households, was the equivalent of Britain’s internal Irish colony. In the 1840s, overtaken by a potato famine of Irish proportions, and as domestic weaving collapsed in face of international competition, the Flemish countryside discharged hundreds of thousands of impoverished cottagers into Wallonian mines and mills. There, their immigrant children quickly grew Francophone, shedding Dutch-speaking roots. As late as 1904, Rosa Luxemburg could speak of Flemish workers as ‘also dispossessed of their language.’ Wallonia’s industrial capital was never locally sourced. Instead, it was sponsored by financiers and landowners from splendid villas in Francophone Brussels, operating through holding companies structured by giant investment banks. An administrative centre under the Habsburgs, well-staffed with lawyers and bankers, Brussels was, like London, an essentially cosmopolitan city, its gaze always directed outward, and embedded in international capital flows that made it more beholden to foreign debtors than to workers in its own hinterland. These qualities also made French-speaking Brussels an essential Fremdkörper, situated within a Dutch-speaking North, overseeing a rapidly industrializing South, each either territorially or linguistically distinct from the Belgian capital. Capitalist development only deepened this outsider status. Far more than London, Brussels would be characterized by the absence of manufacturing, and consequently of an urban proletariat. Instead, as noted by the Flemish-nationalist historian Antoon Roosens, a high concentration of bourgeois and petty-bourgeois consumers made the capital ‘by far the most important market in the country for all finished industrial products.’ This ‘abnormal social composition’ also explained the city’s persistent provincialism, populated by citizens who had ‘made bourgeois mimesis their very mode of thinking and living.’footnote26 More an administrative centre than a megacity, Brussels lacked the ‘red belts’ which gave suburban Paris and London their municipal radicals (even if Marx penned the Communist Manifesto here in 1847). As with Britain, the head-start provided by Belgium’s early industrialization had turned into a disadvantage by the 1900s. Like England’s North, Wallonia was overtaken by the rise of more dynamic manufacturing centres in the Ruhr and beyond. After 1914, Belgian decline accelerated precipitously: World War One brought devastation, deepened by the Depression and Nazi occupation. In the post-war era, Belgium’s position was weaker still than Britain’s, with a much smaller and more exposed domestic market. At this point, however, the trajectories of the two economies diverged—with important consequences for their regional outcomes. A number of factors were involved. First, while post-war British leaders struggled to maintain the uk’s world-imperial privilege, Belgium’s political elites were ready for a new start. Threatened by international competition, they recognized a small economy at the centre of Europe could only survive as an open transit point for neighbouring economies. Churchill and Eden were happy to watch European integration from afar, priding themselves on the special relationship with Washington. With this imperial hangover, Britain never produced an equivalent to Paul-Henri Spaak, who played a central role in drafting the Treaty of Rome and succeeded in getting both the eec and nato headquartered in Brussels. Unlike de Gaulle, Spaak took care to hitch his country’s wagon to European integration without angering the American allies, making clear that Belgium would never plan to build a rival pole to Washington; quite the contrary.footnote27 Europeanization and internal modernization then went hand in hand. Unexpectedly, the Flemish region reaped the primary fruits of this modernizing strategy. A Ten-Year Plan re-tooled the port of Antwerp to meet the needs of American multinationals. It soon transcended mere transit status, providing a penumbra of assembly plants and light-industrial complexes around the docks to finish and repackage us goods for inland destinations. At the same time, relying on its maritime pivot, Flemish policy makers were able to turn the region into an export power for an eager German neighbour. Unlike its Irish counterpart, Flanders did not rest content as a simple safe haven for offshore capital. Instead, the Flemish elite began an ambitious drive towards a West-European knowledge economy, preparing an under-educated workforce for the era of high-value-added production. Petrochemical industries were propped up by a state-led university system, joined by world-class research clusters in bioengineering and medical sciences. By the 1960s, the Anglo-Belgian divergence was becoming plain to see. Between 1950 and 1985, Belgium’s growth rates were 50 per cent higher than the uk’s, driven mainly by modern light-industrial development around Antwerp, assisted by the regulatory machine in Brussels. When uk growth rates did recover, from 1985 to 2008, the expansion was concentrated in the Southeast, driven by crisis-prone financial expansion and asset-price inflation. Here lay another contrast: the role played by London in England’s North–South divide had no parallel in Belgium. First, Brussels has never been an organic part of either region; it is seen from Wallonia as a citadel of industrial exploiters, while for Flanders it is a mere ‘oil stain’ of francophonie. In this sense, Belgium could never be Brussellized, in the way that the uk had Londonized. On the contrary, Brussels had to watch the growth of its rival, Antwerp, as a multinational business centre, while it became mainly a supplier of regulatory services, helping American companies navigate the eec. Second, while the City of London expanded relentlessly on the basis of Eurodollar trading, the post-war retreat of Belgium’s old holding bourgeoisie demoted Brussels as a financial hub. Unlike the uk, Belgium was able to shuffle a redundant rentier class off-stage and kickstart a new developmental trajectory. And Wallonia? In 1960–61, galvanized by a massive strike wave, support grew for a regionalist breakaway movement as proposed by the charismatic metalworkers’ leader André Renard. Flemish support for the return of the Nazi-collaborationist King Leopold iii in 1950, bitterly opposed in Wallonia, helped to cast the Flemish North as a drag on the South’s socialist ambitions. Rather than accept Flemish cohabitation in a house tended by Belgium’s bourgeoisie, Wallonia’s proletariat should contemplate a proper jailbreak. The escape was to be both economic and political: autonomy for the country’s two linguistic communities, and a socialization of industry in the South. Although never a majority force in the Parti Socialiste (ps), Renardists assembled a lively cohort for socialism in one region. Before too long, however—compounding the shock loss of the Congo in 1960—Europe’s oldest steel sector was hit by the consequences of global overcapacity. Suddenly, there was no industry left to nationalize. By the early 1970s, Renard’s followers were left with a desiccated industrial landscape, only meagrely irrigated by state coffers. Meanwhile, in 1968 Flemish students had followed their Parisian counterparts by demanding an end to the Francophone dominance at the country’s oldest academic institution, the Catholic University of Leuven. Regionalization was now continuing at cruising speed, but hardly to the South’s benefit. Instead, Liège and Charleroi became the ruined temples of Belgian manufacturing, Manchesters without the sea, Pittsburghs on the Meuse. These developments gave the final push to a tottering Belgique à papa.footnote28 From 1970 onwards, Belgium’s old guard relaxed its grip on the unitary state as it initiated a series of reforms to regionalize and de-centralize the political system. Three official language communities, Dutch (59.6 per cent), French (40 per cent) and German (0.4 per cent), were established through the talentelling (language count); they would eventually acquire a council each, charged with education. Three political regions—the Brussels Capital Region, Flanders and Wallonia—were also given their own parliaments. An intricate system of financial transfers was set in place—disparagingly known as centenfederalisme, or ‘cash federalism,’ by Flemish nationalists—through which regions and communities would receive the bulk of their budgets from the central government. Step by step, in the 1980s, 90s and 2000s, new institutions began to operate and the Constitution was amended to define Belgium as ‘a federal state composed of communities and regions.’ Wallonia’s leaders decided to swim with the tide. During the crisis years of the 1970s, they picked at the carcass of the unitary state and secured emergency funding for Wallonia. It was clear that the centre of gravity of the Belgian economy had shifted dramatically northward: two economic poles—the port delta around Antwerp and a Brussellian metropole welcoming lobbyists into a growing eu bureaucracy—had replaced the South’s industrial magnets. The shift left behind a self-determining Wallonia that now had little to determine for itself. Subsequent generations of Walloon Socialists vacillated between performative unionism in government, to assure revenue for their region, and an assertive regionalism when stuck in opposition.footnote29 The Flemish bias towards export strategies, coupled with the North’s voting power, further marginalized the Renardist tendency. In the 1980s, the Walloon Socialist leader André Cools tried to counter regional decline by promoting municipal sharing schemes known as intercommunales: local councils could jointly manage public services and safeguard the country’s welfare gains.footnote30 Here was a further difference between Wallonia’s post-industrial status and that of England’s North. Compared to Thatcher’s onslaught, the neoliberal medicine administered by her Belgian admirer Wilfried Martens was relatively mild. The Catholic Party leader was partly checked by the stiff opposition of the Christian-Democrat trade-union wing. Federalism certainly helped to cushion the blow, albeit more through a Hegelian cunning of unreason: Belgium’s byzantine set-up has given Francophone Socialists veto power over a neoliberal push from the export-oriented North, despite the latter’s greater voting strength. With conservatives permanently unable to gain a unicameral majority à la Thatcher, it has been much easier to maintain Belgium’s corporatist structures—union control of social-security finances, enforced social bargaining, wage indexation, generous insurance mechanisms. In a small country with a relatively well-organized working class—in 2019, union membership surpassed 50 per cent—Thatcher’s Blitzkrieg on the miners never was a practical possibility. Unlike Italy or France, Belgian elites were also less eager to instrumentalize the eu to implement capitalist policy by stealth. That option required a greater degree of elite closure anyway, something Belgium’s fractious ruling bloc could never muster. Flanders became the luckiest legatee of Belgium’s regional partition. Fusing the institutions of the Flemish ‘community’ with those of the Flanders ‘region,’ it achieved full parliamentary devolution in 1995. For the Flemings, the Belgian house has been uitgeleefd—out-lived, or perhaps out-grown. As with any separatist squabble, the ‘transfer debate’ remains rife with acrimony; in 2005, Flemish nationalists drove a lorry full of fake euro bills to the south of their language border. The man who performed this stunt, Bart De Wever, is now Mayor of Antwerp. He has become only slightly less histrionic in his advocacy for the city’s export interests. No regionally unified Flemish capitalist class has cohered around this transition—yet.footnote31 Both the port of Antwerp and the Brussels metropolitan region are domains where foreign companies call the shots, ‘facilitated’ by Flemish and local authorities. Attempts to grant a Flemish-separatist project real political-economic depth remain breathless at best, mostly ruses to normalize the region’s far right. Nevertheless, visions of a regionally anchored neoliberalism have enjoyed a resurgence since 2010 with the rise of the free-market n-va (Nieuw-Vlaamse Alliantie, or New Flemish Alliance), currently the dominant party in Flanders—and led by the same Bart De Wever. Opting for a gradualist line—first confederalism, then full independence—the n-va is without doubt the most vocal of all separatist formations. The other contender, Vlaams Belang, has always stuck to a more chauvinist line, preferring to save money by keeping the foreigners out. English Northerners undoubtedly have reasons to be envious of their Walloon cousins. Though companions of the same post-industrial fate, Belgian deindustrialization has treated its working classes more fairly and less punitively. Walloon clientelism has proved less financialized, with social housing keeping up a steady pace of growth, in contrast to the council-housing sell-offs granted by Thatcher to the North’s ex-factory workers—said by some to be a key indicator of the Brexit vote. Contemporary Belgium is certainly no corporatist Eden, untouched by the market turn. But it has resisted many of the trends that have scarred countries in the developed world and boasts a better Gini-coefficient than other early industrializers. Anglo-Belgian divergences should not be overstated, however. Regionalization has hardly been a benediction to Belgium’s South. While Britain has six of the ten poorest regions in Northwest Europe, the Walloon regions of Hainaut, Liège and Charleroi are little better off. And just as South Yorkshire is only a few hours from inner London—still Europe’s richest district—so Liège also lies conspicuously closely to Luxembourg. Federalization has helped Wallonia, but it has hardly saved it. The cinema of the Dardenne brothers, with its focus on ‘poverty’ rather than class, provides aesthetic backing for a ps project of federally funded regional poverty management for the South that has given up hopes of reindustrialization altogether. The Dardennes’ oeuvre, from Rosetta to Two Days, One Night and The Unknown Girl, makes a striking contrast to the class confrontations depicted in the electrifying 1934 documentary Misère au Borinage by Henri Storck and Joris Ivens. Flemish neoliberals remain hopeful about a separatist free-trade breakthrough, letting the ‘best student in the Belgian class’ flourish next to competitors in Poland or Latvia. To no avail, however: anno 2021, the Belgian state is still here, badly mismanaging the covid crisis. But managing, nonetheless. It has to be said that the regional response to covid was just as shambolic as the federal one. Beneath Belgium’s so-called ‘communitarian’ crisis smoulders not only a medical or logistical crisis but above all a political one, affecting Belgium’s party democracy at its core. Recently leaked memos of the 2019 governmental negotiations indicated Francophone Socialists’ willingness to split between regions not only social security, but also labour-market policy and fire services. Hoping to secure its baronies in Brussels with a final pay out, the ps appeared willing to trade in the national achievements of the Belgian labour movement. Some tough questions follow. As the Walloon example shows, behind the question of regionalization stands the more intimidating one of capital investment. The English North never acquired a form of proto-statehood that would allow it to practice a properly local developmentalism; it was forced instead into an amorphous form of rebellion, within a topsy-turvy electoral geography that never provided a platform for regional consciousness. A look at post-industrial regions that did gain this form of statehood, however, is not comforting. In 2016, Walloon prime minister and Socialist Party leader Paul Magnette garnered laurels from the European left for his rebellion in the federal parliament against the imposition of the neoliberalizing eu–Canada trade deal. This act of resistance hid a structural dependency of the Walloon region on Flemish transfers, which have grown precipitously in the wake of federalization. Behind this lies the secular decline of Wallonian industry, unable to profit from containerization and shut out from the German-led Central European export boom. Jäger "],["china.html", "23 China 23.1 Economic Governance", " 23 China 23.1 Economic Governance A general pattern in China’s economic governance: Markets are being used as a tool to initiate change and unleash dynamism, but speculation is suppressed in order to maintain economic, social, and political stability. As I show in my book How China Escaped Shock Therapy, when China first introduced market reforms in the 1980s, the country’s economists were divided over the right approach. Some favored shock-like overnight liberalizations that would have cut the direct ties between state and market, while others argued for reform through state market creation, which was the path ultimately taken. The result of China’s distinct reform path is a highly marketized economy with active state participation in systemically significant markets, with the aim to balance market dynamism and the dangers of market volatility. This logic is reflected in the development of the cryptocurrency market in China. Isabella Weber in Fortune (on Bitcoin suppression) "],["columbia.html", "24 Columbia 24.1 Macroeconomic Policy", " 24 Columbia 24.1 Macroeconomic Policy n April 2021, Ivan Duque’s administration presented a tax reform bill labeled “Law of Sustainable Solidarity” to Congress. The bill contemplated an increment of the VAT on basic goods in conjunction with an increase in the marginal tax rates on the income of the so-called Colombian middle class. The vast majority of whom earns monthly less than 4,000,000 Colombian pesos (around 1,065 U.S. dollars). Although the bill put on the table contained some crucial elements for discussion, such as implementing a “basic monthly income” of 21 U.S. dollars (by far less than the current minimum wage). It contained little or nothing to effectively tackle Colombia’s high social and income inequality (with an official GINI of 0.526 for 2019). Since 2016 after the peace deal between the Colombian government and the FARC, which used to be the oldest and biggest guerrilla in Colombia, the government hasn’t implemented most of the elements contemplated in the peace agreement. Also, although Colombia has had macroeconomic stability for more than 20 years, an indicator such as the official unemployment rate has consistently been above 10%. The level of poverty before the COVID-19 shock was near 32%. Developing Economics (2021) For a new macroeconomic policy in Colombia "],["england.html", "25 England", " 25 England Tom Hazeldine’s The Northern Question imposes a much-needed historical lens on the discussion.footnote4 Rather than trade in essentialism about a North hesitant to change, Hazeldine deploys a Marxist method to explain the region’s woes. His first point of reference is Antonio Gramsci, whose reflections on the ‘Southern Question’ inflect the opening pages of his book. The Italian Marxist saw his party as the challenger to a timid Northern bourgeoisie that had failed to rally the peninsula around a popular-democratic Jacobin programme; instead, it brokered deals with Southern landowners and ecclesiastical classes, burdening the unified Italian nation-state with its typically hybrid character. Only a party with Machiavellian ambitions for national renewal could complete the task shirked by Italy’s Northern leaders, unwilling and unable to bury the old order. Hazeldine proposes a measured projection of this Gramscian frame onto Britain. The Northern Question takes as its epigraph the words of Gramsci’s prodigious Scottish pupil, Tom Nairn: The lamented ‘growing abyss’ between North and South should not really be a subject for mere figures, nor for moral outrage, nor for futile retreads of Westminster-inspired ‘modernization’: it can’t be tackled within the existing State, because it is the existing State, the dominance of the Crown (or ‘anti-industrial’) culture, the thriving pseudo-nationalism of the Old Regime. Any analysis of the North must of course begin with the question of whether ‘it’ actually exists. Hazeldine is clear that there is more to the region than cultural affect, an aggregate of accents and music scenes. Geographic definitions have varied: north of the River Trent, the Mersey, the Ribble—or the Severn–Wash divide, which would include the economically blighted Midlands? Hazeldine’s answer is structural. He wants to explore the relation of the rise and fall of the North, as an industrial powerhouse, to the rise and rise of London, as a capital of empire and high finance. As he notes, deindustrialization has meant that contemporary regional disparities in England have been blurred; they are now characterized less by a national division of labour than by ‘the positional superiority of London in a services-dominated national economic space.’ In this context, the northern rustbelt acts as the ‘senior representative’ of a much larger left-behind England.footnote6 Yet the North has never achieved the ‘escape velocity’ needed to free itself from its industrial past: where the mills and coal-pits started, the North begins. It was the Industrial Revolution that transformed the region from ‘an obscure, ill-cultivated swamp’ (Engels) into the hub of an industrial-capitalist system that would be emulated the world over. Hazeldine argues that the North has been the launch pad for three successive attempts to challenge the hegemony of the Southern-based regime of landed-finance capital. Jäger "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 210102 Climate Finance Shadow Report 2020", " C NEWS C.1 210102 Climate Finance Shadow Report 2020 Oxfam has released this report with subtitle Asessing progress towards the $100 billion commitment Progress is NOT in line with need or pledges. Climate change could undo decades of progress in development and dramatically increase global inequalities. There is an urgent need for climate finance to help countries cope and adapt. Over a decade ago, developed countries committed to mobilize $100bn per year by 2020 to support developing countries to adapt and reduce their emissions. The goal is a critical part of the Paris Agreement. As 2020 draws to a close, Oxfam’s Climate Finance Shadow Report 2020 offers an assessment of progress towards the $100bn goal. Based on 2017–18 reported numbers, developed countries are likely to claim they are on track to meet the $100bn goal. And on their own terms, they may be. But how the goal is met is as important as whether it is met. The dubious veracity of reported numbers, the extent to which climate finance is increasing developing country indebtedness, and the enduring gap in support for adaptation, LDCs and SIDS, are grave concerns. Meeting the $100bn goal on these terms would be cause for concern, not celebration. Oxfam Report (pdf) "]]
